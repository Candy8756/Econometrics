The Econometrics of Financial Markets 

John Y. Campbell 

Andrew W. Lo 

A. Craig MacKinlay 

Princeton University Press 
Princeton, New Jersey 



Contents 

List of Figures xiii 

List of Tables xv 

Preface xvii 

1 Introduction 3 
1.1 Organization of the Book . . . . . . . . . . . . . . . . . .  4 
1.2 Useful Background . . . . . . . . . . . . . . . . . . . . . .  6 

1.2.1 Mathematics Background . . . . . . . . . . . . . .  6 
1.2.2 Probability and Statistics Background . . . . . . . .  6 
1.2.3 Finance Theory Background . . . . . . . . . . . . .  7 

1.3 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . .  8 
1.4 Prices, Returns. and Compounding . . . . . . . . . . . . .  9 

1.4.1 Definitions and Conventions . . . . . . . . . . . . .  9 
1.4.2 The Marginal, Conditional. andJ oint Distribution 

of Returns . . . . . . . . . . . . . . . . . . . . . . . .  13 
1.5 Market Efficiency . . . . . . . . . . . . . . . . . . . . . . .  20 

1.5.1 Efficient Markets and the Law of Iterated 
Expectations . . . . . . . . . . . . . . . . . . . . . .  22 

1.5.2 Is Market Efficiency Testable? . . . . . . . . . . . .  24 

2 The Predictability of Asset Returns 27 
2.1 The Random Walk Hypotheses . . . . . . . . . . . . . . .  28 

2.1.1 The Random Walk 1: IID Increments . . . . . . . .  31 
2.1.2 The Random Walk 2: Independent Increments . . 32 
2.1.3 The Random Walk 3: Uncorrelated Increments . . 33 

2.2 Tests of Random Walk 1: IID Increments . . . . . . . . . .  33 
2.2.1 Traditional Statistical Tests . . . . . . . . . . . . . .  33 
2.2.2 Sequences and Reversals, and Runs . . . . . . . . .  34 



... 
vlll Contents Contents 

2.3 Tests of Random Walk 2: Independent Increments . . . .  41 4.3.3 Other Statistical Models . . . . . . . . . . . . . . .  155 
2.3.1 Filter Rules . . . . . . . . . . . . . . . . . . . . . .  42 4.3.4 Economic Models . . . . . . . . . . . . . . . . . . .  156 
2.3.2 Technical Analysis . . . . . . . . . . . . . . . . . . .  43 4.4 Measuring and Analyzing Abnormal Returns . . . . . . . .  157 

2.4 Tests of Random Walk 3: Uncorrelated Increments . . . .  44 4.4.1 Estimation of the Market Model . . . . . . . . . . .  158 
2.4.1 Autocorrelation Coefficients . . . . . . . . . . . . .  44 4.4.2 Statistical Properties of Abnormal Returns . . . . .  159 
2.4.2 Portmanteau Statistics . . . . . . . . . . . . . . . .  47 4.4.3 Aggregation of Abnormal Returns . . . . . . . . . .  160 
2.4.3 Variance Ratios . . . . . . . . . . . . . . . . . . . .  48 4.4.4 Sensitivity to Normal Return Model . . . . . . . . .  162 

2.5 Long-Horizon Returns . . . . . . . . . . . . . . . . . . . .  55 4.4.5 CARS for the Earnings-Announcement Example . . 163 
2.5.1 Problems with Long-Horizon Inferences . . . . . .  57 4.4.6 Inferences with Clustering . . . . . . . . . . . . . .  166 

2.6 Tests For Long-Range Dependence . . . . . . . . . . . . .  59 4.5 Modifying the Null Hypothesis . . . . . . . . . . . . . . .  167 
2.6.1 Examples of Long-Range Dependence . . . . . . .  59 4.6 Analysis of Power . . . . . . . . . . . . . . . . . . . . . . .  168 
2.6.2 The Hurst-Mandelbrot Rescaled Range Statistic . . 62 4.7 Nonparametric Tests . . . . . . . . . . . . . . . . . . . . .  172 

2.7 Unit Root Tests . . . . . . . . . . . . . . . . . . . . . . . .  64 4.8 Cross-Sectional Models . . . . . . . . . . . . . . . . . . . .  173 
2.8 Recent Empirical Evidence . . . . . . . . . . . . . . . . . .  65 4.9 Further Issues . . . . . . . . . . . . . . . . . . . . . . . . .  175 

2.8.1 Autocorrelations . . . . . . . . . . . . . . . . . . .  66 4.9.1 Role of the Sampling Interval . . . . . . . . . . . .  175 
2.8.2 Variance Ratios . . . . . . . . . . . . . . . . . . . .  68 4.9.2 Inferences with Event-Date Uncertainty . . . . . . .  176 
2.8.3 Cross-Autocorrelations and Lead-Lag Relations . . 74 4.9.3 Possible Biases . . . . . . . . . . . . . . . . . . . . .  177 
2.8.4 Tests Using Long-Horizon Returns . . . . . . . . .  78 4.10 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . .  178 

2.9 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . .  80 
5 The Capital Asset Pricing Model 181 

Market Microstructure 83 Re v i e wof  the C A M. . P . . . . . . . . . . . . . . . . .  181 
3.1 Nonsynchronous Trading . . . . . . . . . . . . . . . . . .  84 Results from Efficient-Set Mathematics . . . . . . . . . . .  184 

3.1.1 A Model of Nonsynchronous Trading . . . . . . . .  85 Statistical Framework for Estimation and Testing . . . . . .  188 
3.1.2 Extensions and Generalizations . . . . . . . . . . .  98 5.3.1 Sharpe-Lintner Version . . . . . . . . . . . . . . . .  189 

3.2 The Bid-Ask Spread . . . . . . . . . . . . . . . . . . . . . .  99 5.3.2 Black Version . . . . . . . . . . . . . . . . . . . . .  196 
3.2.1 Bid-Ask Bounce . . . . . . . . . . . . . . . . . . . .  101 Size of Tests . . . . . . . . . . . . . . . . . . . . . . . . . .  203 
3.2.2 Components of the Bid-Ask Spread . . . . . . . . .  103 Power of Tests . . . . . . . . . . . . . . . . . . . . . . . . .  204 

3.3 Modeling Transactions Data . . . . . . . . . . . . . . . . .  107 Nonnormal and Non-IID Returns . . . . . . . . . . . . . .  208 
3.3.1 Motivation . . . . . . . . . . . . . . . . . . . . . . .  108 Implementation of Tests . . . . . . . . . . . . . . . . . . .  211 
3.3.2 Rounding and Barrier Models . . . . . . . . . . . .  114 5.7.1 Summary of Empirical Evidence . . . . . . . . . . .  211 
3.3.3 The Ordered Probit Model . . . . . . . . . . . . . .  122 5.7.2 Illustrative Implementation . . . . . . . . . . . . .  212 

3.4 Recent Empirical Findings . . . . . . . . . . . . . . . . . .  128 5.7.3 Unobservability of the Market Portfolio . . . . . . .  213 
3.4.1 Nonsynchronous Trading . . . . . . . . . . . . . .  128 Cross-Sectional Regressions . . . . . . . . . . . . . . . . .  215 
3.4.2 Estimating the Effective Bid-Ask Spread . . . . . . .  134 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . .  217 
3.4.3 Transactions Data . . . . . . . . . . . . . . . . . . .  136 

3.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . .  144 6 Multifactor Pricing Models 219 
6.1 Theoretical Background . . . . . . . . . . . . . . . . . . .  219 

4 Event-Study Analysis 149 6.2 Estimation and Testing . . . . . . . . . . . . . . . . . . . .  222 
4.1 Outline of an Event Study . . . . . . . . . . . . . . . . . .  150 6.2.1 Portfolios as Factors with a Riskfree Asset . . . . . .  223 
4.2 An Example of an Event Study . . . . . . . . . . . . . . . .  152 6.2.2 Portfolios as Factors without a Riskfree Asset . . . .  224 
4.3 Models for Measuring Normal Performance . . . . . . . .  153 6.2.3 Macroeconomic Variables as Factors . . . . . . . . .  226 

4.3.1 Constant-Mean-Return Model . . . . . . . . . . . .  154 6.2.4 Factor Portfolios Spanning the Mean-Variance 
4.3.2 Market Model . . . . . . . . . . . . . . . . . . . . .  155 Frontier . . . . . . . . . . . . . . . . . . . . . . . . .2 28 



Contents Contents xi 

Estimation of Risk Premia and Expected Returns . . . . .  231 9 Derivative Pricing Models 339 
Selection of Factors . . . . . . . . . . . . . . . . . . . . . .  233 9.1 Brownian Motion . . . . . . . . . . . . . . . . . . . . . . .  341 
6.4.1 Statistical Approaches . . . . . . . . . . . . . . . . .  233 9.1.1 Constructing Brownian Motion . . . . . . . . . . .  341 
6.4.2 Number of Factors . . . . . . . . . . . . . . . . . .  238 9.1.2 Stochastic Differential Equations . . . . . . . . . .  346 
6.4.3 Theoretical Approaches . . . . . . . . . . . . . . .  239 9.2 A Brief Review of Derivative Pricing Methods . . . . . . . .  349 
Empirical Results . . . . . . . . . . . . . . . . . . . . . . .  240 9.2.1 The Black-Scholes and Merton Approach . . . . . .  350 
Interpreting Deviations from Exact Factor Pricing . . . . .  242 9.2.2 The Martingale Approach . . . . . . . . . . . . . .  354 
6.6.1 Exact Factor Pricing Models, Mean-Variance Anal- 9.3 Implementing Parametric Option Pricing Models . . . . .  355 

ysis. and the Optimal Orthogonal Portfolio . . . . . 2 43 9.3.1 Parameter Estimation of Asset Price Dynamics . . .  356 
6.6.2 Squared Sharpe Ratios . . . . . . . . . . . . . . . .  245 9.3.2 Estimating0 in the Black-Scholes Model . . . . . .  361 
6.6.3 Implications for Separating Alternative Theories . . 246 9.3.3 Quantifying the Precision of Option Price 
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . .  251 Estimators . . . . . . . . . . . . . . . . . . . . . . . .  367 

9.3.4 The Effects of Asset Return Predictability . . . . . .  369 
7 Present-Value Relations 253 9.3.5 Implied Volatility Estimators . . . . . . . . . . . . .  377 

The Relation between Prices. Dividends. and Returns . . .  254 9.3.6 Stochastic Volatility Models . : . . . . . . . . . . . .  379 
7.1.1 The Linear Present-Value Relation with Constant 9.4 Pricing Path-Dependent DerivativesViaM onte Carlo Sim- 

Expected Returns . . . . . . . . . . . . . . . . . . . 2 55 ulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 82 
7.1.2 Rational Bubbles . . . . . . . . . . . . . . . . . . .  258 9.4.1 Discrete Versus Continuous Time . . . . . . . . . .  383 
7.1.3 An Approximate Present-Value Relation with Time- 9.4.2 How Many Simulations to Perform . . . . . . . . .  384 

Varying Expected Returns . . . . . . . . . . . . . . . 2 60 9.4.3 Comparisons with a Closed-Form Solution . . . . .  384 
7.1.4 Prices and Returns in a Simple Example . . . . . .  264 9.4.4 Computational Efficiency . . . . . . . . . . . . . .  386 
Present-Value Relations and US Stock Price Behavior . . .  267 9.4.5 Extensions and Limitations . . . . . . . . . . . . . .  390 
7.2.1 Long-Horizon Regressions . . . . . . . . . . . . . .  267 9.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . .  391 
7.2.2 Volatility Tests . . . . . . . . . . . . . . . . . . . . .  275 
7.2.3 Vector Autoregressive Methods . . . . . . . . . . .  279 10 Fixed-Income Securities 395 
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . .  286 10.1 Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . .  396 

10.1.1 Discount Bonds . . . . . . . . . . . . . . . . . . . .  397 
8 Intertemporal Equilibrium Models 29 1 10.1.2 Coupon Bonds . . . . . . . . . . . . . . . . . . . .  401 

The Stochastic Discount Factor . . . . . . . . . . . . . . .  293 10.1.3 Estimating the Zero-Coupon Term Structure . . . .  409 
8.1.1 Volatility Bounds . . . . . . . . . . . . . . . . . . .  296 10.2 Interpreting the Term Structure of Interest Rates . . . . .  413 
Consumption-Based Asset Pricing with Power Utility . . . .  304 10.2.1 The Expectations Hypothesis . . . . . . . . . . . .  413 
8.2.1 Power Utility in a Lognormal Model . . . . . . . . .  306 10.2.2 Yield Spreads and Interest Rate Forecasts . . . . . .  418 
8.2.2 Power Utility and Generalized Method of 10.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . .  423 

Moments . . . . . . . . . . . . . . . . . . . . . . . . 3 14 
Market Frictions . . . . . . . . . . . . . . . . . . . . . . .  314 1 1 TermStructure Models 427 
8.3.1 Market Frictions and Hansen-Jagannathan 11.1 Affine-Yield Models . . . . . . . . . . . . . . . . . . . . . .  428 

Bounds . . . . . . . . . . . . . . . . . . . . . . . . . 3 15 11. 1 . 1 A Homoskedastic Single-Factor Model . . . . . . .  429 
8.3.2 Market Frictions and Aggregate Consumption 11 .1.2 A Square-Root Single-Factor Model . . . . . . . . .  435 

Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 16 11.1.3 A Two-Factor Model . . . . . . . . . . . . . . . . . .  438 
More General Utility Functions . . . . . . . . . . . . . . .  326 11.1.4 Beyond Affine-Yield Models . . . . . . . . . . . . .  441 
8.4.1 HabitFormation . . . . . . . . . . . . . . . . . . .  326 11.2 Fitting Term-Structure Models to the Data . . . . . . . . .  442 
8.4.2 Psychological Models of Preferences . . . . . . . .  332 11 .2.1 Real Bonds, Nominal Bonds, and Inflation . . . . .  442 
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . .  334 11.2.2 Empirical Evidence on Affine-Yield Models . . . .  445 



xii Contents 

11.3 Pricing Fixed-Income Derivative Securities . . . . . . . . .  455 
11.3.1 Fitting the Current Term Structure Exactly . . . . .  456 
11.3.2 Forwards and Futures . . . . . . . . . . . . . . . . .  458 
11.3.3 Option Pricing in a Term-Structure Model . . . . .  461 List of Figures 

11.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . .  464 

12 Nonlinearities in Financial Data 467 
12.1 Nonlinear Structure in Univariate Time Series . . . . . . .  468 

12.1.1 Some Parametric Models . . . . . . . . . . . . . . .  470 
12.1.2 Univariate Tests for Nonlinear Structure . . . . . .  475 

12.2 Models of Changing Volatility . . . . . . . . . . . . . . . .  479 
12.2.1 Univariate Models . . . . . . . . . . . . . . . . . . .  481 
12.2.2 Multivariate Models . . . . . . . . . . . . . . . . . .  490 
12.2.3 Links between First and Second Moments . . . . .  494 Dividend Payment Timing Convention . . . . . . . . . . . . .  

12.3 Nonparametric Estimation . . . . . . . . . . . . . . . . . .  12 
498 Comparison of Stable and Normal Density Functions 

12.3.1 Kernel Regression . . . . . . . . . . . . . . . . . . .  . . . . .  18 
500 

12.3.2 Optimal Bandwidth Selection . . . . . . . . . . . .  502 Nontrading-Induced Autocorrelations . . . . . . . . . . . . .  96 
12.3.3 Average Derivative Estimators . . . . . . . . . . . .  504 Histogram of Daily Price Fractions and Price Changes for 
12.3.4 Application: Estimating State-Price Densities . . . .  507 Five NYSE Stocks from January 2. 1990 to December 31. 

12.4 Artificial Neural Networks . . . . . . . . . . . . . . . . . .  512 1992 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  111 
12.4.1 Multilayer Perceptrons . . . . . . . . . . . . . . . .  512 2-Histories of Daily Stock Returns for Five NYSE Stocks from 
12.4.2 Radial Basis Functions . . . . . . . . . . . . . . . .  516 January 2. 1990 to December 31. 1992 . . . . . . . . . . . . .  113 
12.4.3 Projection Pursuit Regression . . . . . . . . . . . .  518 The Ordered Probit Model . . . . . . . . . . . . . . . . . . .  125 
12.4.4 Limitations of Learning Networks . . . . . . . . . .  518 
12.4.5 Application: Learning the Black-Scholes Formula . 519 Time Line for an Event Study . . . . . . . . . . . . . . . . . .  157 

12.5 Overfitting and Data-Snooping . . . . . . . . . . . . . . .  523 (a) Plot of Cumulative ~ a r k e t - ~ o dAebln ormal Return for 
12.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . .  524 Earning Announcements; (b) Plot of Cumulative Con- 

stant-Mean-Return-Model Abnormal Return for Earning 
Appendix 527 Announcements . . . . . . . . . . . . . . . . . . . . . . . .  
A.l  Linear Instrumental Variables . . . . . . . . . . . . . . . .  527 Power of Event-Study Test Statistic J1 to Reject the Null Hy- 
A.2 Generalized Method of Moments . . . . . . . . . . . . . .  532 pothesis that the Abnormal Return Is Zero. When the Square 
A.3 Serially Correlated and Heteroskedastic Errors . . . . . . .  534 Root of the Average Variance of the Abnormal Return Across 
A.4 GMM and Maximum Likelihood . . . . . . . . . . . . . .  536 Firms is (a) 2% and (b)4% ......... . . . . . . . . . . . . . . . .  

Power of EventStudy Test Statistic J1 to Reject the Null Hy- 
References 541 pothesis that the Abnormal Return is Zero. for Different 

Sampling Intervals. When the Square Root of the Average 
Author Index 587 Variance of the Abnormal Return Across Firms Is 4% for the 

Daily Interval . . . . . . . . . . . . . . . . . . . . . . . . . .  
Subject Index 597 

Minimum-Variance Portfolios Without Riskfree Asset . . . . .  187 
Minimum-Variance Portfolios With Riskfree Asset . . . . . . .  189 

Distributions for the CAPM Zero-Intercept Test Statistic for 
Four Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . .  250 

xiii 



List of Figures 

Log Real Stock Price and Dividend Series. Annual US Data. 
1872 to 1994 . . . . . . . . . . . . . . . . . . . . . . . . . . .  282 
Log Real Stock Price and Estimated Dividend Component. 
Annual US Data. 1876 to 1994 . . . . . . . . . . . . . . . . . .  283 List of Tables 
Log Dividend-Price Ratio and Estimated Dividend Compo- 
nent. Annual US data. 1876 to 1994 . . . . . . . . . . . . . .  284 

(a) Mean-Standard Deviation Diagram for Asset Returns; 
(b) Implied Standard Deviation-Mean Diagram for Stochas- 
tic Discount Factors . . . . . . . . . . . . . . . . . . . . . . .  299 
(a) Mean-Standard Deviation Diagram for a Single Excess As- 
set Return; (b) Implied Standard Deviation-Mean Diagram 
for Stochastic Discount Factors . . . . . . . . . . . . . . . . .  302 
Feasible Region for Stochastic Discount Factors Implied by 
Annual US Data.  1891 to 1994 . . . . . . . . . . . . . . . . . .  303 Stock market returns. 1962 to 1994. . . . . . . . . . . . . . . .  
Sample Path of a Discrete-Time Random Walk . . . . . . . . .  342 Classification of random walk and martingale hypotheses. . .  
Sample Path and Conditional Expectation of a Brownian Expected runs for a random walk with drift /.L. . . . . . . . . .  
Motion with Drift . . . . . . . . . . . . . . . . . . . . . . . . .  345 Autocorrelation function for fractionally differenced process.  

Autocorrelation in daily, weekly. and monthly stock index 
Zero-Coupon Yield and Forward-Rate Curves in January 
1987 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  returns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

398 
Cash Flows in a Forward Transaction . . . . . . . . . . . . . .  Variance ratios for weekly stock index returns . . . . . . . . . .  

400 
Calculation of Duration for a Coupon Bond . . . . . . . . . .  Variance ratios for weekly size-sorted portfolio returns . . . . .  

402 
Variance ratios for weekly individual security returns . . . . . .  

The Price-Yield Relationship . . . . . . . . . . . . . . . . . . .  407 
Short- and Long-Term Interest Rates 1952 to 1991 . . . . . . .  416 Cross-autocorrelation matrices for size-sorted portfolio returns . 

Asymmetry of cross-autocorrelation matrices. . . . . . . . . .  
Change in Short Rate Divided by Short Rate to the Power y . 450 
Sample and Theoretical Average Forward-Rate Curves . . . .  Summary statistics for daily returns of five NYSE stocks . . . . .  

454 
Relative frequencies of price changes for tick data of five 

The Tent Map . . . . . . . . . . . . . . . . . . . . . . . . . . .  stocks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Monthly Excess Log US Stock Returns. 1926 to 1994 . . . . .  Expected upper bounds for discreteness bias: daily returns.  . 
Shifted and Tilted Absolute-Value Function . . . . . . . . . .  Expected upper bounds for discreteness bias: monthly returns.  
Simulation of Yt = Sin(&) + 0.56, . . . . . . . . . . . . . . . .  Expected upper bounds for discreteness bias: annual returns.  
Kernel Estimator . . . . . . . . . . . . . . . . . . . . . . . . .  Autocorrelation matrices for size-sorted portfolio returns. . .  
Bullish Vertical Spread Payoff Function and Smoothed Estimates of daily nontrading probabilities . . . . . . . . . . .  
Version . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Nontrading-implied weekly index autocorrelations . . . . . . .  
Binary Threshold Model . . . . . . . . . . . . . . . . . . . . .  Summary statistics for transactions data of six stocks . . . . . .  
Comparison of Heaviside and Logistic Activation Functions . Estimates of ordered probit partition boundaries. . . . . . . .  
Multilayer Perceptron with a Single Hidden Layer . . . . . . .  Estimates of ordered probit "slope" coefficients. . . . . . . . .  
MLP(1.5 ) Model of Y, = Sin(Xt)+  0.56, . . . . . . . . . . . .  

Abnormal returns for an event study of the information con- 
Typical Simulated Training Path . . . . . . . . . . . . . . . . .  
Typical Behavior of Four-Nonlinear-Term RBF Model . . . . .  tent of earnings announcements. . . . . . . . . . . . . . . . .  

Power of event-study test statistic J1 to reject the null hypoth- 
esis that the abnormal return is zero . . . . . . . . . . . . . . .  



List of Tables 

Finite-sample size of tests of the Sharpe-Lintner CAPM using 
largesample test statistics. . . . . . . . . . . . . . . . . . . . .  205 
Power of F-test of Sharpe-Lintner CAPM using statistic J1 . . .  207 
Empirical results for tests of the Sharpe-Lintner version of Preface 
the CAPM. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  214 

Summary of results for tests of exact factor pricing using 
zero-intercept F-test. . . . . . . . . . . . . . . . . . . . . . . .  241 

Long-horizon regressions of log stock returns on the log 
dividend-price ratio. . . . . . . . . . . . . . . . . . . . . . . .  269 
Long-horizon regressions of log stock returns on the stochas- 
tically detrended short-term interest rate. . . . . . . . . . . .  270 

Moments of consumption growth and asset returns. . . . . . .  308 The seeds of this book were planted over fifteen years ago, at the very start 
Instrumental variables regressions for returns and consump of our professional careers. While studying financial economics, and as 
tion growth. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  312 we began to teach it, we discovered several excellent textbooks for finan- 

cial theory-Duffie (l992), Huang and Iitzenberger (l988),a nd Ingersoll 
Multiplication rules for stochastic differentials. . . . . . . . .  347 

(1987), for example-but no equivalent textbook for empirical methods. 
Asymptotic standard errors for &. . . . . . . . . . . . . . . . .  365 

During the same period, we participated in research conferences on 
Asymptotic standard errors for d2. . . . . . . . . . . . . . . .  366 

Financial Markets and Monetary Economics, held under the auspices of the 
Cutoff values for comparative statics of Vf. . . . . . . . . . . .  369 

National Bureau of Economic Research in Cambridge, Massachusetts. Many 
Asymptotic variances of Black-Scholes call price sensitivity 

of the papers that captured our attention at these meetings involved new 
estimators. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  370 

econometric methods or new empirical findings in financial economics. We 
Option prices on assets with negatively autocorrelated 
returns. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  felt that this was some of the most exciting research being done in finance, 

376 
and that students should be exposed to this material at an early stage. 

Monte Carlo estimation of lookback option price. . . . . . . .  386 
In 1989 we began to discuss the idea of writing a book that would cover 

Macaulay's and modified duration for selected bonds. . . . .  404 econometric methods as applied to finance, along with some of the more 
Means and standard deviations of term-structure variables. . .  415 prominent empirical results in this area. We began writing in earnest in 
Regression coefficients /3, and fn .  . . . . . . . . . . . . . . . .  420 1991, completing this arduous project five years and almost six hundred 

pages later. This book is considerably longer than we had originally planned, 
but we have finally overcome the temptation to include just one more new 
topic, and have put our pens to rest. Of course, the academic literature has 
evolved rapidly while we have been writing, and it continues to do so even 
as this book goes to press. We have attempted to provide broad coverage, 
but even so, there are many subjects that we do not touch upon, and many 
others that we can only mention in passing. 

We owe many more debts-personal and intellectual-than we can pos- 
sibly acknowledge. Throughout our professional careers our colleagues and 
mentors have offered us advice, debate, inspiration, and friendship; we wish 
to thank in particular Andy Abel, Ben Bernanke, Steve Cecchetti, John Cox, 
Angus Deaton, Gene Fama, Bruce Grundy, Jerry Hausman, Chi-fu Huang, 
Mervyn King, Nobu Kiyotaki, Pete Kyle, Greg Mankiw, Bob Merton, Whitney 
Newey, Bob Shiller,J im Tobin, and Arnold Zellner. 

xvii 



xviii Preface 

Many individuals have also provided us with invaluable comments and 
discussions regarding the contents and exposition of this book. We thank 
David Backus, Nick Barberis, David Barr, Dimitris Bertsimas, Tim Bollerslev, The Econometrics of Financial Markets 
George Constantinides, John Cox, Xavier Gabaix, Lars Hansen, Campbell 
Harvey, John Heaton, Ludger Hentschel, Roger Huang, Ravi Jagannathan, 
Gautam Kaul, Dan Nelson, Amlan Roy, Bob Shiller, Tom Stoker, Jean-Luc 
Vila, Jiang Wang, and the Ph.D. students at Harvard, MIT, Princeton, and 
Wharton on whom this material was "test-marketed" and refined. 

We have relied heavily on the able research assistance of Petr Adamek, 
Sangjoon Kim, Martin Lettau, Terence Lim, Constantin Petrov, Chunsheng 
Zhou, and particularly Matt Van Vlack and Luis Viceira, who undertook the 
difficult tasks of proofreading the manuscript and preparing the index. 

We are grateful to Stephanie H o p e  for her great skill and care in 
preparing the electronic version of this manuscript, and the typesetters at 
Archetype for producing the final version of the book. 

We thank Peter Dougherty, our editor at Princeton University Press, for 
his patience, encouragement, and support throughout this project. 

Several organizations provided us with generous support during var- 
ious stages of this book's gestation; in particular, we thank Batterymarch 
Financial Management, the National Bureau of Economic Research, the 
National Science Foundation, the John M. Olin Foundation, the Alfred P. 
Sloan Foundation, and research centers at Harvard, MIT, Princeton, and 
Wharton. 

And finally, we owe more than we can say to the support and love of our 
families. 

JYC 
AWL 
ACM 



Introduction 

FINANCIAL ECONOMICS is a highly empirical discipline, perhaps the most 
empirical among the branches of economics and even among the social 
sciences in general. This should come as no surprise, for financial markets 
are not mere figments of theoretical abstraction; they thrive in practice 
and play a crucial role in the stability and growth of the global economy. 
Therefore, although some aspects of the academic finance literature may 
seem abstract at first, there is a practical relevance demanded of financial 
models that is often waived for the models of other comparable disciplines.' 

Despite the empirical nature of financial economics, like the other so- 
cial sciences it is almost entirely nonexperimental. Therefore, the primary 
method of inference for the financial economist is model-based statistical 
inference-financial econometrics. While econometrics is also essential in 
other branches of economics, what distinguishes financial economics is the 
central role that uncertainty plays in both financial theory and its empirical 
implementation. The starting point for every financial model is the uncer- 
tainty facing investors, and the substance of every financial model involves 
the impact of uncertainty on the behavior of investors and, ultimately, on 
market prices. Indeed, in the absence of uncertainty, the problems of fi- 
nancial economics reduce to exercises in basic microeconomics. The very 
existence of financial economics as a discipline is predicated on uncertainty. 

This has important consequences for financial econometrics. The ran- 
dom fluctuations that require the use of statistical theory to estimate and test 
financial models are intimately related to the uncertainty on which those 
models are based. For example, the martingale model for asset prices has 
very specific implications for the behavior of test statistics such as the au- 
tocorrelation coefficient of price increments (see Chapter 2). This close 
connection between theory and empirical analysis is unparalleled in the 

 ernst stein (1992) provides a highly readable account of the interplay between theory and 
practice in the development of modern financial economics. 



4 1. Introduction I . 1 .  Organization o f  the Book 5 

social sciences, although it has been the hallmark of the natural sciences ties (Chapters 10 and 11). The last chapter of the book presents nonlinear 
for quite some time. It is one of the most rewarding aspects of financial methods, with applications to both stocks and derivatives. 
econometrics, so much so that we felt impelled to write this graduate-level Second, we start by presenting statistical models of asset returns, and 
textbook as a means of introducing others to this exciting field. then discuss more highly structured economic models. In Chapter 2, for 

Section 1.1 explains which topics we cover in this book, and howwe have example, we discuss methods for predicting stock returns from their own 
organized the material. We also suggest some ways in which the book might past history, without much attention to institutional detail; in Chapter 3 we 
be used in a one-semester course on financial econometrics or empirical show how the microstructure of stock markets affects the short-run behavior 
finance. of returns. Similarly, in Chapter 4 we discuss simple statistical models of the 

In Section 1.2, we describe the kinds of background material that are cross-section of individual stock returns, and the application of these models 
most useful for financial econometrics and suggest references for those to event studies; in Chapters 5 and 6 we show how the Capital Asset Pricing 
readers who wish to review or learn such material along the way. In our Model and multifactor models such as the Arbitrage Pricing Theory restrict 
experience, students are often more highly motivated to pick up the nec- the parameters of the statistical models. In Chapter 7 we discuss longer-run 
essary background after they see how it is to be applied, so we encourage evidence on the predictability of stock returns from variables other than 
readers with a serious interest in financial econometrics but with somewhat past stock returns; in Chapter 8 we explore dynamic equilibrium models 
less preparation to take a crack at this material anyway. which can generate persistent time-variation in expected returns. We use 

In a book of this magnitude, notation becomes a nontrivial challenge the same principle to divide a basic treatment of fixed-income securities 
of coordination; hence Section 1.3 describes what method there is in our in Chapter 10 from a discussion of equilibrium term-structure models in 
notational madness. We urge readers to review this carefully to minimize Chapter 11. 
the confusion that can arise when 6 is mistaken for B and X is incorrectly We have tried to make each chapter as selfcontained as possible. While 
assumed to be the same as X. some chapters naturally go together (e.g., Chapters 5 and 6, and Chapters 

Section 1.4 extends our discussion of notation by presenting notational 10 and l l ) ,  there is certainly no need to read this book straight through 
conventions for and definitions of some of the fundamental objects of our from beginning to end. For classroom use, most teachers will find that there 
study: prices, returns, methods of compounding, and probability distribu- is too much material here to be covered in one semester. There are several 
tions. Although much of this material is well-known to finance students and ways to use the book in a one-semester course. For example one teacher 
investment professionals, we think a brief review will help many readers. might start by discussing short-run time-series behavior of stock prices using 

In Section 1.5, we turn our attention to quite a different subject: the Chapters 2 and 3, then cover cross-sectional models in Chapters 4,5, and 6, 
Efficient Markets Hypothesis. Because so much attention has been lavished then discuss intertemporal equilibrium models using Chapter 8, and finally 
on this hypothesis, often at the expense of other more substantive issues, cover derivative securities and nonlinear methods as advanced topics using 
we wish to dispense with this issue first. Much of the debate involves theo- Chapters 9 and 12. Another teacher might first present the evidence on 
logical tenets that are empirically undecidable and, therefore, beyond the short- and long-run predictability of stock returns using Chapters 2 and 7, 
purview of this text. But for completeness-no self-respecting finance text then discuss static and intertemporal equilibrium theory using Chapters 5, 
could omit market efficiency altogether-Section 1.5 briefly discusses the 6, and 8, and finally cover fixed-income securities using Chapters 10 and 11. 
topic. There are some important topics that we have not been able to include 

in this text. Most obviously, our focus is almost exclusively on US domestic 
asset markets. We say very little about asset markets in other countries, and 
we do not try to cover international topics such as exchange-rate behav- 

1.1 Organization of the Book ior or the home-bias puzzle (the tendency for each country's investors to 
hold a disproportionate share of their own country's assets in their portfo- 

In organizing this book, we have followed two general principles. First, the lios). We also omit such important econometric subjects as Bayesian analysis 
early chapters concentrate exclusively on stock markets. Although many of and frequencydomain methods of timeseries analysis. In many cases our 
the methods discussed can be applied equally well to other asset markets, the choice of topics has been influenced by the dual objectives of the book: 
empirical literature on stock markets is particularly large and by focusing on to explain the methods of financial econometrics, and to review the em- 
these markets we are able to keep the discussion concrete. In later chapters, pirical literature in finance. We have tended to concentrate on topics that 
we cover derivative securities (Chapters 9 and 12) and fixed-income securi- 



6 1. Introduction 1.2. Useful Background 7 

involve econometric issues, sometimes at the expense of other equally inter- intuition and subtleties of elementary probabilistic reasoning. An amaz- 
esting material-including much recent work in behavioral finance-that ingly durable classic that takes just this approach is Feller (1968). Brieman 
is econometrically more straightforward. (1992) provides similar intuition but at a measure-theoretic level. Key con- 

cepts include 

0 definition of a random variable 
1.2 Useful Background 

0 independence 
0 distribution and density functions 

The many rewards of financial econometrics come at a price. A solid back- conditional probability 
ground in mathematics, probability and statistics, and finance theory is nec- 

0 modes of convergence 
essary for the practicing financial econometrician, for precisely the reasons 

0 laws of large numbers 
that make financial econometrics such an engaging endeavor. To assist 

0 central limit theorems. 
readers in obtaining this background (since only the most focused and di- 
rected of students will have it already), we outline in this section the topics Statistics is, of course, the primary engine which drives the inferences 
in mathematics, probability, statistics, and finance theory that have become that financial econometricians draw from the data. As with probability the- 
indispensable to financial econometrics. We hope that this outline can serve ory, statistics can be taught at various levels of mathematical sophistication. 
as a self-study guide for the more enterprising readers and that it will be a Moreover, unlike the narrower (and some would say "purer") focus of proba- 
partial substitute for including background material in this book. bility theory, statistics has increased its breadth as it has matured, giving birth 

to many well-defined subdisciplines such as multivariate analysis, nonpara- 
metrics, time-series analysis, order statistics, analysis of variance, decision 

1.2.1 Mathematics Background theory, Bayesian statistics, etc. Each of these subdisciplines has been drawn 
The mathematics background most useful for financial econometrics is not upon by financial econometricians at one time or another, making it rather 
unlike the background necessary for econometrics in general: multivariate difficult to provide a single reference for all of these topics. Amazingly, 
calculus, linear algebra, and matrix analysis. References for each of these such a reference does exist: Stuart and Ord's (1987) three-volume tour de 
topics are Lang (l973), Strang (l976),a nd Magnus and Neudecker (l988), force. A more compact reference that contains most of the relevant material 
respectively. Key concepts include for our purposes is the elegant monograph by Silvey (1975). For topics in 

time-series analysis, Hamilton (1994) is an excellent comprehensive text. 
e multiple integration Key concepts include 
0 multivariate constrained optimization 
0 matrix algebra 0 Neyman-Pearson hypothesis testing 
0 basic rules of matrix differentiation. 0 linear regression 

0 maximum likelihood 
In addition, option- and other derivative-pricing models, and continuous- basic time-series analysis (stationarity, autoregressive and ARMA pro- 
time asset pricing models, require some passing familiarity with the It6 or cesses, vector autoregressions, unit roots, etc.) 
stochastic calculus. A lucid and thorough treatment is provided by Merton 0 elementary Bayesian inference. 
(1990), who pioneered the application of stochastic calculus to financial 
economics. More mathematically inclined readers may also wish to consult For continuous-time financial models, an additional dose of stochastic pro- 
Chung and Williams (1990). cesses is a must, at least at the level of Cox and Miller (1965) and Hoel, Port, 

and Stone (1972). 

1.2.2 Probability and Statistics Background 
1.2.3 Finance Themy Background 

Basic probability theory is a prerequisite for any discipline in which uncer- 
tainty is involved. Although probability theory has varying degrees of mathe- Since the raison d2tre of financial econometrics is the empirical implemen- 
matical sophistication, from coin-flipping calculations to measure-theoretic tation and evaluation of financial models, a solid background in finance 
foundations, perhaps the most useful approach is one that emphasizes the theory is the most important of all. Several texts provide excellent coverage 



8 1. Introduction 1.4. Prices, Returns, and Compounding 9 

of this material: Duffie (l992), Huang and Litzenberger (l988), Ingersoll i for asset subscripts; k ,  m, and n for lead and lag subscripts; and j as a 
(1987), and Merton (1990). Key concepts include generic subscript. 

0 We use the timing convention that a variable is dated t if it is known by 
0 risk aversion and expected-utility theory the end of period t .  Thus R, denotes a return on an asset held from the 
0 static mean-variance portfolio theory end of period t-1 to the end of period t .  
0 the Capital Asset Pricing Model (CAPM) and the Arbitrage Pricing The- 0 In writing variancecovariance matrices, we use i-2 for the variance- 

ory (APT) covariance matrix of asset returns, I= for the variancecovariance matrix 
0 dynamic asset pricing models of residuals from a time-series or cross-sectional model, and V for the 
0 option pricing theory. variancecovariance matrix of parameter estimators. 

0 We use script letters sparingly. N denotes the normal distribution, and 
L denotes a log likelihood function. 

1.3 Notation 0 We use Pr(.) to denote the probability of an event. 

We have found that it is far from simple to devise a consistent notational The professional literature uses many specialized terms. Inevitably we 
scheme for a book of this scope. The difficulty comes from the fact that also use these frequently, and we italicize them when they first appear in the 
financial econometrics spans several very different strands of the finance book. 
literature, each replete with its own firmly established set of notational 
conventions. But the conventions in one literature often conflict with the 1.4 Prices, Returns, and Compounding 
conventions in another. Unavoidably, then, we must sacrifice either inter- 
nal notational consistency across different chapters of this text or external Virtually every aspect of financial economics involves returns, and there are at 
consistency with the notation used in the professional literature. We have least two reasons for focusing our attention on returns rather than on prices. 
chosen the former as the lesser evil, but we do maintain the following con- First, for the average investor, financial markets may be considered close to 
ventions throughout the book: perfectly competitive, so that the size of the investment does not affect price 

changes. Therefore, since the investment "technology" is constant-returns- 
0 We use boldface for vectors and matrices, and regular face for scalars. to-scale, the return is a complete and scale-free summary of the investment 

Where possible, we use bold uppercase for matrices and bold lowercase opportunity. 
for vectors. Thus x is a vector while X is a matrix. Second, for theoretical and empirical reasons that will become apparent 

0 Where possible, we use uppercase letters for the levels of variables and below, returns have more attractive statistical properties than prices, such 
lowercase letters for the natural logarithms (logs) of the same variables. as stationarity and ergodicity. In particular, dynamic general-equilibrium 
Thus if P is an asset price, p is the log asset price. models often yield nonstationary prices, but stationary returns (see, for 

0 Our standard notation for an innovation is the Greek letter E .  Where example, Chapter 8 and Lucas [I9781)  . 
we need to define several different innovations, we use the alternative 
Greek letters r ] ,  6 ,  and {. 

0 Where possible, we use Greek letters to denote parameters or parameter 1.4.1 Definitions and Conuentions 
vectors. 

Denote by Pt the price of an asset at date t and assume for now that this asset 
0 We use the Greek letter L to denote a vector of ones. 

pays no dividends. The simple net return, Rt,o n the asset between dates t - 1 
0 We use hats to denote sample estimates, so if B is a parameter, 6 is an 

and t is defined as 
estimate of B.  

0 When we use subscripts, we always use uppercase letters for the upper 
limits of the subscripts. Where possible, we use the same letters for 
upper limits as for the subsc'ripts themselves. Thus subscript t runs 
from 1 to T, subscript k runs from 1 to K, and so on. An exception is The simplegross return on the asset isjust one plus the net return, 1 + &. 
that we will let subscript i (usually denoting an asset) run from 1 to N From this definition it is apparent that the asset's gross return over the 
because this notation is so common. We use t and t for time subscripts; most recent k periods from date t - k to date t ,  written 1 + & ( k ) ,  is simply 



10 1. Introduction 1.4. Prices, Returns, and Compounding 11 

equal to the product of the k single-period returns from t - k + 1 to t, i.e., Continuous Compounding 
The difficulty of manipulating geometric averages such as (1.4.3) motivates 
another approach to compound returns, one which is not approximate and 
also has important implications for modeling asset returns; this is the notion 
of continuous compounding. The continuously compounded return or logreturn 
rt of an asset is defined to be the natural logarithm of its gross return (1 +I?,): 

and its net return over the most recent k periods, written Rt(k), is simply 
equal to its k-period gross return minus one. These multiperiod returns are pt 

rt - log(1 + Rt) = log - = pt - P,-~, (1.4.5) 
called compound returns. Pt- 1 

Although returns are scale-free, it should be emphasized that they are where pt = log Pt. When we wish to emphasize the distinction between Rt 
not unitless, but are always defined with respect to some time interval, e.g., and rt, we shall refer to Rt as a simple return. Our notation here deviates 
one "period." In fact, & is more properly called a rate of return, which is slightly from our convention that lowercase letters denote the logs of up- 
more cumbersome terminology but more accurate in referring to as a rate percase letters, since here we have rt = log(1 + &) rather than log(&); we 
or, in economic jargon, a flow variable. Therefore, a return of 20% is not do this to maintain consistency with standard conventions. 
a complete description of the investment opportunity without specification The advantages of continuously compounded returns become clear 
of the return horizon. In the academic literature, the return horizon is when we consider multiperiod returns, since 
generally given explicitly, often as part of the data description, e.g., "The 
CRSP monthly returns file was used." 

However, among practitioners and in the financial press, a return- 
horizon of one year is usually assumed implicitly; hence, unless stated oth- 
erwise, a return of 20% is generally taken to mean an annual return of 20%. 
Moreover, multiyear returns are often annualized to make investments with 
different horizons comparable, thus: and hence the continuously compounded multiperiod return is simply the 

sum of continuously compounded single-period returns. Compounding, 
a multiplicative operation, is converted to an additive operation by taking 
logarithms. However, the simplification is not merely in reducing multi- 
plication to addition (since we argued above that with modern calculators 

Since single-period returns are generally small in magnitude, the follow- and computers, this is trivial), but more in the modeling of the statistical 
ing approximation based on a first-order Taylor expansion is often used to behavior of asset returns over time-it is far easier to derive the time-series 
annualize multiyear returns: properties of additive processes than of multiplicative processes, as we shall 

see in Chapter 2. 
Continuously compounded returns do have one disadvantage. The sim- 

ple return on a portfolio of assets is a weighted average of the simple returns 
on the assets themselves, where the weight on each asset is the share of the 
portfolio's value invested in that asset. If portfolio p places weight wip in as- 

Whether such an approximation is adequate depends on the particular set i, then the return on the portfolio at time t, Rpt, is related to the returns 
application at hand; it may suffice for a quick and coarse comparison of 
investment performance across many assets, but for finer calculations in on individual assets, &,, i = 1 . . . N, by Rp, = zE1w ipKt. Unfortunately 

which the volatility of returns plays an important role, i.e., when the higher- continuously compounded returns do not share this convenient property. 
Since the log of a sum is not the same as the sum of logs, rptd oes not equal 

order terms in the Taylor expansion are not negligible, the approximation 
(1.4.4) may break down. The only advantage of such an approximation is wiprit 
convenience-it is easier to calculate an arithmetic rather than a geomet- In empirical applications this problem is usually minor. When returns 
ric average-however, this advantage has diminished considerably with the are measured over short intervals of time, and are therefore close to zero, 
advent of cheap and convenient computing power. the continuously compounded return on a portfolio is close to the weighted 



1. Introduction 1.4. Prices, Returns, and Compounding 13 

simple excess return on asset i is 

Zit = R z t  - R o t >  (1.4.8) 

where is the reference return. Alternatively one can define a log excess 
return as 

Figure 1.1. Dividad Payment Timing Convention 
.zit = rzt-  rot. (1.4.9) 

The excess return can also be thought of as the payoff on an arbitrage 
portfolio that goes long in asset i and short in the reference asset, with no 

average of the continuously compounded returns on the individual assets: net investment at the initial date. Since the initial net investment is zero, 

rp, zE1w iprit.' We use this approximation in Chapter 3. Nonetheless the return on the arbitrage portfolio is undefined but its dollar payoff is 
it is common to use simple returns when a cross-section of assets is being proportional to the excess return as defined above. 
studied, as in Chapters 4-6, and continuously compounded returns when 
the temporal behavior of returns is the focus of interest, as in Chapters 2 1.4.2 The Margznal, Conditional, andJ oint Distribution of Returns 
and 7. 

Having defined asset returns carefully, we can now begin to study their 
Dividend Payments behavior across assets and over time. Perhaps the most important charac- 
For assets which make periodic dividend payments, we must modify our teristic of asset returns is their randomness. The return of IBM stock over 
definitions of returns and compounding. Denote by Dt the asset's dividend the next month is unknown today, and it is largely the explicit modeling 
payment at date t and assume, purely as a matter of convention, that this of the sources and nature of this uncertainty that distinguishes financial 
dividend is paid just before the date-t price Pt is recorded; hence P, is taken economics from other social sciences. Although other branches of eco- 
to be the ex-dividend price at date t. Alternatively, one might describe Pt as nomics and sociology do have models of stochastic phenomena, in none 
an end-of-period asset price, as shown in Figure 1.1. Then the net simple of them does uncertainty play so central a role as in the pricing of finan- 
return at date t may be defined as cial assets-without uncertainty, much of the financial economics literature, 

both theoretical and empirical, would be superfluous. Therefore, we must 
articulate at the very start the types of uncertainty that asset returns might 
exhibit. 

Multiperiod and continuously compounded returns may be obtained The Joint Distribution 
in the same way as in the no-dividends case. Note that the continuously Consider a collection of N assets at date t, each with return Gt at date t, 
compounded return on a dividend-paying asset, rt = log(Pt+ Dt) -10g(P,-~), where t = 1, . . . , T. Perhaps the most general model of the collection of 
is a nonlinear function of log prices and log dividends. When the ratio returns { e t ]is  its joint distribution function: 
of prices to dividends is not too variable, however, this function can be 
approximated by a linear function of log prices and dividends, as discussed 
in detail in Chapter 7. 

where x is a vector of state variables, variables that summarize the economic 
environment in which asset returns are determined, and 8 is a vector of 

Excess Returns fixed parameters that uniquely determines G. For notational convenience, 
It is often convenient to work with an asset's excess return, defined as the we shall suppress the dependence of G on the parameters 8 unless it is 
difference between the asset's return and the return on some reference needed. 
asset. The reference asset is often assumed to be riskless and in practice is The probability law G governs the stochastic behavior of asset returns 
usually a short-term Treasury bill return. Working with simple returns, the and x, and represents the sum total of all knowable information about them. 

We may then view financial econometrics as the statistical inference of 8, 
'1n the limit where time is continuous, Ito's Lemma, discussed in Section 9.1.2o f Chapter9, given G and realizations of {R,). Of course, (1.4.10) is far too general to 

can be used to relate simple and continuously compounded returns. 



14 1. Introduction 1.4. Prices, Returns, and Compounding 15 

be of any use for statistical inference, and we shall have to place further tion that is relevant for issues involving predictability. However, the proper- 
restrictions on G in the coming sections and chapters. However, (1.4.10) ties of the unconditional distribution of returns may still be of some interest, 
does serve as a convenient way to organize the many models of asset re- especially in cases where we expec t  predictability to be minimal. 
turns to be developed here and in later chapters. For example, Chapters 2 One of the most common models for asset returns is the temporally 
through 6 deal exclusively with the joint distribution of { e t ) ,le aving addi- independently and identically distributed (IID) normal model, in which 
tional state variables x to be considered in Chapters 7 and 8. We write this returns are assumed to be independent over time (although perhaps cross- 
joint distribution as GR. sectionally correlated), identically distributed over time, and normally dis- 

Many asset pricing models, such as the Capital Asset Pricing Model tributed. The original formulation of the CAPM employed this assumption 
(CAPM) of Sharpe (1964), Lintner (1965a, b), and Mossin (1966) consid- of normality, although returns were only implicitly assumed to be tempo- 
ered in Chapter 5, describe the joint distribution of the cross section of re- rally IID (since it was a static "two-period" model). More recently, models 
turns {Rlt., . . , RNt)a t a single date t. To reduce (1.4.10) to this essentially of asymmetric information such as Grossman (1989) and Grossman and 
static structure, we shall have to assert that returns are statistically indepen- Stiglitz (1980) also use normality. 
dent through time and that the joint distribution of the cross-section of While the temporally IID normal model may be tractable, it suffers from 
returns is identical across time. Although such assumptions seem extreme, at least two important drawbacks. First, most financial assets exhibit limited 
they yield a rich set of implications for pricing financial assets. The CAPM, liability, so that the largest loss an investor can realize is his total investment 
for example, delivers an explicit formula for the trade-off between risk and and no more. This implies that the smallest net return achievable is -1 
expected return, the celebrated security market line. or -100%. But since the normal distribution's support is the entire real 

line, this lower bound of -1 is clearly violated by normality. Of course, it 
The Conditional Distribution may be argued that by choosing the mean and variance appropriately, the 
In Chapter 2, we place another set of restrictions on GR which will allow us probability of realizations below -1 can be made arbitrarily small; however 
to focus on the dynamics of individual asset returns while abstracting from it will never be zero, as limited liability requires. 
cross-sectional relations between the assets. In particular, consider the joint Second, if single-period returns are assumed to be normal, then multi- 
distribution F of . . . , RT}f or a given asset i, and observe that we may period returns cannot also be normal since they are the products of the single- 
always rewrite F as the following product: period returns. Now the sums of normal single-period returns are indeed 

normal, but the sum of single-period simple returns does not have any eco- 
nomically meaningful interpretation. However, as we saw in Section 1.4.1, 
the sum of single-period continuously compounded returns does have a 
meaningful interpretation as a multiperiod continuously compounded re- 

From (1.4.11), the temporal dependencies implicit in {fit)a re apparent. turn. 
Issues of predictability in asset returns involve aspects of their conditional 
distributions and, in particular, how the conditional distributions evolve The Lognormal Distribution 
through time. A sensible alternative is to assume that continuously compounded single- 

By placing further restrictions on the conditional distributions F, , ( . ) ,  we period returns rit are IID normal, which implies that single-period 
shall be able to estimate the parameters 8 implicit in (1.4.11 ) and exam- gross simple returns are distributed as IID l o g n m a l  variates, since ri, = 
ine the predictability of asset returns explicitly. For example, one version log(1 + &,). We may express the lognormal model then as 
of the random-walk hypothesis is obtained by the restriction that the con- 
ditional distribution of return &, is equal to its marginal distribution, i.e., 
F,,(&, I . ) = F,,(&,) .  If this is the case, then returns are temporally indepen- 
dent and therefore unpredictable using past returns. Weaker versions of the Under the lognormal model, if the mean and variance of ri, are pi and o?, 

random walk are obtained by imposing weaker restrictions on I . I .  respectively, then the mean and variance of simple returns are given by 

The Unconditional Distribution 
In cases where an asset return's conditional distribution differs from its 
marginal or unconditional distribution, it is clearly the conditional distribu- 



16 1. Introduction 1.4. Prices, Returns, and Compounding 

Alternatively, if we assume that the mean and variance of simple returns 4, the sample variance 
are mi and sf, respectively, then under the lognormal model the mean and 
variance of rit are given by 

the sample skewness 

and the sample kurtosis 

The lognormal model has the added advantage of not violating limited 
liability, since limited liability yields a lower bound of zero on (1 + &), 
which is satisfied by (1 + &) = ecf when rit is assumed to be normal. 

The lognormal model has a long and illustrious history, beginning with In large samples of normally distributed data, the estimators and K are 
the dissertation of the French mathematician Louis Bachelier (1900), which normally distributed with means 0 and 3 and variances 6/ T and 241T , 
contained the mathematics of Brownian motion and heat conduction, five respectively (see Stuart and Ord [1987, Vol. 11). Since 3 is the kurtosis of the 
years prior to Einstein's (1905) famous paper. For other reasons that will be- normal distribution, sample excess kurtosis is defined to be sample kurtosis 
come apparent in later chapters (see, especially, Chapter g), the lognormal less 3. Sample estimates of skewness for daily US stock returns tend to be 
model has become the workhorse of the financial asset pricing literature. negative for stock indexes but close to zero or positive for individual stocks. 

But as attractive as the lognormal model is, it is not consistent with all the Sample estimates of excess kurtosis for daily US stock returns are large and 
properties of historical stock returns. At short horizons, historical returns positive for both indexes and individual stocks, indicating that returns have 
show weak evidence of skewness and strong evidence of excess kurtosis. The more mass in the tail areas than would be predicted by a normal distribution. 
skewness, or normalized third moment, of a random variable r with mean p 
and variance a2i s defined by Stable Distributions 

Early studies of stock market returns attempted to capture this excess kur- 
tosis by modeling the distribution of continuously compounded returns as 
a member of the stable class (also called the stable Pareto-Lhy or stable Pare- 
tian), of which the normal is a special case.3 The stable distributions are a 

The kurtosis, or normalized fourth moment, of r is defined by natural generalization of the normal in that, as their name suggests, they are 
stable under addition, i.e., a sum of stable random variables is also a stable 
random variable. However, nonnormal stable distributions have more prob- 
ability mass in the tail areas than the normal. In fact, the nonnormal stable 
distributions are so fat-tailed that their variance and all higher moments are 

The normal distribution has skewness equal to zero, as do all other sym- infinite. Sample estimates of variance or kurtosis for random variables with 
metric distributions. The normal distribution has kurtosis equal to 3, but 
fat-tailed distributions with extra probability mass in the tail areas have higher  he French probabilist Paul LKvy (1924) was perhaps the first to initiate a general investi- 
or even infinite kurtosis. gation of stable distributions and provided a complete characterization of them through their 

Skewness and kurtosis can be estimated in a sample of data by construct- log-characteristic functions (see below). LCvy (1925) also showed that the tail probabilities 
of stable distributions approximate those of the Pareto distribution, hence the term "stable 

ing the obvious sample averages: the sample mean Pareto-Gvy" or "stable Paretian" distribution. For applications to financial asset returns, see 
Blattberg and Gonedes (1974);F ama (1965);F ama and Roll (1971); Fielitz (1976); Fielitz and 
Rozell(1983);G ranger and Morgenstern (1970);H agerman (1978);H su, Miller, and Wichern 
(1974);  Mandelbrot (1963);  Mandelbrot and Taylor (1967);O fficer (1972);S amuelson (1967, 
1976); Simkowitz and Beedles (1980);a nd Tucker (1992). 



I .  Introduction 1.4. Prices, Returns, and Compounding 19 

almost always requires finite second moments of returns, and often finite 
higher moments as well. Stable distributions also have some counterfac- 
tual implications. First, they imply that sample estimates of the variance 
and higher moments of returns will tend to increase as the sample size in- 
creases, whereas in practice these estimates seem to converge. Second, they 
imply that long-horizon returns will be just as non-normal as short-horizon 
returns (since long-horizon returns are sums of short-horizon returns, and 
these distributions are stable under addition). In practice the evidence 
for non-normality is much weaker for long-horizon returns than for short- 
horizon returns. 

Recent research tends instead to model returns as drawn from a fat- 
tailed distribution with finite higher moments, such as the t distribution, 
or as drawn from a mixture of distributions. For example the return might 
be conditionally normal, conditional on a variance parameter which is itself 

Figurn 1.2. Comparison of Stable and Nonnal Density Functions random; then the unconditional distribution of returns is a mixture of nor- 
mal distributions, some with small conditional variances that concentrate 
mass around the mean and others with large conditional variances that put 
mass in the tails of the distribution. The result is a fat-tailed unconditional 

these distributions will not converge as the sample size increases, but will distribution with a finite variance and finite higher moments. Since all 
tend to increase indefinitely. moments are finite, the Central Limit Theorem applies and long-horizon 

Closed-form expressions for the density functions of stable random vari- returns will tend to be closer to the normal distribution than short-horizon 
ables are available for only three special cases: the normal, the Cauchy, and returns. It is natural to model the conditional variance as a time-series 
the Bernoulli cases.4 Figure 1.2 illustrates the Cauchy distribution, with process, and we discuss this in detail in Chapter 12. 
density function 

An Empirical Illustration 
Table 1.1 contains some sample statistics for individual and aggregate stock 

In Figure 1.2, (1.4.23) is graphed with parameters S = 0 and y = 1, and it returns from the Center for Research in Securities Prices (CRSP) for 1962 
is apparent from the comparison with the normal density function (dashed to 1994 which illustrate some of the issues discussed in the previous sec- 
lines) that the Cauchy has fatter tails than the normal. tions. Sample moments, calculated in the straightforward way described 

Although stable distributions were popular in the 1960's and early 1970's, in (1.4.19)-(1.4.22), are reported for value- and equal-weighted indexes 
they are less commonly used today. They have fallen out of favor partly be- of stocks listed on the New York Stock Exchange (NYSE) and American 
cause they make theoretical modelling so difficult; standard finance theory Stock Exchange (AMEX), and for ten individual stocks. The individual 

stocks were selected from marketcapitalization deciles using 1979 end-of- 
year market capitalizations for all stocks in the CRSP NYSE/AMEX universe, 

4HoweverL, kvy (1925) derived the following explicit expression for the logarithm of the 
characteristic function p(t) of any stable random variable X: logp(t) s 1 0 ~ ~ [ e '=' ~ G] t  - where International Business Machines is the largest decile's representative 
yItla[l - @sgn(t)t an(a~/2)]w,  here (a, B, 6, y )  are the four parameters that characterize and Continental Materials Corp. is the smallest decile's representative. 
each stable distribution. 6 E (-w, co) is said to be the location parameter, ,9 E (-co, co) is the Panel A reports statistics for daily returns. The daily index returns have 
skewness index, y E (0, co) is the scale parameter, and LY E (0,2] is the exponent. When a = 2, extremely high sample excess kurtosis, 34.9 and 26.0 respectively, a clear 
the stable distribution reduces to a normal. As a decreases from 2 to 0, the tail areas of the 
stable distribution become increasingly "fatter" than the normal. When a E (1,2), the stable sign of fat tails. Although the excess kurtosis estimates for daily individual 
distribution has a finite mean given by 6, but when LY E (0, I], even the mean is infinite. The stock returns are generally less than those for the indexes, they are still large, 
parameter B measures the symmetry of the stable distribution; when b = 0 the distribution is ranging from 3.35 to 59.4. Since there are 8179 observations, the standard 
symmetric, and when B > 0 (or p < 0) the distribution is skewed to the right (or left). When error for the kurtosis estimate under the null hypothesis of normality is 
B = 0 and a = 1 we have the Cauchy distribution, and when a = 112, p = 1 , 6  = 0, and y = 1 
we have the Bernoulli distribution. J24/8179 = 0.054, so these estimates of excess kurtosis are overwhelmingly 



20 I. Introduction 1.5. Market Efjcienqy 21 

statistically significant. The skewness estimates are negative for the daily Table 1.1. Stock market returns, 1962 to 1994. 
index returns, -1.33 and -0.93 respectively, but generally positive for the 
individual stock returns, ranging from -0.18 to 2.25. Many of the skewness Standard Excess 
estimates are also statistically significant as the standard error under the null Security Mean Deviation Skewness Kurtosis Minimum Maximum 

hypothesis of normality is ,/m=  0.027. Panel A: Daily Returns 
Panel B reports sample statistics for monthly returns. These are con- 

Value-Weighted Index 
siderably less leptokurtic than daily returns-the value- and equal-weighted Equal-Weighted Index 
CRSP monthly index returns have excess kurtosis of only 2.42 and 4.14, re- International Business 
spectively, an order of magnitude smaller than the excess kurtosis of daily Machines 
returns. As there are only 390 observations the standard error for the kurto- General Signal Corp. 
sis estimate is also much larger, 0.248. This is one piece of evidence that has Wrigley Co. 
led researchers to use fat-tailed distributions with finite higher moments, for Interlake Corp. 
which the Central Limit Theorem applies and drives longer-horizon returns Raytech Corp. 
towards normality. Ampco-Pittsburgh Corp. 

Energen Corp. 
General Host Corp. 

1.5 Market Efficiency Garan Inc. 
Continental Materials Corp. 

The origins of the Efficient Markets Hypothesis (EMH) can be traced back Panel B: Monthly Returns 

at least as far as the pioneering theoretical contribution of Bachelier (1900) Value-Weighted Index 
and the empirical research of Cowles (1933). The modern literature in eco- Equal-Weighted Index 
nomics begins with Samuelson (1965), whose contribution is neatly sum- , International Business 
marized by the title of his article: "Proof that Properly Anticipated Prices Machines 

Fluctuate R a n d ~ m l ~I"n. ~an  informationally efficient market-not to be General Signal Corp. 

confused with an allocationally or Paretoefficient market-price changes Wrigley Co. 

must be unforecastable if they are properly anticipated, i.e., if they fully Interlake Corp. 

incorporate the expectations and information of all market participants. Raytech Corp. 

Fama (1970) summarizes this idea in his classic survey by writing: "A Ampco-Pittsburgh Corp. 
Energen Corp. 

market in which prices always 'fully reflect' available information is called 
General Host Corp. 

'efficient'." Fama's use of quotation marks around the words "fully reflect" 
Garan Inc. 

indicates that these words are a form of shorthand and need to be explained 
Continental Materials Corp. 

more fully. More recently, Malkiel (1992) has offered the following more 
explicit definition: Summary statistics for daily and monthly returns (in percent) of CRSP equal- and value- 

weighted stock indexes and ten individual securities continuously listed over the entire sample 
A capital market is said to be efficient if it fully and correctly reflects period from July 3,1962 to December 30, 1994. Individual securities are selected to represent 
all relevant information in determining security prices. Formally, the stock. in each size decile. Statistics are defined in (1.4.19)-(1.4.22). 

market is said to be efficient with respect to some information set . . . if 
security prices would be unaffected by revealing that information to all . . . implies that it is impossible to make economic profits by trading on 
participants. Moreover, efficiency with respect to an information set the basis of [that information set]. 

Malkiel's first sentence repeats Fama's definition. His second and third sen- 
SBernstein (1992) discusses the contributions of Bachelier, Cowles, Samuelson, and many tences expand the definition in two alternative ways. The second sentence 

other early authors. The articles reprinted in Lo (1996) include some of the most important 
papers in this literature. suggests that market efficiency can be tested by revealing information to 



22 1. Introduction 1.5. Market Efficienq 23 

market participants and measuring the reaction of security prices. If prices be smooth rather than random. Black (1971) has attacked this idea rather 
do not move when information is revealed, then the market is efficient with effectively: 
respect to that information. Although this is clear conceptually, it is hard to A perfect market for a stock is one in which there are no profits to 
carry out such a test in practice (except perhaps in a laboratory). be made by people who have no special information about the com- 

Malkiel's third sentence suggests an alternative way to judge the effi- pany, and in which it is difficult even for people who do have special 
ciency of a market, by measuring the profits that can be made by trading on information to make profits, because the price adjusts so rapidly as the 
information. This idea is the foundation of almost all the empirical work information becomes available. . . . Thus we would like to see randomness 
on market efficiency. It has been used in two main ways. First, many re- in the prices of successive transactions, rather than great continuity.. . . 
searchers have tried to measure the profits earned by market professionals Randomness means that a series of small upward movements (or small 
such as mutual fund managers. If these managers achieve superior returns downward movements) is very unlikely. If the price is going to move up, 
(after adjustment for risk) then the market is not efficient with respect to the it should move up all at once, rather than in a series of small steps.. . . 
information possessed by the managers. This approach has the advantage Large price movements are desirable, so long as they are not consistently 
that it concentrates on real trading by real market participants, but it has the followed by price movements in the opposite direction. 
disadvantage that one cannot directly observe the information used by the 
managers in their trading strategies (see Fama [1970, 19911 for a thorough Underlying this confusion may be a belief that returns cannot be random 
review of this literature). if security prices are determined by discounting future cash flows. Smith 

As an alternative, one can ask whether hypothetical trading based on (1968), for example, writes: "I suspect that even if the random walkers an- 
an explicitly specified information set would earn superior returns. To nounced a perfect mathematic proof of randomness, I would go on believing 
implement this approach, one must first choose an information set. The that in the long run future earnings influence present value." 
classic taxonomy of information sets, due to Roberts (1967), distinguishes In fact, the discounted present-value model of a security price is entirely 
among consistent with randomness in security returns. The key to understanding 

this is the so-called Law of Iterated Expectations. To state this result we define 
Weak-form Efficiency: The information set includes only the history of information sets I, and J,, where I, C J ,  so all the information in It is also in 

prices or returns themselves. J but J is superior because it contains some extra information. We consider 
Semistrong-Form Efficiency. The information set includes all information expectations of a random variable X conditional on these information sets, 

known to all market participants (publicly available information). written E[X 1 I,] or E[X I J ,] .  The Law of Iterated Expectations says that 
Strong-Form Efficiency: The information set includes all information E[X I I,] = E[E[X ( J ]  ( I , ] .  In words, if one has limited information 

known to any market participant (pr ivate  information). It, the best forecast one can make of a random variable X is the forecast 
of the forecast one would make of X if one had superior information J .  

The next step is to specify a model of "normal" returns. Here the classic This can be rewritten as E[X - E[X I J , ]  I I,] = 0, which has an intuitive 

assumption is that the normal returns on a security are constant over time, interpretation: One cannot use limited information I, to predict the forecast 
error one would make if one had superior information J .  

but in recent years there has been increased interest in equilibrium models 
with time-varying normal security returns. Samuelson (1965) was the first to show the relevance of the Law of 

Iterated Expectations for security market analysis; LeRoy (1989) gives a 
Finally, abnormal security returns are computed as the difference be- 

lucid review of the argument. We discuss the point in detail in Chapter 7, 
tween the return on a security and its normal return, and forecasts of the 

but a brief summary may be helpful here. Suppose that a security price at 
abnormal returns are constructed using the chosen information set. If the 

time t ,  P t ,c an be written as the rational expectation of some "fundamental 
abnormal security return is unforecastable, and in this sense "random," then value" V*, conditional on information It available at time t .  Then we have 
the hypothesis of market efficiency is not rejected. 

1.5.1 EfJicient Markets and the Law of Iterated Expectations The same equation holds one period ahead, so 
The idea that efficient security returns should be random has often caused 
confusion. Many people seem to think that an efficient security price should 



24 1 .  Introduction 1.5. Market Efficiency 

But then the expectation of the change in the price over the next period is market efficiency is an idealization that is economically unrealizable, but 
that serves as a useful benchmark for measuring relative efficiency. 

For these reasons, in this book we do not take a stand on market effi- 
ciency itself, but focus instead on the statistical methods that can be used 
to test the joint hypothesis of market efficiency and market equilibrium. 

because It C It+l,s o El [Et+i[  V*] ] = Et[  V*] by the Law of Iterated Expecta- 
Although many of the techniques covered in these pages are central to the 

tions. Thus realized changes in prices are unforecastable given information 
marketefficiency debate-tests of variance bounds, Euler equations, the 

in the set It. 
CAPM and the AFT-we feel that they can be more profitably applied to 
measuring efficiency rather than to testing it. And if some markets turn 

1.5.2 Is Market Efficiency Testable? out to be particularly inefficient, the diligent reader of this text will be well- 
prepared to take advantage of the opportunity. 

Although the empirical methodology summarized here is wellestablished, 
there are some serious difficulties in interpreting its results. First, any test of 
efficiency must assume an equilibrium model that defines normal security 
returns. If efficiency is rejected, this could be because the market is truly 
inefficient or because an incorrect equilibrium model has been assumed. 
This joint hypothesis problem means that market efficiency as such can never 
be rejected. 

Second, perfect efficiency is an unrealistic benchmark that is unlikely 
to hold in practice. Even in theory, as Grossman and Stiglitz (1980) have 
shown, abnormal returns will exist if there are costs of gathering and pro- 
cessing information. These returns are necessary to compensate investors 
for their information-gathering and information-processing expenses, and 
are no longer abnormal when these expenses are properly accounted for. 
In a large and liquid market, information costs are likely to justify only small 
abnormal returns, but it is difficult to say how small, even if such costs could 
be measured precisely. 

The notion of relative efficiency-the efficiency of one market measured 
against another, e.g., the New York Stock Exchange vs. the Paris Bourse, fu- 
tures markets vs. spot markets, or auction vs. dealer markets-may be a more 
useful concept than the all-or-nothing view taken by much of the traditional 
market-efficiency literature. The advantages of relative efficiency over ab- 
solute efficiency are easy to see by way of an analogy. Physical systems are 
often given an efficiency rating based on the relative proportion of energy 
or fuel converted to useful work. Therefore, a piston engine may be rated 
at 60% efficiency, meaning that on average 60% of the energy contained in 
the engine's fuel is used to turn the crankshaft, with the remaining 40% lost 
to other forms of work such as heat, light, or noise. 

Few engineers would ever consider performing a statistical test to deter- 
mine whether or not a given engine is perfectly efficient-such an engine 
exists only in the idealized frictionless world of the imagination. But measur- 
ing relative efficiency-relative to the frictionless ideal-is commonplace. 
Indeed, we have come to expect such measurements for many household 
products: air conditioners, hot water heaters, refrigerators, etc. Similarly, 



The Predictability of Asset Returns 

ONE OF THE EARLIEST and most enduring questions of financial economet- 
rics is whether financial asset prices are forecastable. Perhaps because of 
the obvious analogy between financial investments and games of chance, 
mathematical models of asset prices have an unusually rich history that pre- 
dates virtually every other aspect of economic analysis. The fact that many 
prominent mathematicians and scientists have applied their considerable 
skills to forecasting financial securities prices is a testament to the fascination 
and the challenges of this problem. Indeed, modern financial economics is 
firmly rooted in early attempts to "beat the market," an endeavor that is still 
of current interest, discussed and debated in journal articles, conferences, 
and at cocktail parties! 

In this chapter, we consider the problem of forecasting future price 
changes, using only past price changes to construct our forecasts. Although 
restricting our forecasts to be functions of past price changes may seem too 
restrictive to be of any interest-after all, investors are constantly bombarded 
with vast quantities of diverse information-nevertheless, even as simple a 
problem as this can yield surprisingly rich insights into the behavior of asset 
prices. We shall see that the martingale and the random walk, two of the most 
important ideas in probability theory and financial economics, grew out of 
this relatively elementary exercise. Moreover, despite the fact that we shall 
present more sophisticated models of asset prices in Chapters 4-9, where 
additional economic variables are used to construct forecasts, whether fu- 
ture price changes can be predicted by past price changes alone is still a 
subject of controversy and empirical investigation. 

In Section 2.1 we review the various versions of the random walk hy- 
pothesis and develop tests for each of these versions in Sections 2.2-2.4. 
Long-horizon returns play a special role in detecting certain violations of 
the random walk and we explore some of their advantages and disadvan- 
tages in Section 2.5. Focusing on long-horizon returns leads naturally to 



28 2. The Predictability of Asset Returns 

the notion of long-range dependence, and a test for this phenomenon is 
presented in Section 2.6. For completeness, we provide a brief discussion of 
tests for unit roots, which are sometimes confused with tests of the random 
walk. In Section 2.8 we present several empirical illustrations that document 
important departures from the random walk hypothesis for recent US stock 
market data. 

2.1 The Random Walk Hypotheses 

A useful way to organize the various versions of the random walk and mar- 
tingale models that we shall present below is to consider the various kinds 
of dependence that can exist between an asset's returns rt and rt+k at two 
dates t and t + k.  To do this, define the random variables f (r t )a nd g(rt+k) 
where f (.) and g( . )  are two arbitrary functions, and consider the situations 
in which 

for all t and for k#O. For appropriately chosen f (.) and g( . ) ,v irtually all 
versions of the random walk and martingale hypotheses are captured by 
(2 .1 .1) ,w hich may be interpreted as an orthogonality condition. 

For example, iff  (.) and g( . )  are restricted to be arbitrary linear func- 
tions, then (2.1.1)i mplies that returns are serially uncorrelated, correspond- 
ing to the Random Walk 3 model described in Section 2.1.3 below. Alterna- 
tively, iff (.) is unrestricted but g( . )  is restricted to be linear, then (2.1.1)i s 
equivalent to the martingale hypothesis described in Section 2.1. Finally, if 
(2.1.1)h olds for all functions f (.) and g( . ) ,t his implies that returns are mu- 
tually independent, corresponding to the Random Walk I and Random Walk 2 
models discussed in Sections 2.1.1 and 2.1.2, respectively. This classification 
is summarized in Table 2.1. 

Although there are several other ways to characterize the various ran- 
dom walk and martingale models, condition (2.1.1)  and Table 2.1 are partic- 
ularly relevant for economic hypotheses since almost all equilibrium asset- 
pricing models can be reduced to a set of orthogonality conditions. This 
interpretation is explored extensively in Chapters 8 and 12. 

The Martingale Model 
Perhaps the eqrliest model of financial asset prices was the martingale model, 
whose origin lies in the history of games of chance and the birth of p rob  
ability theory. The prominent Italian mathematician Girolamo Cardano 
proposed an elementary theory of gambling in his 1565 manuscript Liber de 



30 2. The Predictability of Asset Returns 2.1. The Random Walk Hypotheses 31  

Ludo Aleae ( The Book of Games of Chance), in which he wrote:' the information contained in the asset's price history; hence the conditional 
expectation of future price changes, conditional on the price history, cannot 

The most fundamental principle of all in gambling is simply equal con- 
be either positive or negative (if shortsales are feasible) and therefore must 

ditions, e.g., of opponents, of bystanders, of money, of situation, of the 
be zero. This notion of efficiency has a wonderfully counterintuitive and 

dice box, and of the die itself. To the extent to which you depart from 
seemingly contradictory flavor to it: The more efficient the market, the more 

that equality, if it is in your opponent's favour, you are a fool, and if in 
random is the sequence of price changes generated by the market, and the 

your own, you are unjust. 
most efficient market of all is one in which price changes are completely 

This passage clearly contains the notion of a fairgame, a game which is neither random and unpredictable. 
in your favor nor your opponent's, and this is the essence of a martingale, a However, one of the central tenets of modern financial economics is the 
stochastic process {P t ]w hich satisfies the following condition: necessity of some trade-off between risk and expected return, and although 

the martingale hypothesis places a restriction on expected returns, it does 
not account for risk in any way. In particular, if an asset's expected price 

or, equivalently, change is positive, it may be the reward necessary to attract investors to 
hold the asset and bear its associated risks. Therefore, despite the intuitive 

E[pt+~-  Pt I Pt, P t - I , ..  .I = 0. (2.1.3) 
appeal that the fair-game interpretation might have, it has been shown that 

If Pt represents one's cumulative winnings or wealth at date t from playing the martingale property is neither a necessary nor a sufficient condition for 
some game of chance each period, then a fair game is one for which the rationally determined asset prices (see, for example, Leroy [1973], Lucas 
expected wealth next period is simply equal to this period's wealth (see [1978], and Chapter 8). 
(2.1.2))  , conditioned on the history of the game. Alternatively, a game is fair Nevertheless, the martingale has become a powerful tool in probability 
if the expected incremental winnings at any stage is zero when conditioned and statistics and also has important applications in modern theories of as- 
on the history of the game (see (2.1.3))  . set prices. For example, once asset returns are properly adjusted for risk, 

If Pt is taken to be an asset's price at date t ,  then the martingale hypoth- the martingale property does hold (see Lucas [1978], Cox and Ross [1976], 
esis states that tomorrow's price is expected to be equal to today's price, Harrison and Kreps [1979]). In particular, we shall see in Chapter 8 that 
given the asset's entire price history. Alternatively, the asset's expected price marginal-utility-weighted prices do follow martingales under quite general 
change is zero when conditioned on the asset's price history; hence its price conditions. This risk-adjusted martingale property has led to averitable revo- 
is just as likely to rise as it is to fall. From a forecasting perspective, the lution in the pricing of complex financial instruments such as options, swaps, 
martingale hypothesis implies that the "best" forecast of tomorrow's price is and other derivative securities (see Chapters 9, 12, and Merton [1990], for 
simply today's price, where "best" means minimal mean-squared error (see example). Moreover, the martingale led to the development of a closely re- 
Chapter 7). lated model that has now become an integral part of virtually every scientific 

Another aspect of the martingale hypothesis is that nonoverlapping discipline concerned with dynamics: the random walk hypothesis. 
price changes are uncorrelated at all leads and lags, which implies the in- 
effectiveness of all linear forecasting rules for future price changes based 

2.1.1 The Random Walk 1: IZD Increments 
on historical prices alone. The fact that so sweeping an implication could 
come from as simple a model as (2.1.2) foreshadows the important role that Perhaps the simplest version of the random walk hypothesis is the inde- 
the martingale hypothesis will play in the modeling of asset price dynamics pendently and identically distributed (IID) increments case in which the 
(see the discussion below and Chapter 7). dynamics of {P t }a re given by the following equation: 

In fact, the martingale was long considered to be a necessary condition 
for an efJicient asset market, one in which the information contained in past 
prices is instantly, fully, and perpetually reflected in the asset's current price.2 where is the expected price change or drift, and IID(0, a*)d enotes that t ti s 
If the market is efficient, then it should not be possible to profit by trading on independently and identically distributed with mean 0 and variance a*. The 

'See Hald (1990, Chapter 4) for further details. efficient if the conditional expectation of future price changes is zero, conditioned on all 
"ee Samuelson (1965, 1972, 1973). Roberts (1967) calls the martingale hypothesis weak- available public information, and all available public and private information, respectively. 

f m m arket efficiency. He also defines an asset market to be semistrongrform and strong-fm See Chapter 1 for further discussion of these concepts. 



32 2. The Predictability of Asset Returns 2.2. Tests of Random Walk I: IID Increments 33 

independence of the increments {Er]i mplies that the random walk is also a returns has remained the same over this two-hundred-year period is simply 
fair game, but in a much stronger sense than the martingale: Independence implausible. Therefore, we relax the assumptions of RWl to include pro- 
implies not only that increments are uncorrelated, but that any nonlinear cesses with independent but not identically distributed (INID) increments, 
functions of the increments are also uncorrelated. We shall call this the and we shall call this the Random Walk 2 model or RW2. RW2 clearly contains 
Random Walk 1 model or RW1. RW1 as a special case, but also contains considerably more general price pro- 

To develop some intuition for RW1, consider its conditional mean and cesses. For example, RW2 allows for unconditional heteroskedasticity in the 
variance at date t, conditional on some initial value Po at date 0: et's, a particularly useful feature given the time-variation in volatility of many 

financial asset return series (see Section 12.2 in Chapter 12). 
E[Pt I Pol = P o + P ~  (2.1.5) Although RW2 is weaker than RW1 (see Table 2.1), it still retains the 

most interesting economic property of the IID random walk: Any arbitrary 
Var[Pt ( Po] = a 2t ,  (2.1.6) transformation of future price increments is unforecastable using any arbi- 

trary transformation of past price increments. 
which follows from recursive substitution of lagged Pt in (2.1.4) and the IID 
increments assumption. From (2.1.5) and (2.1.6) it is apparent that the 
random walk is nonstationary and that its conditional mean and variance 2.1.3 The Random Walk 3: Uncorrelated Increments 
are both linear in time. These implications also hold for the two other forms An even more general version of the random walk hypothesis-the one most 
of the random walk hypothesis (RW2 and RW3) described below. often tested in the recent empirical literature-may be obtained by relaxing 

Perhaps the most common distributional assumption for the innova- the independence assumption of RW2 to include processes with dependent 
tions or increments el is normality. If the ct's are IID N(0, (r2),t hen (2.1.4) but uncorrelated increments. This is the weakest form of the random walk 
is equivalent to an arithmetic Brownian motion, sampled at regularly spaced hypothesis, which we shall refer to as the Random Walk 3 model or RW3, 
unit intervals (see Section 9.1 in Chapter 9). This distributional assump- and contains RW1 and RW2 as special cases. A simple example of a process 
tion simplifies many of the calculations surrounding the random walk, but that satisfies the assumptions of RW3 but not of RWl or RW2 is any process 
suffers from the same problem that afflicts normally distributed returns: for which Cov[et, etPk]=  0 for all k # 0, but where cov[€f,€ Kk] # 0 for 
violation of limited liability. If the conditional distribution of Pt is normal, some k # 0. Such a process has uncorrelated increments, but is clearly not 
then there will always be a positive probability that Pt<O. independent since its squared increments are correlated (see Section 12.2 

To avoid violating limited liability, we may use the same device as in in Chapter 12 for specific examples). 
Section 1.4.2, namely, to assert that the natural logarithm of prices pt - 
log Pt follows a random walk with normally distributed increments; hence 

2.2 Tests of Random Walk 1: IID Increments 
pt = p + pt-I + et, ct IID N(0, a 2) .  (2.1.7) 

Despite the fact that RW1 is implausible from a priori theoretical considera- 
This implies that continuously compounded returns are IID normal variates tions, nevertheless tests of RW1 provide a great deal of intuition about the 
with mean p and variance (r2,w hich yields the lognormal model of Bache- behavior of the random walk. For example, we shall see in Section 2.2.2 that 
lier (1900) and Einstein (1905). We shall return to this in Section 9.1 of the drift of a random walk can sometimes be misinterpreted as predictabil- 
Chapter 9. ity if not properly accounted for. Before turning to those issues, we begin 

with a brief review of traditional statistical tests for the IID assumptions in 

2.1.2 The Random Walk 2: Independent Increments Section 2.2.1. 

Despite the elegance and simplicity of RW1, the assumption of identically 2.2.1 Traditional Statistical Tests 
distributed increments is not plausible for financial asset prices over long 
time spans. For example, over the two-hundred-year history of the NewYork Since the assumptions of IID are so central to classical statistical inference, it 
Stock Exchange, there have been countless changes in the econolnic, SO- should come as no surprise that tests for these two assumptions have a long 
cial, technological, institutional, and regulatory environment in which stock and illustrious history in statistics, with considerably broader applications 
prices are determined. The assertion that the probability law of daily stock than to the random walk. Because of their breadth and ubiquity, it is virtually 



34 2. The Predictability of Asset Returns 2.2. Tests of Random Walk I :  IID Increments 35 

impossible to catalog all tests of IID in any systematic fashion, and we shall tests also provides us with an opportunity to develop some machinery that 
mention only a few of the most well-known tests. we shall require later. 

Since IID are properties of random variables that are not specific to a 
particular parametric family of distributions, many of these tests fall under Sequences and Reversals 
the rubric of nonparametric tests. Some examples are the Spearman rank We begin with the logarithmic version of RWI or geometric Brownian mo- 
correlation test, Spearman's footrule test, the Kendall r correlation test, tion in which the log price process pt is assumed to follow an IID random 
and other tests based on linear combinations of ranks or R-statistics (see walk without drift: 
Randles and Wolfe [I9791 and Serfling [1980]). By using information con- 
tained solely in the ranks of the observations, it is possible to develop tests 
of IID that are robust across parametric families and invariant to changes in 
units of measurement. Exact sampling theories for such statistics are gener- and denote by It the following random variable: 

ally available but cumbersome, involving transformations of the (discrete) 
uniform distribution over the set of permutations of the ranks. However, for 
most of these statistics, normal asymptotic approximations to the sampling 
distributions have been developed (see Serfling [1980]). 

More recent techniques based on the empirical distribution function Much like the classical Bernoulli coin-toss, It indicates whether the date-t 
of the data have also been used to construct tests of IID. These tests of- continuously compounded return rt is positive or negative. In fact, the coin- 
ten require slightly stronger assumptions on the joint and marginal distri- tossing analogy is quite appropriate as many of the original tests of RW1 were 
bution functions of the data-generating process; hence they fall into the based on simple coin-tossing probabilities. 
class of semiparametric tests. Typically, such tests form a direct compari- One of the first tests of RW1 was proposed by Cowles and Jones (19373 
son between the joint and marginal empirical distribution functions or an and consists of a comparison of the frequency of sequences and reversals in his- 
indirect comparison using the quantiles of the two. For these test statis- torical stock returns, where the former are pairs of consecutive returns with 
tics, exact sampling theories are generally unavailable, and we must rely on the same sign, and the latter are pairs of consecutive returns with opposite 
asymptotic approximations to perform the tests (see Shorack and Wellner signs. Specifically, given a sample of n+l returns rl, . . . , rn+l, the number 
[l986]). of sequences N, and reversals N, may be expressed as simple functions of 

Under parametric assumptions, tests of IID are generally easier to con- the It's: 
struct. For example, to test for independence among k vectors which are n 
jointly normally distributed, several statistics may be used: the likelihood N, = C Y,, = I I + 1 - 1 I )  (2.2.3) 
ratio statistic, the canonical correlation, eigenvalues of the covariance ma- t=l 
trices, etc. (see Muirhead [1983]). Of course, the tractability of such tests N,. = n - N,. (2.2.4) 
must be traded off against their dependence on specific parametric assump 
tions. Although these tests are often more powerful than their nonparamet- If log prices follow a driftless IID random walk (2.2.1), and if we add the 
ric counterparts, even small departures from the hypothesized parametric further restriction that the distribution of the increments et is symmetric, 
family can lead to large differences between the actual and nominal sizes of then whether rt is positive or negative should be equally likely, a fair coin-toss 
the tests in finite samples. with probability one-half of either outcome. This implies that for any pair of 

consecutive returns, a mquence and a reversal are equally probable; hence 
the Cowles-Jones ratio CJ = NJN, should be approximately equal to one. 

2.2.2 Sequences and Reversals, and Runs More formally, this ratio may be interpreted as a consistent estimator of the 

The early tests of the random walk hypothesis were largely tests of RWl and ratio CJ of the probability rr, of a sequence to the probability of a reversal 

RW2. Although they are now primarily of historical interest, nevertheless 1 - IT,s ince: 

we can learn a great deal about the properties of the random walk from such 
tests. Moreover, several recently developed econometric tools rely heavily 
on RW1 (see, for example, Sections 2.5 and 2.6),h ence a discussion of these 



36 2. The Predictability of Asset Returns 2.2. Tests of Random Walk 1: IID Increments 37 

where "P+* "d enotes convergence in probability. The fact that this ratio which is close to the value of 1.17 that Cowles and Jones (1937, Table 1) 

exceeded one for many historical stock returns series led Cowles and Jones report for the annual returns of an index of railroad stock prices from 1835 

(1937) to conclude that this "represents conclusive evidence of structure in to 1935. Is the difference statistically significant? 

stock prices."3 To perform a formal comparison of the ?o values 1.19 and 1.17, we 

However, the assumption of a zero drift is critical in determining the require a sampling theory for the estimator CJ. Such a theory may be o b  

value of CJ. In particular, CJ will exceed one for an IID random walk with tained by noting from (2.2.3) that the estimator N, is a binomial random 

drift, since a drift--either positive or negative--clearly makes sequences variable, i.e., the sum of n Bernoulli random variables Y, where 
more likely than reversals. To see this, suppose that log prices follow a 
normal random walk with drift: 1 with probability n, = n 2+  (1 - n)*; 

I = (  
0 with probability 1 - n, 

Then the indicator variable It is no longer a fair coin-toss but is biased in hence we may approximate the distribution of N, for large n by a normal 
the direction of the drift, i.e., distribution with mean E[N,] = nn, and variance Var[N,]. Because each 

pair of adjacent x's will be dependent,4 thevariance of N, is not nn,(l--n,)- 
1 with probability n 

I t  = ( the usual expression for the variance of a binomial random variable-but is 
0 with probability 1 - n, instead 

where 
n --= Pr(rt > 0) = 8 (2.2.6) Var[N,] = nn,(l - n,) + 2nCov[Y,, 

If the drift P is positive then n > 4,a nd if it is negative then n < 4. Under 
this more general specification, the ratio of n, to 1 - n, is given by Applying a first-order Taylor approximation or the delta method (see Sec- 

tion A.4 of the Appendix) to a = N,/(n - N,) using the normal asymptotic 
approximation for the distribution of N, then yields 

As long as the drift is nonzero, it will always be the case that sequences are 
more likely than reversals, simply because a nonzero drift induces a trend 
in the process. It is only for the "fair-game" case of n = 4 that CJ achieves 
its lower bound of one. 

To see how large an effect a nonzero drift might have on CJ, suppose where "2"i ndicates that the distributional relation is asymptotic. Since the 
that P = 0.08 and a = 0.21, values which correspond roughly to annual US Cowles andJones (1937) estimate of 1.17yields?, = 0.5392 and? = 0.6399, 
stock returns indexes over the last half-century. This yields the following with a sample size n of 99 returns, (2.2.8) implies that the approximate 
estimate of n: standard error of the 1.17 estimate is 0.2537. Therefore, the estimate 1.17 is 

not statistically sig@ficantly different from 1.19. Moreover, under the null 
hypothesis n = i,C J has a mean of one and a standard deviation of 0.2010; 
hence neither 1.17 or 1.19 is statistically distinguishable from one. This 
provides little evidence against the random walk hypothesis. 

On the other hand, suppose the random walk hypothesis were false- 
would this be detectable by the CJ statistic? To see how departures from the 
random walk might affect the ratio CJ, let the indicator It be the following 

3 ~ an la ter study, Cowles (1960) corrects for biases in time-averaged price data and still 
finds CJ ratios in excess of one. However, his conclusion is somewhat more guarded: ". . . while 
our various analyses have disclosed a tendency towards persistence in stock price movements, 4 ~ fnac t, & is a two-state Markov chain with probabilities Pr(& = 1 1 & - I  = 1) = (P' + 
in no case is this sufficient to provide more than negligible profits after payment of brokerage 
costs." (1 - ~ ) ~ ) /apnd,   Pr(& = 0 I &-I  = 0) = 112. 



38 2. The Predictability of Asset Returns 2.2. Tests of Random Walk 1: ZZD Increments 39 

a twestate Markov chain: of 0s (of length 2, 1, and 2, respectively), thus six runs in total. In contrast, 
the sequence 0000011 11 1 contains the same number of 0s and Is, but only 

Zl+ 1 
1 0 2 runs. By comparing the number of runs in the data with the expected 

number of runs under RW1, a test of the IID random walk hypothesis may 
be constructed. To perform the test, we require the sampling distribution 
of the total number of runs NNnsi n a sample of n. Mood (1940) was the first 

where a denotes the conditional probability that rt+l is negative, conditional to provide a comprehensive analysis of runs, and we shall provide a brief 
on a positive rt, and B denotes the conditional probability that r,+l is positive, summary of his most general results here. 
conditional on a negative rt. If a = 1 - B, this reduces to the case examined Suppose that each of n IID observations takes on one of q possible 
above (set n = 1 -a) :  the IID random walk with drift. As long as a # 1 - B, values with probability ni, i = 1,.  . . , q (hence xin i = 1). In the case of 
It (hence rt) will be serially correlated, violating RW1. In this case, the the indicator variable It defined in (2.2.2), q is equal to 2; we shall return 
theoretical value of the ratio CJ is given by to this special case below. Denote by Nrun,(i) the total number of runs 

of type i (of any length), i = 1, . . . , q; hence the total number of runs 
Nruns=  xiN runs(i).U sing combinatorial arguments and the properties of 
the multinomial distribution, Mood (1940) derives the discrete distribution 
of Nruns(if)r om which he calculates the following moments: 

which can take on any nonnegative real value, as illustrated by the following 
table. 

Moreover, Mood (1940) shows that the distribution of the number of runs 
converges to a normal distribution asymptotically when properly normal- 
ized. In particular, we have 

As a and B both approach one, the likelihood of reversals increases and 
hence CJ approaches 0. As either a! or B approaches zero, the likelihood 
of sequences increases and CJ increases without bound. In such cases, CJ 
is clearly a reasonable indicator of departures from RW1. However, note 2 N(0, ni(l - n,) - 3n?(l - x , ) ~ )  
that there exist combinations of (a, /I) for which a#l-B and CJ=l, e.g., 
(a,B )=(:, :); hence the CJ statistic cannot distinguish these cases from 
RWl (see Problem 2.3 for further discussion). 

Runs 
Another common test for RWl is the runs test, in which the number of 
sequences of consecutive positive and negative returns, or runs, is tabulated 
and compared against its sampling distribution under the random walk 
hypothesis. For example, using the indicator variable Zl defined in (2.2.2), 
a particular sequence of 10 returns may be represented by 1001110100, where ''A"i ndicates that the equality holds asymptotically. Tests of RW1 
containing three runs of 1s (of length 1,3,a nd 1, respectively) and three runs may then be performed using the asymptotic approximations (2.2.14) or 



2. The Predictability of Asset Returns 2.3. Tests of Random Walk 2: Independent Increments 41  

To perform a test for the random walk in the Bernoulli case, we may 
Table 2.2. Expected runs for a random walk with dri,fl p.  calculate the following statistic: 

and perform the usual test of significance. A slight adjustment to this statis- 
tic is often made to account for the fact that while the normal approximation 
yields different probabilities for realizations in the interval [N,,,,, N,,, + I), 
the exact probabilities are constant over this interval since N,,,, is integer- 
valued. Therefore, a continuity correction is made in which the r-statistic is eval- 
uated at the midpoint of the interval (see Wallis and Roberts [1956]); thus 

Nmns + 4 - 2n77(1- n )  
Z = " N(0, 1). 

2,/nn(l - n) [ l  - 3n(l  - n)]  
Expected total number of runs in a sample of n independent Bernoulli trials representing pos- 
itive/negative continuously compounded returns for a Gaussian geometric Brownian motion Other aspects of runs have also been used to test the IID random walk, 
with drift p = 0%,.  . . ,20%a nd standard deviation a = 21%. such as the distribution of runs by length and by sign. Indeed, Mood's 

(1940) seminal paper provides an exhaustive catalog of the properties of 
runs, including exact marginal and joint distributions, factorial moments, 
centered moments, and asymptotic approximations. An excellent summary 

(2.2.16), and the probabilities ni may be estimated directly from the data as of these results, along with a collection of related combinatorial problems 
the ratios iii = ni/n, where ni is of the number of runs in the sample of n in probability and statistics is contained in David and Barton (1962). Fama 
that are the ith type; thus n = Cini.  (1965) presents an extensive empirical analysis of runs for US daily, fourday, 

To develop some sense of the behavior of the total number of runs, nineday, and sixteenday stock returns from 1956 to 1962, and concludes 
consider the Bernoulli case k = 2 corresponding to the indicator variable that, ". . .there is no evidence of important dependence from either an 
It defined in (2.2.2) of Section 2.2.2 where n denotes the probability that investment or a statistical point of view." 
It = 1. In this case, the expected total number of runs is More recent advances in the analysis of Markov chains have generalized 

the theory of runs to non-IID sequences, and by recasting patterns such 
as a run as elements of a permutation group, probabilities of very complex 
patterns may now be evaluated explicitly using the$rst-passage or hitting time 

Observe that for any n 2 1, (2.2.17) is a globally concave quadratic function of a random process defined on the permutation group. For these more 
in n on [O,l] which attains a maximum value of (n + 1)/2 at n = 4. There- recent results, see Aldous (1989),A ldous and Diaconis (1986),a nd Diaconis 
fore, a driftless random walk maximizes the expected total number of runs (1988). 
for any fixed sample size n or, alternatively, the presence of a drift of either 
sign will decrease the expected total number of runs. 

To see the sensitivity of E[N,,,] with respect to the drift, in Table 2.2 2.3 Tests of Random Walk 2: Independent Increments 
we report the expected total number of runs for a sample of n = 1,000 
observations for a geometric random walk with normally distributed incre- The restriction of identical distributions is clearly implausible, especially 
ments, drift p = 0%, . . . ,20%,a nd standard deviation a = 21% (which is when applied to financial data that span several decades. However, testing 
calibrated to match annual US stock index returns); hence n = @(p/a).  for independence without assuming identical distributions is quite difficult, 
From Table 2.2 we see that as the drift increases, the expected total number particularly for time series data. If we place no restrictions on how the 
of runs declines considerably, from 500.5 for zero-drift to 283.5 for a 20% marginal distributions of the data can vary through time, it becomes virtually 
drift. However, all of these values are still consistent with the random walk impossible to conduct statistical inference since the sampling distributions 
hypothesis. of even the most elementary statistics cannot be derived. 



42 2. The Predictability of Asset Returns 2.3. Tests of Random Walk 2: Independent Increments 43 

Some of the nonparametric methods mentioned in Section 2.2.1 such as more frequent trading, Fama and Blume (1966) show that even a 0.1% 
rank correlations do test for independence without also requiring identical roundtrip transaction cost is enough to eliminate the profits from such fil- 
distributions, but the number of distinct marginal distributions is typically ter rules. 
a finite and small number. For example, a test of independence between 
I Q  scores and academic performance involves two distinct marginal dis- 2.3.2 Technical Analysis 
tributions: one for IQ scores and the other for academic performance. 
Multiple observations are drawn from each marginal distribution and vari- As a measure of predictability, the filter rule has the advantage of practical 
ous nonparametric tests can be designed to check whether the product of relevance-it is a specific and readily implementable trading strategy, and 
the marginal distributions equals the joint distribution of the paired o b  the metric of its success is total return. The filter rule isjust one example of 
servations. Such an approach obviously cannot succeed if we hypothesize a much larger class of trading rules arising from technical analysis or charting. 
a unique marginal distribution for each observation of I Q  and academic Technical analysis is an approach to investment management based on the 
performance. belief that historical price series, trading volume, and other market statis- 

Nevertheless, there are two lines of empirical research that can be tics exhibit regularities-often (but not always) in the form of geometric 
viewed as a kind of "economic" test of RW2: jilterrules, and technical analysis. patterns such as double bottoms, head-and-shoulders, and support and resistance 
Although neither of these approaches makes much use of formal statistical levels-that can be profitably exploited to extrapolate future price move- 
inference, both have captured the interest of the financial community for ments (see, for example, Edwards and Magee [I9661 and Murphy [1986]). 
practical reasons. This is not to say that statistical inference cannot be ap- In the words of Edwards and Magee (1966): 
plied to these modes of analysis, but rather that the standards of evidence in Technical analysis is the science of recording, usually in graphic form, 
this literature have evolved along very different paths. Therefore, we shall the actual history of trading (price changes, volume of transactions, 
present only a cursory review of these techniques. etc.) in a certain stock or in "the averages" and then deducing from 

that pictured history the probable future trend. 

2.3.1 Filter Rules Historically, technical analysis has been the "black sheep" of the academic 
finance community. Regarded by many academics as a 1p ursuit that lies some- 

To test RW2, Alexander (1961, 1964) applied a jilter rule in which an asset 
where between astrology and voodoo, technical analysis has never enjoyed 

is purchased when its price increases by x%, and (short)sold when its price 
the same degree of acceptance that, for example, fundamental analysis has 

drops by x%. Such a rule is said to be an x% jilter, and was proposed by received. This state of affairs persists today, even though the distinction be- 
Alexander (1961) for the following reasons: tween technical and fundamental analysis is becoming progressively f ~ z z i e r . ~  

Suppose we tentatively assume the existence of trends in stock market Perhaps some of the prejudice against technical analysis can be at- 
prices but believe them to be masked by the jiggling of the market. We tributed to semantics. Because fundamental analysis is based on quantities 
might filter out all movements smaller than a specified size and examine familiar to most financial economists-for example, earnings, dividends, 
the remaining movements. and other balance-sheet and income-statement items-it possesses a natu- 

ral bridge to the academic literature. In contrast, the vocabulary of the 
The total return of this dynamic portfolio strategy is then taken to be a technical analyst is completely foreign to the academic and often mystifying 
measure of the predictability in asset returns. A comparison of the total to the general public. Consider, for example, the following, which might 
return to the return from a buy-and-hold strategy for the Dow Jones and be found in any recent academic finance journal: 
Standard and Poor's industrial averages led Alexander to conclude that The magnitudes and decay pattern of the first twelve autocorrelations 
". . . there are trends in stock market prices.. . ." and the statistical significance of the Box-Pierce Qstatistic suggest the 

Fama (1965) and Fama and Blume (1966) present a more detailed em- presence of a high-frequency predictable component in stock returns. 
pirical analysis of filter rules, correcting for dividends and trading costs, 
and conclude that such rules do not perform as well as the buy-and-hold 5 ~ oerx ample, many technical analysts no longer base their forecasts solely on past prices 
strategy. In the absence of transactions costs, very small filters (1% in and volume but also use earnings and dividend information and other "fundamental" data, 
Alexander [I9641 and between 0.5% and 1.5% in Fama and Blume [1966]) and as many fundamental analysts now look at past price and volume patterns in addition to 

do yield superior returns, but because small filters generate considerably more traditional variables. 



44 2. The Predictability of Asset Returns 2.4. Tests of Random Walk 3: Uncorrelated Increments 45 

Contrast this with the statement: Given a covariance-stationary time series { r , } ,t he kth order autocovariance 
and autocorrelation coefficients, y (k) and p(k), respectively, are defined as" 

The presence of clearly identified support and resistance levels, coupled 
with a one-third retracement parameter when prices lie between them, 
suggests the presence of strong buying and selling opportunities in the 
near-term. 

Both statements have the same meaning: Using historical prices, one can 
predict future prices to some extent in the short run. But because the two where the second equality in (2.4.3) follows from the covariance-stationarity 
statements are so laden with jargon, the type of response they elicit depends of { r t ] .F or a given sample { ~ , ) f a, u~t,o covariance and autocorrelation coeffi- 
very much on the individual reading them. cients may be estimated in the natural way by replacing population moments 

Despite the differences in jargon, recent empirical evidence suggests with sample counterparts: 
that technical analysis and more traditional financial analysis may have much 
in common (see, in particular, Section 2.8). Recent studies by Blume, Easley, 
and O'Hara (1994), Brock, Lakonishok, and LeBaron (1992), Brown and 
Jennings (1989), LeBaron (1996), Neftci (1991), Pau (1991), Taylor and 
Allen (1992),a nd Treynor and Ferguson (1985) signal a growing interest in 
technical analysis among financial academics, and so it may become a more 
active research area in the near future. 

2.4 Tests of Random Walk 3: Uncorrelated Increments 
The sampling theory for f (k) and b(k) depends, of course, on the data- 

One of the most direct and intuitive tests of the random walk and martin- generating process for { r l } .F or example, if rt is a finite-order moving aver- 
gale hypotheses for an individual time series is to check for serial correlation, age, 
correlation between two observations of the same series at different dates. 
Under the weakest version of the random walk, RW3, the increments or 
firstdifferences of the level of the random walk are uncorrelated at all leads 
and lags. Therefore, we may test RW3 by testing the null hypothesis that the where { c t ]i s an independent sequence with mean 0, variance a*,f ourth 
autocorrelation coefficients of the first-differences at various lags are all zero. moment 77a4,a nd finite sixth moment, then Fuller (1976, Theorem 6.3.5) 

This seemingly simple approach is the basis for a surprisingly large va- shows that the vector of autocovariance coefficient estimators is asymptoti- 
riety of tests of the random walk, and we shall develop these tests in this cally multivariate normal: 
chapter. For example, tests of the random walk may be based on the autocor- 
relation coefficients themselves (Section 2.4.1). More powerful tests may be 
constructed from the sum of squared autocorrelations (Section 2.4.2). Lin- 
ear combinations of the autocorrelations may also have certain advantages where 
in detecting particular departures from the random walk (Sections 2.4.3 and 
2.5). Therefore, we shall devote considerable attention to the properties of 
autocorrelation coefficients in the coming sections. 

2.4.1 Autocorrelation Coefficients 

The autocorrelation coefficient is a natural time-series extension of the well- 
known correlation coefficient between two random variables x and y: 

"he requirement of covariance-stationarity is primarily for notational convenience- 
otherwise y (k) and p ( k )  may be funct~onso f t  as well as k, and may not even be well-defined if 
second moments are not finite. 



46 2. The Predictability of Asset Returns 2.4. Tests of Random Walk 3: Uncorrelated Increments 47 

Under the same assumptions, Fuller (1976, Corollary 6.3.5.1) shows that and with uniformly bounded sixth moments, he shows that the sample au- 
the asymptotic distribution of the vector of autocorrelation coefficient esti- tocorrelation coefficients are asymptotically independent and normally dis- 
mators is also multivariate normal: tributed with distribution: 

where 

These results yield a variety of autocorrelation-based tests of the random 
walk hypothesis RWl. 

Lo and MacKinlay (1988), Richardson and Smith (1994),a nd Romano 
and Thombs (1996) derive asymptotic approximations for sample autocor- 
relation coefficients under even weaker conditions-uncorrelated weakly 
dependent observations-and these results may be used to construct tests 

For purposes of testing the random walk hypotheses in which all the pop- of RW2 and RW3 (see Section 2.4.3 below). 
ulation autocovariances are zero, these asymptotic approximations reduce 
to simpler forms and more can be said of their finite-sample means and 2.4.2 Portmanteau Statistics 
variances. In particular, if (r t}s atisfies RW1 and has variance a*a nd sixth 
moment proportional to a6 ,t hen Since RW1 implies that all autocorrelations are zero, a simple test statistic 

of RWl that has power against many alternative hypotheses is the Q-statistic 
due to Box and Pierce (1970): 

I 9 + 0 ( T p 2 )  if k = C # 0 
c o v [ i m ,  mi = (2.4.12) 

O(T-*) otherwise. 
Under the RW1 null hypothesis, and using (2.4.14), it is easy to see that 

A 

From (2.4.11 ) we see that under RW1, where p(k)=O for all k>O, the sample Q = b2(k) is asymptotically distributed as Xi.L jung and Box 
autocorrelation coefficients j(k) are negatively biased. This negative bias (1978) provide the following finite-sample correction which yields a better 
comes from the fact that the autocorrelation coefficient is a scaled sum of fit to the x i  for small sample sizes: 
cross-products of deviations of rt from its mean, and if the mean is unknown 
it must be estimated, most commonly by the sample mean (2.4.6). But 
deviations from the sample mean sum to zero by construction; therefore 
positive deviations must eventually be followed by negative deviations on 

By summing the squared autocorrelations, the Box-Pierce Q-statistic is de- 
average and vice versa, and hence the expected value of cross-products of 

signed to detect departures from zero autocorrelations in either direction 
deviations is negative. 

and at all lags. Therefore, it has power against a broad range of alternative 
For smaller samples this effect can be significant: The expected value 

hypotheses to the random walk. However, selecting the number of auto- 
of j(1) for a sample size of 10 observations is -10%. Under RW1, Fuller 

correlations m requires some care-if too few are used, the presence of 
(1976) proposes the following bias-corrected estimator ~ (k ) : '  

higher-order autocorrelation may be missed; if too many.are used, the test 
may not have much power due to insignificant higher-order autocorrela- 
tions. Therefore, while such a portmanteau statistic does have some appeal, 
better tests of the random walk hypotheses may be available when specific 
alternative hypotheses can be identified. We shall turn to such examples in 

  NO^ that p ( k )  is not unbiased; the term "bias-corrected" refers to the fact that the next sections. 
E [ p ( k ) ] = ~ ( ? ' - ' ) .  



2.4. Tats of Random Walk 3: Uncorrelated Increments 
48 2. Th.e Predictability of Asset Returns 49 

2.4.3 Variance Ratios one-period returns will be larger than the sum of the one-period return's 
variances; hence variances will grow faster than linearly. Alternatively, in 

An important property of all three random walk hypotheses is that the vari- the presence of negative first-order autocorrelation, the variance of the sum 
ance of random walk increments must be a linear function of the time of two one-period returns will be smaller than the sum of the one-period 
in te r~a lF.~or  example, under RW1 for log prices where continuously com- return's variances; hence variances will grow slower than linearly. 
pounded returns rf= log Pt- log PtPl are IID, the variance of T,+T, -~  must For comparisons beyond one- and two-period returns, higher-order au- 
be twice the variance of r,. Therefore, the plausibility of the random walk tocorrelations come into play. In particular, a similar calculation shows that 
model may be checked by comparing the variance of rt+rtPl to twice the the general q-period variance ratio statistic VR(q)s atisfies the relation: 
variance of r,.' Of course, in practice these will not be numerically identical 
even if RW1 were true, but their ratio should be statistically indistinguishable 
from one. Therefore, to construct a statistical test of the random walk hy- 
pothesis using variance ratios, we require their sampling distribution under 
the random walk null hypothesis. where rt(k) = r,+ rf-1 +..  .+rt-k+l and p(k) is the kth order autocorrelation 

coefficient of { r , } .T his shows that VR(q)i s a particular linear combination 
Population Properties of Variance Ratios of the first 12-1 autocorrelation coefficients of { r t ] ,w ith linearly declining 
Before deriving such sampling distributions, we develop some intuition for weights. 
the population values of the variance ratio statistic under various scenar- Under RW1, (2.4.19) shows that for all q, VR(q)=l  since in this case 
ios. Consider again the ratio of the variance of a two-period continuously p(k)=O for all k > l .  Moreover, even under RW2 and RW3, VR(q)m ust still 
compounded return r,(2) = rt + rt-1 to twice the variance of a one-period equal one as long as the variances of r, are finite and the "average variance" 
return rt, and for the moment let us assume nothing about the time series EL, Var[r,]/ T converges to a finite positive number. But (2.4.19) is even 
of returns other than stationarity. Then this variance ratio, which we write 

more informative for alternatives to the random walk because it relates the 
as VR(2) ,r educes to: 

behavior of VR(q)  to the autocorrelation coefficients of {r t }  under such 
alternatives. For example, under an AR(1)  alternative, r, = $rt-1 + e t ,  
(2.4.19) implies that 

- 2Var[rt] + 2 Cov[rt, r,-1 J 
2 Var [r ,]  

where p(1) is the first-order autocorrelation coefficient of returns { r t ] .F or 
any stationary time series, the population value of the variance ratio statistic 
VR(2)i s simply one plus the first-order autocorrelation coefficient. In par- Relations such as this are critical for constructing alternative hypotheses for 
ticular, under RW1 all the autocorrelations are zero, hence VR(2)=1 in this which the variance ratio test has high and low power, and we shall return to 
case, as expected. this issue below. 

In the presence of positive first-order autocorrelation, VR(2)w ill exceed 
one. If returns are positively autocorrelated, the variance of the sum of two Sampling Distribution of =(q) and =(q) under R WI 

To construct a statistical test for RWl we follow the exposition of Lo and 
 his linearity property is more difficult to state in the case of RW2 and RW3 because the MacKinla~( 1988)a nd begin by stating the null hypothesis Ho under which 

variances of increments may vary through time. However, even in these cases the variance the sampling distribution of the test statistics will be derived.'' Let pi denote 
of the sum must equal the sum of the variances, and this is the linearity property which the the log price process and rt r p,-pt-1 continuously compounded returns. 
variance ratio test exploits. We shall construct tests of all three hypotheses below. 

 an^ studies have exploited this property of the random walk hypothesis in devising em- 
pirical tests of predictability;r ecent examples include Campbell and Mankiw (1987),C ochrane 'O~ora lternative expositions see ~am~bel l 'anMda nkiw (1987), Cochrane (1988), Faust 
(1988), Faust (1992), Lo and MacKinlay (1988), Poterba and Summers (1988), Richardson (l992),P oterba and Summers (1988), Richardson (1993),a nd Richardson and Stock (1989). 
(1993), and Richardson and Stock (1989). 



50 2. The Predictability of Asset Returns 2.4. Tests of Random Walk 3: Uncorrelated Increments 5 1 

Then the null hypothesis we consider in this section is" variance of the difference of a consistent estimator and an asymptotically 
efficient estimator is simply the difference of the-asymptotic variances.l2 

Ho : rt = p + ~ t ,  ~t IID N(0,  a2) .  If we define the variance difference estimator as VD(2) = 8: - 6:, then 
(2.4.23), (2.4.24), and Hausman's result implies: 

Let our data consist of 2n+l observations of log prices { p o , p l ,  . 
consider the following estimators for p and a 2:  

The null hypothesis H can then be tested using (2.4.25) and any consistent 
estimator 264 of 2a4 (for example, 2(6,2)'): Construct the standardized 
statistic =(2)/&? which has a limiting standard normal distribution un- 
der RWl, and reject the null hypothesis at the 5% level if it lies outside the 
interval [-1 .96, 1.961. 

The asymptotic distribution of the two-period variance ratio statistic 
E ( 2 )  = 6:/62 now follows directly from (2.4.25) using a first-order Taylor 
approximation or the delta method (see Section A.4 of the Appendix):13 

Equations (2.4.20) and (2.4.21) are the usual sample mean and variance 
estimators. They are also the maximum-likelihood estimators of p and a2 (see 
Section 9.3.2 in Chapter 9). The second estimator 6; of a2m akes use of the 
random walk nature of p,: Under RWl the mean and variance of increments The nu l l9o thes i s  Ho can be tested by computing the standardized statis- 
are linear in the increment interval, hence the a 2 can be estimated by one- tic &(VR(2)-l)/A which is asymptotically standard normal-if it lies 
half the sample variance of the increments of even-numbered observations outside the interval [-1.96, 1.961, RW1 may be rejected at the 5% level of 
{&,p 2, p4, . . . significance. 

,fin}. 
Under standard asymptotic theory, all three estimators are strongly con- Although the variance ratio is often preferred to the variance difference 

sistent: Holding all other parameters constant, as the total number of obser- because the ratio is scale-free, observe that if 2(8,2)2 is used to estimate 2a4, 
vations 2n increases without bound the estimators converge almost surely to then the standard significance test of VD=O for the difference will yield the 
their population values. In addition, it is well known that 62 and 6b2p ossess same inferences as the corresponding test of VR- 1 =O for the ratio since: 
the following normal limiting distributions (see, for example, Stuart and 
Ord [1987]): 

Therefore, in this simple context the two test statistics are equivalent. How- 
ever, there are other reasons that make the variance ratio more appealing 

However, we seek the limiting distribution of the ratio of the variances. '2~rieflyH, ausman (1978) exploits the fact that any asymptotically efficient estimator of 
Although it may readily be shown that the ratio is also asymptotically normal a parameter 8, say ie,m ust possess the property that it is asymptotically uncorrelated with 
with unit mean under RWl, the variance of the limiting distribution is not the difference ia-  Ge, where 6, is any other estimator of 8. If not, then there exists a linear 

apparent since the two variance estimators are clearly not asymptotically combination of 8, and 6, -8, that is more efficient than 6?,c ontradicting the assumed efficiency 

uncorrelated. of 8e. The result follows directly, then, since: 

But since the estimator 6; is asymptotically efficient under the null avar[&l = a ~ a r [+6 ~ 6,  - 6J = a~ar[8,]+  a~ar[6,-  8,] 
hypothesis RW1, we may use Hausman's (1978) insight that the asymptotic avar[Ga - 6?] = avar[ia] - a~ar[8,], 

where aVarr.1 denotes the asymptotic variance operator. 
"we assume normality only for expositional convenience-the results in this section apply I 3 h p articular, apply the delta method to f (81, &)&$ /i2w here 6, &:-Sf, 82equivSf,a nd 

much more generally to log price processes with IID increments that possess finite fourth observe that 6; -8: and Sz are asymptoticallyu ncorrelated because S: is an efficient estimator. 
moments. 



52 2. The Predictability of Asset Returns 2.4. Tests of Random Walk 3: Uncorrelated Increments 53 

and these are discussed in Cochrane (1988), Faust (1992), and Lo and 
MacKinlay (1988, 1989). 

The variance difference and ratio statistics can be easily generalized 
to multiperiod returns. Let our sample consist of nq+l observations {po ,  and define the statistics: 

$11., . . , pnq}w, here q is any integer greater than one and define the estima- 
tors: 

This yields an unbiased variance difference estimator, however, the variance 
ratio estimator is still biased (due to Jensen's Inequality). Nevertheless, 
simulation experiments reported in Lo and MacKinlay (1989) show that 
the figte-sample properties of m ( q )  are closer to their asymptotic limits 
than VR(q). 

Under the null hypothesis H, the asymptotic distributions of the vari- 
ance difference and variance ratio are given by 

Using similar arguments, the asymptotic distributions of %(q) and =(q) 
under the RW1 null hypothesis are These statistics can then be standardized in the usual way to yield asymptot- 

ically standard normal test statistics. As before, if a4 is estimated by 2 in 
standardizing the variance difference statistic, the result is the same as the 
standardized variance ratio statistic: 

Two important refinements of these statistics can improve their finite- 
sample properties substantially. The first is to use overla@ing q-period re- 
turns in estimating the variances by defining the following alternative esti- 
mator for a2: 

1 cnq - 2 
6:(q) = 7  ( p k  - Pk-q - QP) - (2.4.34) 

n9  q ' k
Sampling Distribution of m ( q )  under R W3 

This estimator contains nq- q+ 1 terms, whereas the estimator 6:(q) contains Since there is a growing consensus among financial economists that volatil- 
only n terms. Using overlapping q-period returns yields a more efficient ities change over time (see Section 12.2 in Chapter 12), a rejection of the 
estimator and hence a more powerful test. random walk hypothesis because of heteroskedasticity would not be of much 

The second refinement involves correcting the bias in the variance es- interest. Therefore, we seek a test for RW3. As long as returns are uncorre- 
timators 6; and 6; before dividing one by the other. Denote the unbiased lated, even in the presence of heteroskedasticity the variance ratio must still 
estimators as 5; and 5:(q), where approach unity as the number of observations increases without bound, for 

the variance of the sum of uncorrelated increments must still equal the sum 
of the variances. However, the asymptotic variance of the variance ratios will 
clearly depend on the type and degree of heteroskedasticity present. 

One approach is to model the heteroskedasticity explicitly as in Section 
12.2 of Chapter 12, and then calculate the asymptotic variance of m ( q )  un- 
der this specific null hypothesis. However, to allow for more general forms 



54 2. The Predictability of Asset Returns 2.5. Long-Horizon Returns 55 

of heteroskedasticity, we follow the approach taken by Lo and MacKinlay Second, note that under Hi (condition (H4)) the autocorrelation coeffi- 
(1988) which relies on the heteroskedasticity-consistent methods of White cient estimators C(k) are asymptotically uncorrelated.15 If the asymptotic 
(1980) and White and Domowitz (1984). This approach applies to a much variance 8k of each of the C(k)'s can be obtained under H;, the asymptotic 
broader class of log price processes { p t ) t han the IID normal increments variance 8(q) of m ( q )  may be calculated as the weighted sum of the ah's, 
process of the previous section, a particularly relevant concern for US stock where the weights are simply the weights in relation (2.4.41) squared. De- 
returns as Table 1.1 illustrates.14 Specifically, let rt = p + r t , a nd define note by and B(q) the asymptotic variances of 6(k) and m ( q ) ,  respectively. 
the following compound null hypothesis HG: Then under the null hypothesis Hg Lo and MacKinlay (1988) show that 

1. The statistics m ( q ) ,a nd m(q) -  1 converge almost surely to zero for all 
q as n increases without bound. 

(H2) { r t )i s $-mixing with coefficients $ (m)  of size rl(2r-1) or is a-mixing with 2. The following is a heteroskedasticity-consistente stimator of 6k: 
coefficients a ( m )  of size r / ( r -  1), where r > 1, such that for all t and for any 
r 2 0, there exists some 8 > 0 for which E[lct 6 , - ,  (*('+')I c A < GO. 

3. The following is a heteroskedasticityconsistent estimator of 8(q): 

(H4) Forallt, E [ t tr t - , r t e t - k ]  = 0 for any nonzeroj and k wherej # k. 

Condition (Hl )  is the uncorrelated increments property of the random 
walk that we wish to test. Conditions (H2) and (H3) are restrictions on the 
maximum degree of dependence and heterogeneity allowable while still Despite the presence of general heteroskedasticity, the standardized test 
permitting some form of the Law of Large Numbers and the Central Limit statistic $*(q) 
Theorem to obtain (see White [I9841 for the definitions of $- and a-mixing 
random sequences). Condition (H4) implies that the sample autocorrela- f i(VR(q) - 1) 

$*(q)  = A N ( 0 , l )  (2.4.44) 
tions of rt  are asymptotically uncorrelated; this condition may be weakened $ 
considerably at the expense of computational simplicity (see note 15). 

This compound null hypothesis assumes that pt possesses uncorrelated can be used to test Hi in the usual way. 
increments but allows for quite general forms of heteroskedasticity, includ- 
ing deterministic changes in the variance (due, for example, to seasonal 
factors) and Engle's (1982) ARCH processes (in which the conditional vari- 2.5 Long-Horizon Returns 
ance depends on past information). 

Since -(q) still approaches one under H;, we need only compute its Several recent studies have focused on the properties of long-horizon re- 
asymptotic variance [call it 8(q)] to perform the standard inferences. Lo turns to test the random walk hypotheses, in some cases using 5- to 10- 
and MacKinlay (1988) do this in two steps. First, recall that the following year returns over a 65-year sample. There are fewer nonoverlapping long- 
equality holds asymptotically under quite general conditions: horizon returns for a given time span, so sampling errors are generally 

l5 Although this restriction on the fourth cross-moments of ct may seem somewhat unintu- 
itive, it is satisfied for any process with independent increments (regardless of heterogeneity) 
and also for linear Gaussian ARCH processes. This assumption may be relaxed entirely, requir- 
ing the estimation of the asymptotic covariances of the autocorrelation 'estimators in order to 
estimate the limiting variance 0 of m(qv)ia  (2.4.41). Although the resulting estimator of 0 

I40f course, second moments are still assumed to be finite; otherwise, the variance ratio 
would be more complicated than equation (2.4.43), it is conceptually straightforward and may 

is no longer well defined. This rules out distributions with infinite variance, such as those in readily be formed along the lines of Newey and West (1987). An even more general (and pos- 
the stable Pareto-Ley family (with characteristic exponents that are less than 2) proposed by sibly more exact) sampling theory for the variance ratios may be obtained using the results of 
Mandelbrot (1963) and Fama (1965). However, many other forms of leptokurtosis are allowed, Dufour (1981) and Dufour and Roy (1985). Again, this would sacrifice much of the simplicity 
such as that generated by Engle's (1982) autoregressive conditionally heteroskedastic (ARCH) of our asymptotic results. 
process (see Section 12.2 in Chapter 12). 



56 2. The Predictability of Asset Returns 2.5. Long-Horizon Returns 57 

larger for statistics based on long-horizon returns. But for some alternatives 
to the random walk, long-horizon returns can be more informative than 
their shorter-horizon counterparts (see Section 7.2.1 in Chapter 7 and Lo 
and MacKinlay [ 19891)  . 

One motivation for using long-horizon returns is the permanent/tran- 
sitory components alternative hypothesis, first proposed by Muth (1960) in 
a macroeconomic context. In this model, log prices are composed of two 
components: a random walk and a stationary process, 

where (2.5.6) requires the additional assumption that yy(q)+O as q-+oo, 
an asymptotic independence condition that is a plausible assumption for 

Jt = any zero-mean stationary process, most economic time series.16 This shows that for a sufficiently long hori- 
zon q, the permanent/transitory components model must yield a variance 

and {wt} and (ytJ are mutually independent. The common interpretation ratio less than one. Moreover, the magnitude of the difference between the 
for (2.5.1) as a model of stock prices is that wt is the "fundamental" com- long-horizon variance ratio and one is the ratio of the variance of Ayt to 
ponent that reflects the efficient markets price, and yt is a zero-mean sta- the variance of Apt, a kind of "signal/ (signal+noise)" ratio, where the "sig- 
tionary component that reflects a short-term or transitory deviation from nal" is the transitory component and the "noise" is the permanent markets 
the efficient-markets price wt, implying the presence of "fads" or other mar- component. In fact, one might consider extracting the "signal/noise" ratio 
ket inefficiencies. Since yt is stationary, it is mean-reverting by definition from VR(q) in the obvious way: 
and reverts to its mean of zero in the long run. Although there are several 
difficulties with such an interpretation of (2.5.1)-not the least of which -1 - Var [A Y] 

1 + 
is the fact that market efficiency is tautological without additional economic VR(q) Var [A W] ' 

structure-nevertheless,s uch an alternative provides a good laboratory for 
studying the variance ratio's performance. 2.5.1 P m b h w ith Long-Horiion Inferences 

While VR(q) can behave in many ways under (2.5.1) for small q (de- 
pending on the correlation structure of yt), as q gets larger the behavior of There are, however, several difficulties with long-horizon returns that stem 
VR(q) becomes less arbitrary. In particular, observe that from the fact that when the horizon q is large relafive to the total time span 

T=nq, the asymptotic approximations that are typically used to perform 
inferences break down. 

For example, consider the test statistic m(q) - l  which is asymptotically 
normal with mean 0 and variance: 

under the RW1 null hypothesis. Observe that for all q>2, the bracketed term 
where yy(q)= Cov[yt, yt+q]i s the autocovariance function of yt. Therefore, in (2.5.8) is bounded between and 1 and is monotonically increasing in 
in this case the population value of the variance ratio becomes q. Therefore, for fixed n, this implies upper and lower bounds for V are 

4 and &, respectively. Now since variances cannot be negative, the lower 

16~hiis  implied by ergodicity, and even the long-rangedependent time series discussed in 
Section 2.6 satisfy this condition. 



58 2. The Pwdictability of Asset Return 2.6. Tests For Long-Range Dependence 59 

bound for %??(q)-1  is -1 . But then the smallest algebraic value that the test and it is notoriously difficult to draw inferences about periodicities that ex- 

statistic (VR(q)-1) /0  can take on is: ceed the span of the data." We shall see explicit evidence of such difficulties 
in the empirical results of Section 2.8. However, in some cases long-horizon 
returns can yield important insights, especially when other economic vari- 

Min W q )-  1 = - -  -1  - -6= -m. (2.5.9) ables such as the dividend-price ratio come into play-see Section 7.2.1 in 
0 Min 8 Chapter 7 for further discussion. 

Suppose that q is set at two-thirds of the sample size T so that ~ / q = ; .  This 
implies that the normalized test statistic m(q)/* can never be less than 2.6 Tests For Long-Range Dependence 
-1.73; hence the test will never reject the null hypothesis at the 95% level of 
significance, regardless of the data! Of course, the test statistic can still reject There is one departure from the random walk hypothesis that is outside the 
the null hypothesis by drawing from the right tail, but against alternative statistical framework we have developed so far, and that is the phenomenon 
hypotheses that imply variance ratios less than one for large q-such as the of long-range dependence. Long-rangedependent time series exhibit an un- 
permanent/transitory components model (2.5.1)-the variance ratio test usually high degree of persistence-in a sense to be made precise below-so 
will have very little power when q/ T is not close to zero. that observations in the remote past are nontrivially correlated with obser- 

A more explicit illustration of the problems that arise when q/  T is large vations in the distant future, even as the time span between the two ob- 
may be obtained by performing an alternate asymptotic analysis, one in servations increases. Nature's predilection towards long-range dependence 
which q grows with T so that q(T)/ T approaches some limit8 strictly between has been welldocumented in the natural sciences such as hydrology, mete- 
zero and one. In this case, under RW1 Richardson and Stock (1989) show orology, and geophysics, and some have argued that economic time series 
that the unnormalized variance ratio S ( q ) c onverges in distribution to the are also long-range dependent. In the frequency domain, such time se- 
following: ries exhibit power at the lowest frequencies, and this was thought to be so 

commonplace a phenomenon that Granger (1966) dubbed it the "typical 
spectral shape of an economic variable." Mandelbrot and Wallis (1968) used 
the more colorful term "Joseph Effect," a reference to the passage in the 
Book of Genesis (Chapter 41) in which Joseph foretold the seven years of 
plenty followed by the seven years of famine that Egypt was to experience.18 

where B(.) is standard Brownian motion defined on the unit interval (see 
Section 9.1 in Chapter 9). Unlike the standard "fixed-q" asymptotics, in this 
case S ( q )d oes not converge in probability to one. Instead, it converges in 2.6.1 Examples of Long-Range Depehdence 

distribution to a random variable that is a functional of Brownian motion. A typical example of long-range dependence is given by the fractionally dif- 
The expected value of this limiting distribution in (2.5.10) is ferenced time series models of Granger (l98O), Granger and Joyeux (l98O), 

and Hosking (1981), in which pl satisfies the following difference equation: 

where L is the lag operator, i.e., Lp, = ptPl. Granger and Joyeux (1980) 
In our earlier example where q/ T = $, the alternative asymptotic approxi- 

and Hosking (1981) show that when the quantity (l-L)d is extended to 
mation (2.5.10) implies that ~[*(q)] converges to $, considerably less than noninteger powers of d in the mathematically natural way, the result is a 
one despite the fact that RW1 holds. 

These biases are not unexpected in light of the daunting demands we "See the discussion and analysis in Section 2.6 for further details. 
are placing on long-horizon returns-without more specific economic struc-  his biblical analogy is not completely frivolous, since long-range dependence has been 
ture, it is extremely difficult to infer much about phenomena that spans a documented in various hydrological studies, not the least of which was Hurst's (1951) seminal 
significant portion of the entire dataset. This problem is closely related to study on measuring the long-term storage capacity of reservoirs. Indeed, much of Hurst's 

research was motivated by his empirical observations of the Nile, the very same river that 
one in spectral analysis: estimating the spectral density function near fre- played so prominent a role in Joseph's prophecies. 
quency zero. Frequencies near zero correspond to extremely long periods, 



60 2. T k P redictability of Asset Returns 2.6. Tests For Long-Range Dependence 6 1 

well-defined time series that is said to be fractionally dijjferenced of order d 
(or, equivalently, fractionally integrated of order - d). Briefly, this involves Table 2.3. Autocarrelation function fifiactionally differenced fn-ocess. 

expanding the expression ( 1-  L ) v~ia t he binomial theorem for noninteger 
powers: 

and then applying the expansion to p,: 

Comparison of autocorrelation functions of fractionally differenced time series ( 1-  L ) ~ P=  et 

where the autoregressive coefficients Ak are often reexpressed in terms of for d = 4,- 4, with that of an AR(1) pt = + e l ,  $J = .5. The variance of el was chosen 
to yield a unit variance for pt in all three cases. 

the gamma function: 

series have first-order autocorrelations of 0.500, at lag 25 the AR(1) cor- 
relation is 0.000 whereas the fractionally differenced series has correlation 

pt may also be viewed as an infinite-order MA process since 0.173, declining only to 0.109 at lag 100. In fact, the defining characteristic 
of long-range dependent processes has been taken by many to be this slow 
decay of the autocovariance function. 

More generally, long-range dependent processes {pt}m ay be defined to 
be those processes with autocovariance functions yp(k)s uch that 

It is not obvious that such a definition of fractional differencing might yield 
a useful stochastic process, but Granger (1980),G ranger andJ oyeux (1980), 
and Hosking (1981)s how that the characteristics of fractionally differenced I kvfi(k) for v E ( - 1 , O )  or, 

yp(k) " as k + 00, (2.6.6) 
time series are interesting indeed. For example, they show that p, is station- -kufi(k) for v E (- 2,- 1 )  
ary and invertible for d ~ (;, - + ) (see Hosking [1981])  and exhibits a unique 
kind of dependence that is positive or negative depending on whether d is 
positive or negative, i.e., the autocorrelation coefficients of pt are of the same where fi (k) is any slowly varying function at infinity.20A lternatively, long- 

sign as d. So slowly do the autocorrelations decay that when d is positive range dependence has also been defined as processes with spectral density 

their sum diverges to infinity, and collapses to zero when d is negative.lg functions s(h) such that 

To develop a sense for long-range dependence, compare the autocor- 
relations of a fractionally differenced pt with those of a stationary AR(1) in 
Table 2.3. Although both the AR(1) and the fractionally differenced ( d = i )  

where A(k) is a slowly varying function. For example, the autocovariance 

Ig~andelbroat nd others have called the d<O case antipersistace, reserving the term long- 
range &pendace for the d>O case. However, since both cases involve autocorrelations that 
decay much more slowly than those of more conventional time series, we call both long-range 'A function f ( x )  is said to be slowly varying at ca if lim,,, f ( t x ) / f( x) = 1 for all t E 
dependent. [a, w). The function logx is an example of a slowly varying function at infinity. 



62 2. The Predictability of Asset Returns 2.6. Tests For Long-Range Dependence 63 

function and spectral density near frequency zero of the fractionally differ- In several seminal papers Mandelbrot, Taqqu, and Wallis demonstrate 
enced process (2.6.1) is the superiority of R/S analysis to more conventional methods of determin- 

ing long-range dependence, such as analyzing autocorrelations, variance 
ratios, and spectral decompositions. For example, Mandelbrot and Wal- 
lis (1969b) show by Monte Carlo simulation that the R/S statistic can de- 
tect long-range dependence in highly non-Gaussian time series with large 
skewness and/or kurtosis. In fact, Mandelbrot (1972, 1975) reports the 
almost-sure convergence of the R/S statistic for stochastic processes with 
infinite variances, a distinct advantage over autocorrelations and variance 
ratios which need not be well-defined for infinite variance processes. Fur- 

where d~ (-1, I). Depending on whether d is negative or positive, the ther aspects of the R/S statistic's robustness are developed in Mandelbrot 
? 2 

spectral densq  of (2.6.1) at frequency zero will either be zero or infinite. and Taqqu (1979). Mandelbrot (1972) also argues that, unlike spectral anal- 
ysis which detects periodic cycles, R/S analysis can detect nonperiodic cycles, 
cycles with periods equal to or greater than the sample period. 

2.6.2 The Hunt-Mandelbrot Rescaled Range Statistic Although these claims may all be contested to some degree, it is a well- 
The importance of long-range dependence in asset markets was first stud- established fact that long-range dependence can indeed be detected by the 
ied by Mandelbrot (1971), who proposed using the range over standard "classical" R/S statistic. However, perhaps the most important shortcoming 
deviation, or R/S, statistic, also called the rescaled range, to detect long-range of the rescaled range is its sensitivity to short-range dependence, implying 
dependence in economic time series. The R/S statistic was originally devel- that any incompatibility between the data and the predicted behavior of 
oped by the English hydrologist Harold Edwin Hurst (1951) in his studies the R/S statistic under the null hypothesis need not come from long-range 
of river discharges. The R/S statistic is the range of partial sums of de- dependence, but may merely be a symptom of short-term memory. 
viations of a time series from its mean, rescaled by its standard deviation. In particular Lo (1991) shows that under RWI the asymptotic distri- 
Specifically, consider a sample of continuously compounded asset returns bution of (I/&) is given by the random variable V, the range of a 
( r l ,5 ,. . . , r,) and let?, denote the sample mean xj7 . Then the classical Brownian bridge, but under a stationary AR(1) specification with autore- 

rescaled-range statistic, which we shall call 6 , is  given by gressive coefficient 4 the normalized R/S statistic converges to CV where 
e~J(1+4)/(1-4). For weekly returns of some portfolios of common stock, 

k  k  6 is as large as 50%, implying that the mean of &/fmi ay be biased up- 
M ~ Xx  ( ? -  7,) - Min x ( ? -  Fn)I  (2.6.10) 

l i k i n  . 
J= 1 

where s, is the usual (maximum likelihood) standard deviation estimator, 
must be chosen to allow for fluctuations in the supply of water above the dam while still 
maintaining a relatively constant flow of water below the dam. Since dam construction costs 
are immense, the importance of estimating the reservoir capacity necessary to meet long-term 
storage needs is apparent. The range is an estimate of this quantity. If 4 is the riverflow 
(per unit time) above the dam and 57, is the desired riverflow below the dam, the bracketed 

The first term in brackets in (2.6.10) is the maximum (over k) of the partial quantity in (2.6.10) is the capacity of the reservoir needed to ensure this smooth flow given the 
sums of the first k deviations of 5 from the sample mean. Since the sum pattern of flows in periods 1 through n. For example, suppose annual riverflows are assumed 

to be 100,50,100, and 50 in years 1 through 4. If a constant annual flow of 75 below the dam 
of all n deviations of 7's from their mean is zero, this maximum is always is desired each year, a reservoir must have a minimum total capacity of 25 since it must store 25 
nonnegative. The second term in (2.6.10) is the minimum (over k) of this units in years 1 and 3 to provide for the relatively dry years 2 and 4. Now suppose instead that 
same sequence of partial sums, and hence it is always nonpositive. The the natural pattern of riverflow is 100,100,50,50i n years 1 through 4. To ensure a flow of 75 

difference of the two quantities, called the range for obvious reasons, is below the dam in this case, the minimum capacity must increase to 50 so as to accommodate 
the excess storage needed in years 1 and 2 to supply water during the "dry spell" in years 3 

always nonnegative and hence & ~ 0 . * l  and 4. Seen in this context, it is clear that an increase in persistence will increase the required 
storage capacity as measured by the range. Indeed, it was the apparent persistence of "dry 
spells" in Egypt that sparked Hurst's lifelong fascination with the Nile, leading eventually to 

2 1 ~ hbe h avior of & may be better understood by considering its origins in hydrological his interest in the rescaled range. 
studies of reservoir design. To accommodate seasonalities in riverflow, a reservoir's capacity 



64 2. The Predictability of Asset Returns 2.8. Recent Empirical Evidence 65 

ward by 73%! Since the mean of V is m x 1 . 2 5 , t he mean of the classical The unit root test is designed to reveal whether Xf is dfference-stationaq 
rescaled range would be 2.16 for such an AR(1) process. (the null hypothesis) or trend-stationary (the alternative hypothesis); this 

Lo (1991) develops a modification of the R/S statistic to account for distinction rests on whether 4 is unity, hence the term unit root hypothesis. 
the effects of short-range dependence, derives an asymptotic sampling the- The test itself is formed by comparing the ordinary least squares estima- 
ory under several null and alternative hypotheses, and demonstrates via tor 6 to unity via its (nonstandard) sampling distribution under the null 
Monte Carlo simulations and empirical examples drawn from recent histor- hypothesis (2.7.1), which was first derived by Dickey and Fuller (1979).23 
ical stock market data that the modified R/S statistic is considerably more Under the null hypothesis, any shock to Xt is said to be permanent since 
accurate, often yielding inferences that contradict those of its classical coun- E[Xt+k I X,] = pk + Xt for all k>O, and a shock to Xt will appear in the 
terpart. In particular, what the earlier literature had assumed was evidence conditional expectation of all future Xt+k. In this case Xt is often called a 
of long-range dependence in US stock returns may well be the result of stochastic trend since its conditional expectation depends explicitly on the 
quickly decaying short-range dependence instead. stochastic variable Xt. In contrast, under the alternative (2.7.2), a shock to 

& is said to be temporary, since E [Xt+k I Xt] = p(t+k) + $ ~ ~ ( ~ t - patn)d,  
the influence of Xt on the conditional expectation of future Xt+k diminishes 

2.7 Unit Root Tests as k increases. 
Because the ct's are allowed to be an arbitrary zero-mean stationary 

A more recent and more specialized class of tests that are often confused process under both the unit root null (2.7.1) and alternative hypothesis 
with tests of the random walk hypotheses is the collection of unit root tests (2.7.2), the focus of the unit root test is not on the predictability of Xt, as 
in which the null hypothesis is it is under the random walk hypotheses. Even under the null hypothesis 

(2.7.1), the increments of Xt may be predictable. Despite the fact that the 
random walk hypotheses are contained in the unit root null hypothesis, 
it is the permanent/temporary nature of shocks to Xt that concerns such 

often with the following alternative hypothesis: tests. Indeed, since there are also nonrandom walk alternatives in the unit 
root null hypothesis, tests of unit roots are clearly not designed to detect 
predictability, but are in fact insensitive to it by construction. 

where ct is any zero-mean stationary process, such that 

2.8 Recent Empirical Evidence 

Predictability in asset returns is a very broad and active research topic, and it 
is impossible to provide a complete survey of this vast literature in just a few 

Heuristically, condition (2.7.3) requires that variance of the partial sum pages. Therefore, in this section we focus exclusively on the recent empirical 
EL,e t increase at approximately the same rate as T, so that each new et literature.24 We hope to give readers a sense for the empirical relevance of 
added to the partial sum has a nontrivial contribution to the partial sum's predictability in recent equity markets by applying the tests developed in the 
variance.22 This condition ensures that the usual limit theorems are appli- earlier sections to stock indexes and individual stock returns using daily and 
cable to the et7sa, nd it is satisfied by virtually all of the stationary processes weekly data from 1962 to 1994 and monthly data from 1926 to 1994. Despite 
that we shall have occasion to study (except for those in Section 2.6). 

''since then, advances in econometric methods have yielded many extensions and general- 
izations to this simple framework: tests for multiple unit roots in multivariate ARIMA systems, 

"1f the partial sum's variance were to grow slower than T, so that the limit in (2.7.3) were tests for cointegration, consistent estimation of models with unit roots cointegration, etc. (see 
0, the uncertainty in the sequence of e,'s would be "cancelling out" over time and would not Campbell and Perron [I9911 for a thorough survey of this literature). 
be a very useful model of random price dynamics. An example of such a process is an MA(1) 24~owevewr,e  would be remiss if we did not cite the rich empirical tradition on which the 
with a unit root, i.e., el = ql - ql-1, where qt is white noise. recent literature is built, which includes: Alexander (1961, l964), Cootner (l964), Cowles 

If the partial sum's variance were to grow faster than T, so that the limit in (2.7.3) were 00, (1960), Cowles and Jones (1937), Fama (1965), Fama and Blume (1966) Kendall (1953), 
this would be an example of long-range dependence, in which the autocorrelation function of Granger and Morgenstern (1963), Mandelbrot (1963), Osborne (1959,1962), Roberts (1959), 
the el's decays very slowly. An example of such a process is a fractionally differenced process and Working (1960). 
(1 - L ) % ~=  ql, where ql is white noise. See Section 2.5 and Lo (1991) for further discussion. 



2. I he Yredzctabzlzty of Asset Keturns 

ipecificity of these examples, the empirical results illustrate many of the 
-s that have arisen in the broader search for predictability among asset 
irns. 

T&Ie 2.4. Autocmlation in daily, weekly, and monthly stock index returns. 

Sample Sample 
Period Size Mean SD $1 & $8 $4 & &, 

~ l2e.4 reports the means, standard deviations, autocorrelations, and Box- 
:rce Q-statistics for daily, weekly, and monthly CRSP stock returns indexes A. Daily Returns 
)m July 3, 1962 to December 31, 1 9 9 4 . ~D~u ring this period, panel A of CRSP Value-Weighted Index 
.ble 2.4 reports that the daily equal-weighted CRSP index has a first-order 
ttocorrelation b(1) of 35.0%. Recall from Section 2.4.1 that under the IID 62:07:03-94:12:380, 179 0.041 0.824 17.6 -0.7 0.1 -0.8 263.3 269.5 

62:07:03-78:10:247, 090 0.028 0.738 27.8 1.2 4.6 3.3 329.4 343.5 
rndom walk null hypothesis RWl, the asymptotic sampling distribution of 78:10:30-94:12:340, 089 0.054 0.901 10.8 -2.2 -2.9 -3.5 69.5 72.1 
(1) is normal with mean 0 and standard deviation l/z/T (see (2.4.14)). CRSP Equal-Weighted Index 
'his implies that a sample size of 8,179 observations yields a standard error 62:07:03-94:12:308 ,179 0.070 0.764 35.0 9.3 8.5 9.9 1,301.9 1,369.5 
tf 1.11% for ;(I); hence an autocorrelation of 35.0% is clearly statistically 62:07:03-78:10:247, 090 0.063 0.771 43.1 13.0 15.3 15.2 1,062.21 ,110.2 
ignificant at all conventional levels of significance. Moreover, the Box- 78:10:30-94:12:30 4,089 0.078 0.756 26.2 4.9 2.0 4.9 348.9 379.5 
'ierce Q-statistic with five autocorrelations has a value of 263.3 which is 
iignificant at all the conventional significance levels (recall that this statistic B. Weekly Returns 
is distributed asymptotically as a Xiv ariate for which the 99.5-percentile CRSP Value-Weighted Index 
is 16.7). 62:07:10-94:12:27 1,695 0.196 2.093 1.5 -2.5 3.5 -0.7 8.8 36.7 

Similar calculations for the value-weighted indexes in panel A show 62:07:10-78:10:03 848 0.144 1.994 5.6 -3.7 5.8 1.6 9.0 21.5 
that both CRSP daily indexes exhibit statistically significant positive serial 78:lO:lO-94:12:27 847 0.248 2.188 -2.0 -1.5 1.6 -3.3 5.3 25.2 
correlation at the first lag, although the equal-weighted index has higher CRSP Equal-Weighted Index 
autocorrelation which decays more slowly than the value-weighted index. 62:07:10-94:12:271 ,695 0.339 2.321 20.3 6.1 9.1 4.8 . 94.3 109.3 
The subsample autocorrelations demonstrate that the significance of the 62:07:10-78:10:03 848 0.324 2.460 21.8 7.5 11.9 6.1 60.4 68.5 
autocorrelations is not an artifact of any particularly influential subset of the 78:lO:lO-94:12:27 847 0.354 2.174 18.4 4.3 5.5 2.2 33.7 51.3 
data; both indexes are strongly positively autocorrelated in each subsample. 

To develop a sense of the economic significance of the autocorrelations C. Monthly Returns 
in Table 2.4, observe that the R2 of a regression of returns on a constant CRSP Value-Weighted Index 
and its first lag is the square of the slope coefficient, which is simply the 62:07:31-94:12:30 390 0.861 4.336 4.3 -5.3 -1.3 -0.4 6.8 12. 
first-order autocorrelation. Therefore, an autocorrelation of 35.0% implies 62:07:31-78:09:29 195 0.646 4.219 6.4 -3.8 7.3 6.2 3.9 9. 
that 12.3% of the variation in the daily CRSP equal-weighted index return 78:10:31-94:12:30 195 1.076 4.450 1.3 -6.3 -8.3 -7.7 7.5 14 
is predictable using the preceding day's index return. CRSP Equal-Weighted Index 

62:07:31-94:12:30 390 1.077 5.749 17.1 -3.4 -3.3 -1.6 12.8 21 
62:07:31-78:09:29 195 1.049 6.148 18.4 -2.5 4.4 2.4 7.5 15 

25Unless stated otherwise, we take returns to be continuously compounded. Portfolio 78:10:31-94:12:30 195 1.105 5.336 15.0 -1.6-12.4 -7.4 8.9 lL 
returns are calculated first from simple returns and then are converted to a continuously 
compounded return. The weekly return of each security is computed as the return from 
Tuesday's closing price to the following Tuesday's closing price. If the following Tuesday's Autocorrelation coefficients (in percent) and Box-Pierce @statistics for CRSP daily, we8 
price is missing, then Wednesday's price (or Monday's if Wednesday's is also missing) is used. and monthly value- and equal-weighted return indexes for the sample period from July 3, 
If both Monday's and Wednesday's prices are missing, the return for that week is reported to December 30,1994 and subperiods. 
-- --;*cin~:t his occurs only rarely. To compute weekly returns on size-sorted portfolios, for 

'--:-- -*tllrns that week are assigned to portfolios based on the 
-- --l.l* i g  missing, then the end 

r 1 . -  



68 2. The Predictability of Asset Returns 2.8. Recent Empirical Evidence 69 

The weekly and monthly return autocorrelations reported in panels B Table 2.5. Variance ratios for weekly stock index returns. 
and C of Table 2.4, respectively, exhibit patterns similar to those of the daily 
autocorrelations: positive and statistically significant at the first lag over the ~~~b~~ Number q of base observations aggregated 
entire sample and for all subsamples, with smaller and sometimes negative Sample period nq of base to form variance ratio 
higher-order autocorrelations. observations 2 4 8 16 

A. CRSP Equal-Weighted Index 
2.8.2 Variance Ratios 

62:07:1 0-94: 12:27 1,695 1.20 1.42 1.65 1.74 
The fact that the autocorrelations of daily, weekly, and monthly index re- (4.53)*  (5.30)* (5.84)*  (4.85)* 
turns in Table 2.4 are positive and often significantly different from zero has 62:07:10-78:10:03 848 1.22 1.47 1.74 1. 90 

(3.47)*  (4.44)*  (4.87)*  (4.24)* 
implications for the behavior of the variance ratios of Section 2.4 and we ex- 78:lO:lO-94:12:27 847 1.19 1.35 1.48 1.54 
plore these implications in this section for the returns of indexes, portfolios, (2.96)*  (2.96)*  (3.00)* (2.55)* 
and individual securities. 

B. CRSP Value-Weighted Index 
CRSP Indexes 62:07:1 0-94: 12:27 1,695 1.02 1.02 1. 04 1.02 
The autocorrelations in Table 2.4 suggest variance ratios greater than one, (0.51) (0.30) (0.41) (0.14) 
and this is confirmed in Table 2.5 which reports variance ratios defined 62:07:1&78: 10:03 848 1.06 1.08 1.14 1.19 
in (2.4.37) and, in parentheses, heteroskedasticityconsistent asymptotically (1.11) (0.89) (1.05) (0.95) 

78:lO:lO-94:12:27 847 0.98 0.97 0.93 0.88 
standard normal test statistics @*(q)d efined in (2.4.44), for weekly CRSP (-0.45) (-0.40) (-0.50) (-0.64) 
equal- and value-weighted market return indexes.26 Panel A contains results 
for the equal-weighted index and panel B contains results for the value- 
weighted index. Within each panel, the first row presents the variance Variance-ratio test of the random walk hypothesis for CRSP equal- and value-weighted indexes, 

for the sample period from July 10,1962 to December 27,1994 and subperiods. The variance 
ratios and test statistics for the entire 1,695-week sample and the next two ratios m ( q )  are reported in the main rows, with heteroskedasticity-consistent test statistics 
rows present similar results for the two subsamples of 848 and 847 weeks. 9*(q) given in parentheses immediately below each main row. Under the random walk null 

Panel A shows that the random walk null hypothesis RW3 is rejected at hypothesis, the value of the variance ratio is one and the test statistics have a standard normal 
all the usual significance levels for the entire time period and all subperi- distribution asymptotically. Test statistics marked with asterisks indicate that the corresponding 

variance ratios are statistically different from one at the 5% level of significance. 
ods for the equal-weighted index. Moreover, the rejections are not due to 
changing variances since the @*(q)'sa re heteroskedasticity-consistent. The 
estimates of the variance ratio are larger than one for all cases. For example, 
the entries in the first column of panel A correspond to variance ratios with returns: the degree of predictability seems to be declining through time. 
an aggregation value q of 2. In view of (2.4.18), ratios with q=2 are approx- To the extent that such predictability has been a source of "excess" profits, 
imately equal to 1 plus the first-order autocorrelation coefficient estimator its decline is consistent with the fact that financial markets have become 
of weekly returns; hence, the entry in the first row, 1.20, implies that the increasingly competitive over the sample period. 
first-order autocorrelation for weekly returns is approximately 20%, which The variance ratios for the equal-weighted index generally increase with 
is consistent with the value reported in Table 2.4. With a corresponding q: the variance ratio climbs from 1.20 (for q=2) to 1.74 (for q = 16), and 
@*(q)s tatistic of 4.53, the random walk hypothesis is resoundingly rejected. the subsample results show a similar pattern. To interpret this pattern, 

The subsample results show that although RW3 is easily rejected over observe that an analog of (2.4.18) can be derived for ratios of variance 
both halves of the sample period, the variance ratios are slightly larger and ratios: 
the rejections slightly stronger over the first half. This pattern is repeated 
in Table 2.6 and in other empirical studies of predictability in US stock 

where ~ ~ (is 1th)e f irst-order autocorrelation coefficient for q-period returns 
'?Since in our sample the values of 1C.*(g)-computedu nder the null hypothesis RW3--are rt+rt-l+. . . + Q - ~ + ~Th erefore, the fact that the variance ratios in panel A 

always statistically less significant than the values of @(q) cakulated under RW1, to conserve of Table 2.5 are increasing implies positive serial correlation in multiperiod 
space we report only the more conservative statistics. 



70 2. The Predictability of Asset Returns 2.8. Recent Empirical Evidence 71 

returns. For example, ~ ( 4 ) ~ ( 2 ) = 1 . 4 2 / 1 . 2 0 = 1 . 1 8w,  hich implies that 2- 
week returns have a first-order autocorrelation coefficient of approximately Table 2.6. Variance ratiosjior weekly size-smted portfolio rpturns. 

18%. 
Panel B of Table 2.5 shows that the value-weighted index behaves quite Number Number q of base observations aggregated 

Time period nq of base to form variance ratio 
differently. Over the entire sample period, the variance ratios are all greater observations 2 
than one, but not by much, ranging from 1.02 for q=2 to 1.04 for q=8. 4 8 16 

Moreover, the test statistics @* ( q )a re all statistically insignificant, hence RW3 A. Portfolio of firms with market values in smallest CRSP quintile 
cannot be rejected for any q. The subsample results show that during the first 62:07:10-94:12:27 1,695 1.35 1.77 2.24 
half of the sample period, the variance ratios for the value-weighted index do (7.15)* (9.42)* (10.74)* 
increase with q (implying positive serial correlation for multiperiod returns), 62:07:10-78:10:03 848 1.34 1.76 2.22 
but during the second half of the sample, the variance ratios decline with (5.47)*  (7.33)*  (8.03)* 
q (implying negative serial correlation for multiperiod returns). These two 78:lO:lO-94:12:27 847 1.37 1.79 2.22 

(4.67)* (5.91)* (6.89)*  
opposing patterns are responsible for the relatively stable behavior of the 
variance ratios over the entire sample period. B. Portfolio of firms with market values in central CRSP quintile 

Although the test statistics in Table 2.5 are based on nominal stock 62:07:1 0-94: 12:27 1,695 1.20 1.39 1.59 
returns, i t  is apparent that virtually the same results would obtain with real (4.25)* (4.85)* (5.16)* 
or excess returns. Since the volatility of weekly nominal returns is so much 62:07:10-78:1 0:03 848 1.21 1.43 1.66 

larger than that of the inflation and Treasury-bill rates, the use of nominal, (3.25)* (4.03)* (4.27)" 
78:1 0: 10-94:1  2:27 847 1.19 1.33 1.44 

real, or excess returns in volatility-based tests will yield practically identical (2.79)* (2.74)* (2.63)" 
inferences. 

C. Portfolio of firms with market values in largest CRSP quintile 

Size-Sorted Portfolios 62:07:10-94:12:27 1,695 1. 06 1.10 1.14 
The fact that RW3 is rejected by the equal-weighted index but not by the (1.71) (1.46) (1.38) 

62:07:1 0-78:10:03 848 1.11 1.21 1.30 
value-weighted index suggests that market capitalization or size may play a (2.05)* (2.15)* (2.12)* 
role in the behavior of the variance ratios. To obtain a better sense of this 78:10:10-94:12:27 847 1.01 1. OO 0.98 
intuition, Table 2.6 presents variance ratios for the returns of size-sorted (0.29)* (0.05) (-0.13) 
portfolios. We compute weekly returns for five sizesorted portfolios from 
the CRSP NYSE-AMEX daily returns file. Stocks with returns for any given Variance-ratio test of the random walk hypothesis for sizesorte'd portfolios, for the sample 
week are assigned to portfolios based on which quintile their beginning- period from July 10, 1962 to December 27, 1994, and subperiods. The variance ratios %%(q) 
of-year market capitalization belongs to. The portfolios are equal-weighted are reported in the main rows, with heteroskedasticity<onsistent test statistics q*(q)g iven in 
and have a changing composition.27 Panel A of Table 2.6 reports the results parentheses immediately below each main row. Under the random walk null hypothesis, the 

for the portfolio of small firms (first quintile), panel B reports the results value of the variance ratio is one and the test statistics have a standard normal distribution 
asymptotically. Test statistics marked with asterisks indicate that the corresponding variance 

for the portfolio of medium-size firms (third quintile), and panel C reports ratios are statistically different from one at the 5% level of significance. 
the results for the portfolio of large firms (fifth quintile). 

Evidence against the random walk hypothesis for the portfolio of com- 
panies in the smallest quintile is strong for the entire sample and for both 
subsamples: in panel A all the @*(q)s tatistics are well above the 5% critical than one, implying a first-order autocorrelation of 35% for weekly returns 
value of 1.96, ranging from 4.67 to 10.74. The variance ratios are all greater over the entire sample period. 

For the portfolios of medium-size companies, the $*(q)  statistics in 
' 7 ~ aels o performed our tests using value-weighted portfolios and obtained essentially panel B shows that there is also strong evidence against RW3, although 

the same results. The only difference appears in the largest quintile of the value-weighted the variance ratios are smaller now, implying lower serial correlation. For 
portfolio, for which the random walk hypothesis was generally not rejected. This, of course, the portfolio of the largest firms, panel C shows that evidence against RW3 
is not surprising, given that the largest value-weighted quintile is quite similar to the value- 
weighted market index is sparse, limited only to the first half of the sample period. 



72 2. The Predictability of Asset Returns 

The results for size-based portfolios are generally consistent with those 
for the market indexes: variance ratios are generally greater than one and 
increasing in q, implying positive serial correlation in multiperiod returns, 
statistically significant for portfolios of all but the largest companies, and 
more significant during the first half of the sample period than the second 
half. 

Individual Securities 
Having shown that the random walk hypothesis is inconsistent with the be- 
havior of the equal-weighted index and portfolios of small- and mediumsize 
companies, we now turn to the case of individual security returns. Table 2.7 
reports the cross-sectional average of the variance ratios of individual stocks 
that have complete return histories in the CRSP database for our entire 
1,695-week sample period, a sample of 411  companies. Panel A contains the 
cross-sectional average of the variance ratios of the 411  stocks, as well as of 
the 100 smallest, 100 intermediate, and 100 largest stocks.28 Cross-sectional 
standard deviations are given in parentheses below the main rows. Since the 
variance ratios are clearly not cross-sectionally independent, these standard 
deviations cannot be used to form the usual tests of significance-they are 
reported only to provide some indication of the cross-sectional dispersion 
of the variance ratios. 

The average variance ratio with q=2 is 0.96 for the 411  individual secu- 
rities, implying that there is negative serial correlation on average. For all 
stocks, the average serial correlation is -4%, and -5% for the smallest 100 
stocks. However, the serial correlation is both statistically and economically 
insignificant and provides little evidence against the random walk hypoth- 
esis. For example, the largest average @*(q)s tatistic over all stocks occurs 
for q=4 and is -0.90 (with a cross-sectional standard deviation of 1.19); the 
largest average @*(q)f or the 100 smallest stocks is -1.67 (for q=2, with a 
cross-sectional standard deviation of 1.75). These results are consistent with 
French and Roll's (1986) finding that daily returns of individual securities 
are slightly negatively autocorrelated. 

For comparison, panel B reports the variance ratio of equal- and value- 
weighted portfolios of the 411 securities. The results are consistent with 
those in Tables 2.5 and 2.6: significant positive autocorrelation for the equal- 
weighted portfolio, and autocorrelation close to zero for the value-weighted 
portfolio. 

That the returns of individual securities have statistically insignificant au- 
tocorrelation is not surprising. Individual returns contain much company- 
specific or idiosyncratic noise that makes it difficult to detect the presence of 
predictable components. Since the idiosyncratic noise is largely attenuated 

'X~id-samplme arket values are used as the size measure. 



7 4  2. The Predictability of Asset Returns 2.8. Recent Empirical Evidence 

by forming portfolios, we would expect to uncover the predictable systematic Tabk 2.8. Cross-autocorrelation matrices fm size-sorted portfolio returns. 
component more readily when securities are combined. Nevertheless, the 
weak negative autocorrelations of the individual securities are an interesting 
contrast to the stronger positive autocorrelation of the portfolio returns. 

2.8.3 Cross-Autocorrelations and Lead-Lag Relations 

Despite the fact that individual security returns are weakly negatively au- 
tocorrelated, portfolio returns-which are essentially averages of individ- 
ual security returns-are strongly positively autocorrelated. This somewhat 
paradoxical result can mean only one thing: large positive cross-autocor- 
relations across individual securities across time. 

To see this, consider a collection of N securities and denote by Rt the 
( N x 1 )  vector of their period-t simple returns [ Rlt . . . RNt 1'. We switch to 
simple returns here because the focus of our analysis is on the interaction 
of returns within portfolios, and continuously compounded returns do not 
aggregate across securities (see Section 1 .4 .1  in Chapter 1 for further discus- 
sion). For convenience, we maintain the following assumption throughout 
this section:2g 

(Al )  Rt is a jointly covariance-stationary stochastic ~Y(OCRFwS ith expectation E [Rt]  
= p = [ Pi CL:! . . . C L N  1' and autocovariance matrices E [ ( R t - k  - p ) ( R t  - 
p ) ' ]  = ( k )  where, with no loss of generality, we take k>_0 since r ( k )  = I?'(- k ).  
If L is defined to be a vector of ones [ 1 . . . 1 ] ' ,  we can express the equal- 
weighted market index as h1=  L ' R , / N .  The first-order autocovariance 
of htm ay then be decomposed into the sum of the first-order own-autoco- 
variances and cross-autocovariances of the component securities: 

Autocorrelation matrices of the vectorXl = [ Rll RZ1 R31 f i l  &l 1' where R,, is the week- 
and therefore the first-order autocorrelation of ktc an be expressed as 1 return on the equal-weighted portfolio of stocks in the ith quintile, i=1, . . . , 5  (quintile 

1 contains the smallest stocks), for the sample of NYSE-AMEX stocks from July 10, 1962 to 
December27,1994 (1,695 observations). Note that T(k)=  D - ' / ~ E [ ( X ~ - ~ - @- )F()X' I~D - ~ ' ~  
where D = diag(a:, . . . , a5'); thus the ( i ,  j)th element is the correlation between K1-k and 
41. Asymptotic standard errors for the autocorrelations under an IID null hypothesis are given 

where tr(.) is the trace operator which sums the diagonal entries of its square- by l / f i  = 0.024. 

matrix argument. The first term of the right side of (2.8.3) contains only 

cross-autocovariancesa nd the second term only the own-autocovariances. If 
'gA.ssumption (Al) is made for notational simplicity, since joint covariance-stationarity al- 

lows us to eliminate time-indexes from population moments such as CL and T ( k ) ;t he qualitative the own-autocovariances are generally negative, and index autocovariance 
features of our results will not change under the weaker assumptions of weakly dependent is positive, then the cross-autocovariances must be positive. Moreover, the 
heterogeneously distributed vectors Z?. This would merely require replacing expectations cross-autocovariances must be large, so large as to exceed the sum of the 
with corresponding probability limits of suitably defined time-averages. See L.o and MacKinlay negative own-autocovariances. 
(1990~f)o r further details. 



76 2. The Predictability of Asset Returns 2.8. Recent Empirical E v i h c e  

Table 2.8 reports autocorrelation matrices f ( k )o f the vector of weekly Table 2.9. A s y m m t v  of cross-autormelation matrices. 
returns of five size-sorted portfolios, formed from the sample of stocks using 
weekly returns from July 10, 1962, to December 27, 1994 (1,695 observa- 
tions). Let Xt denote the vector [ Rlt R2t R3, &, &t If,w here &, is the 
return on the equal-weighted portfolio ofstocks in the ith quintile. Then the 
kth order autocorrelation matrix of Xt is given by T(k) = D - ' / ~ E [ ( x ~ - ~  
p)(Xt - p ) ' ] ~ - ' / ~w,h ere D = diag(a:, . . . ,052) and p = E[Xt]. By this 
convention, the i, jth element of Y(k) is the correlation of &t-k with I$. 
The estimator f (k) is the usual sample autocorrelation matrix. 

An interesting pattern emerges from Table 2.8: The entries below the di- 
agonals of f (k) are almost always larger than those above the diagonals. For 
example, the first-order autocorrelation between last week's return on large 
stocks (IStel)w ith this week's return on small stocks (Rlt)i s 26.5%, whereas 
the first-order autocorrelation between last week's return on small stocks 
(R1,-l) with this week's return on large stocks (&,) is only 2.4%. Similar 
patterns may be seen in the higher-order autocorrelation matrices, although 
the magnitudes are smaller since the higher-order crossautocorrelations de- 
cay. The asymmetry of the f ( k )  matrices implies that the autocovariance 
matrix estimators r(k) are also asymmetric. 

This intriguing had-Zag pattern, where larger capitalization stocks lead 
and smaller capitalization stocks lag, is more apparent in Table 2.9 which 
reports the difference of the autocorrelation matrices and their transposes. 
Every lowerdiagonal entry is positive (hence every upperdiagonal entry is 
negative), implying that the correlation between current returns of smaller 
stocks and past returns of larger stocks is always larger than the correlation Differences between autocorrelation matrices and their transposes for the vector of size- 
between current returns of larger stocks and past returns of smaller stocks. sorted portfolio returns XI = [ Rlt RZ1 Rst & & ]' where is the week-t return on 

Of course, the nontrading model of Chapter 3 also yields an asym- the equal-weighted portfolio of stocks in the ith quintile, k 1 , . . . , 5  (quintile 1 contains the 
metric autocorrelation matrix. However, we shall see in that chapter that smallest stocks), for the sample of NYSE-AMEX stocks from July 10, 1962 to December 27, 

1994 (1,695 observations). Note that r ( k )  = D-'/'E[(x,-~ - @)(XI-  w)']~- lILwLh,  ere 
unrealistically high probabilities of nontrading are required to generate D = diag[cr:, . . ., 41. 
cross-autocorrelations of the magnitude reported in Table 2.8. 

The results in Tables 2.8 and 2.9 point to the complex patterns of cross- 
effects among securities as significant sources of positive index autocorre- 
lation. Indeed, Lo and MacKinlay (1990~s)h ow that over half of the posi- But the presence of positive cross-effects provides another channel 
tive index autocorrelation is attributable to positive crosseffects. They also through which contrarian strategies can be profitable. If, for example, a 
observe that positive crosseffects can explain the apparent profitability of high return for security A today implies that security B's return will probably 
contrarian investment strategies, strategies that are contrary to the general be high tomorrow, then a contrarian investment strategy will be profitable 

market direction. These strategies, predicated on the notion that investors even if each security's returns are unforecastable using past returns of that 

tend to overreact to information, consist of selling "winners" and buying security alone. To see how, suppose the market consists of only the two 

"losers." Selling the winners and buying the losers will earn positive ex- stocks, A and B; if A's return is higher than the market today, a contrar- 
ian sells it and buys B. But if A and B are positively cross-autocorrelated, a 

pected profits in the presence of negative serial correlation because current 
higher return for A today implies a higher return for B tomorrow on aver- 

losers are likely to become future winners and current winners are likely to age, and thus the contrarian will have profited from his long position in B 
become future losers. on average. 



78 2. The Predictability of Asset Returns 2.8. Recent Empirical Evidence 79 

Nowhere is it required that the stock market overreacts, i.e., that indi- substantial mean-reversion in stock market prices at longer horizons, which 
vidual returns are negatively autocorrelated. Therefore, the fact that some they attribute to the presence of a "transitory" component such as the y, 
contrarian strategies have positive expected profits need not imply stock component in (2.5.1). 
market overreaction. In fact, for the particular contrarian strategy that Lo There is, however, good reason to be wary of such inferences when they 
and MacKinlay (1990~e)x amine, over half of the expected profits is due are based on long-horizon returns. Perhaps the most obvious concern is the 
to crosseffects and not to negative autocorrelation in individual security extremely small sample size: From 1926 to 1985, there are only 12 nonover- 
returns. lapping five-year returns. While overlapping returns do provide some incre- 

These cross-effects may also explain the apparent profitability of several mental information, the results in Boudoukh and Richardson (1994), Lo 
other trading strategies that have recently become popular in the financial and MacKinlay (1989), Richardson and Smith (1991), and Richardson and 
community. For example, long/short or market-neutral strategies in which Stock (1989) suggest that this increment is modest at best and misleading 
long positions are offset dollar-fordollar by short positions can earn superior at worst. In particular, Richardson and Stock (1989) propose an asymptotic 
returns in exactly the fashion described above, despite the fact that they are approximation which captures the spirit of overlapping long-horizon return 
designed to take advantage of own-effects, i.e., positive and negative forecasts calculations-they allow the return horizon q to increase with the sample 
of individual securities' expected returns. The performance of matched-book size T so that q/ T converges to a finite value 6 between zero and one- 
or pairs trading strategies can also be attributed to cross-effects as well as which shows that variance ratios can be severely biased when the return 
owneffects. horizon is a significant fraction of the total sample period. For example, 

Although several studies have attempted to explain these striking lead- using their asymptotic approximation (2.5.10), discussed in Section 2.5.1, 
lag effects (see, for example, Badrinath, Kale, and Noe [1995], Boudoukh, the expected value for the variance ratio with overlapping returns is given 
Richardson, and Whitelaw [1994], Jegadeesh and Swaminathan [1993], by (2.5.12) under RW1. This expression implies that with a return horizon 
Conrad, Kaul, and Nimalendran [1991], Brennan, Jegadeesh, and Swami- of 96 months and a sample period of 60 years, 6=8/60=0.133 hence the ex- 
nathan [1993], Jegadeesh and Titman [1995], and Mech [1993]), we are pected variance ratio is (1-6)2=0.751, despite the fact that RW1 is assumed 
still far from having a complete understanding of their nature and sources. to hold. Under RW2 and RW3, even more dramatic biases can occur (see, 

for example, Romano and Thombs [I9961)  . 
These difficulties are reflected in the magnitudes of the standard errors 

2.8.4 Tests Using Long-Horizon Returns associated with long-horizon return autocorrelations and variance ratios 
(see, for example, Richardson and Stock (1989, Table 5), which are typically 

Several recent studies have employed longer-horizon returns-multi-year re- so large as to yield z-statistics close to zero regardless of the point estimates. 
turns in most cases-in examining the random walk hypothesis, predictabil- Richardson (1993) and Richardson and Stock (1989) show that properly 
ity, and the profitability of contrarian strategies, with some surprising results. adjusting for the small sample sizes, and for other statistical issues associated 
Distinguishing between short and long return-horizons can be important be- with long-horizon returns, reverses many of the inferences of Fama and 
cause it is now well known that weekly fluctuations in stock returns differ French (1988b) and Poterba and Summers (1988). 
in many ways from movements in three- to five-year returns. We consider Moreover, the point estimates of autocorrelation coefficients and other 
the econometric trade-offs between short- and long-horizon returns in more time series parameters tend to exhibit considerable sampling variation for 
detail in Chapter 7, and provide only a brief discussion here of the long- long-horizon returns. For example, simple bias adjustments can change 
horizon implications for the random walk hypotheses. the signs of the autocorrelations, as Kim, Nelson, and Startz (1988) and 

In contrast to the positive serial correlation in daily, weekly, and monthly Richardson and Stock (1989) demonstrate. This is not surprising given the 
index returns documented by Lo and MacKinlay (1988) and others, Fama extremely small sample sizes that long-horizon returns produce (see, for 
and French (1988b) and Poterba and Summers (1988) find negative serial example, the magnitude of the bias adjustments in Section 2.4.1). 
correlation in multi-year index returns. For example, Poterba and Sum- Finally, Kim, Nelson, and Startz (1988) show that the negative serial cor- 
mers (1988) report a variance ratio of 0.575 for 96-month returns of the relation in long-horizon returns is extremely sensitive to the sample period 
value-weighted CRSP NYSE index from 1926 to 1985, implying negative se- and may be largely due to the first ten years of the 1926 to 1985 sample. 
rial correlation at some return horizons (recall that the variance ratio is a Although ten years is a very significant portion of the data and cannot be 
specific linear combination of autocorrelation coefficients). Both Fama and excluded without careful consideration, nevertheless it is troubling that the 
French (1988b) and Poterba and Summers (1988) conclude that there is 



80 2. The Predictability of Asset Returns 

sign of the serial correlation coefficient hinges on data from the Great De- 2.3 Characterize the set of all two-state Markov chains (2.2.9) that do not 
pression. This conundrum-whether to omit data influenced by a single satisfy RW1 and for which the CJ statistic is one. What are the general prop- 
cataclysmic event, or to include it and argue that such an event is repre- erties of such Markov chains, e.g., do they generate sequences, reversals, 
sentative of the economic system-underscores the fragility of small-sample etc.? 
statistical inference. Overall, there is little evidence for mean reversion in 

2.4 Derive (2.4.19) for processes with stationary increments. Why do the 
long-horizon returns, though this may be more of a symptom of small sam- weights decline linearly? Using this expression, construct examples of non- 
ple sizes rather than conclusive evidence against mean reversion-we simply 

random-walk processes for which the variance ratio test has very low power. 
cannot tell. 

These considerations point to short-horizon returns as the more imme- 2.5 Using daily and monthly returns data for ten individual stocks and the 
diate source from which evidence of predictability might be culled. This is equal- and value-weighted CRSP market indexes (EWRETD and VWRETD) , 
not to say that a careful investigation of returns over longer time spans will perform the following statistical analysis using any statistical package of your 
be uninformative. Indeed, it may be only at these lower frequencies that the choice. Note that some of the stocks do not have complete return histories, 
impact of economic factors such as the business cycle is detectable. More- so be sure to use only valid observations. Also, for subsample analyses, split 
over, to the extent that transaction costs are greater for strategies exploiting the available observations into equal subsamples. 
short-horizon predictability, long-horizon predictability may be a more gen- 2.5.1 Compute the sample mean f i ,s tandard deviation 6 ,a nd first-order 
uine form of unexploited profit opportunity. Nevertheless, the econometric autocorrelation coefficient b(1) for daily simple returns over the entire 
challenges posed by long-horizon returns are considerable, and the need 1962 to 1994 sample period for the ten stocks and the two indexes. Split 
for additional economic structure is particularly great in such cases. the sample into four equal subperiods and compute the same statistics in 

each subperiod-are they stable over time? 
2.9 Conclusion 

2.5.2 Compute the sample mean f i ,s tandard deviation 6, and first-order 
Recent econometric advances and empirical evidence seem to suggest that autocorrelation coefficient b(1) for continuously compounded daily re- 
financial asset returns are predictable to some degree. Thirty years ago this turns over the entire 1962 to 1994 period, and for each of the four equal 
would have been tantamount to an outright rejection of market efficiency. subperiods. Compare these to the results for simple returns--can con- 
However, modern financial economics teaches us that other, perfectly ra- tinuous compounding change inferences substantially? 
tional, factors may account for such predictability. The fine structure of 2.5.3 Plot histograms of daily simple returns for VWRETD and EWRETD 
securities markets and frictions in the trading process can generate pre- over the entire 1962 to 1994 sample period. Plot another histogram 
dictability. Time-varying expected returns due to changing business condi- of the normal distribution with mean and variance equal to the sample 
tions can generate predictability. A certain degree of predictability may be mean and variance of the returns plotted in the first histograms. Do daily 
necessary to reward investors for bearing certain dynamic risks. Motivated simple returns look approximately normal? Which looks closer to nor- 
by these considerations, we shall develop many models and techniques to mal: VWRETD or EWRETD? Perform the same analysis for continuously 
address these and other related issues in the coming chapters. compounded daily returns and compare these results to those for simple 

re turns. 

2.5.4 Using daily simple returns for the entire 1962 to 1994 sample pe- 
riod, construct 99% confidence intervals for f i  for VWRETD, EWRETD, 

2.1 If {P,]i s a martingale, show that: (1) the minimum mean-squared error 
and the ten individual stock return series. Divide the sample into four 

forecast of Pt+1, conditioned on the entire history (P, ,P t-1, . . .}, is simply 
equal subperiods and construct 99% confidence intervals in each of the 

Pt; (2) nonoverlapping kth differences are uncorrelated at all leads and lags four subperiods for the twelve series--do they shift a great deal? 
for all k > 0. 

2.2 How are the RW1, RW2, RW3, and martingale hypotheses related (in- 2.5.5 Compute the skewness, kurtosis, and studentized range of daily 
clude a Venn diagram to illustrate the relations among the four models)? simple returns of VWRETD, EWRETD, and the ten individual stocks over 
Provide specific examples of each. the entire 1962 to 1994 sample period, and in each of the four equal 



82 2. The Predictability of Asset Returns 

subperiods. Which of the skewness, kurtosis, and studentized range esti- 
mates are statistically different from the skewness, kurtosis, and studen- 
tized range of a normal random variable at the 5% level? For these twelve 
series, perform the same calculations using monthly data. What do you Market Microstructure 
conclude about the normality of these return series, and why? 

WHILE IT IS ALWAYS the case that some features of the data will be lost in the 
process of modeling economic phenomena, determining which features 
to focus on requires some care and judgment. In exploring the dynamic 
properties of financial asset prices in Chapter 2, we have taken prices and 
returns as the principal objects of interest without explicit reference to the 
institutional structures in which they are determined. We have ignored 
the fact that security prices are generally denominated in fixed increments, 
typically eighths of a dollar or ticks for stock prices. Also, securities do not 
trade at evenly spaced intervals throughout the day, and on some days they 
do not trade at all. Indeed, the very process of trading can have an important 
impact on the statistical properties of financial asset prices: In markets with 
designated marketmakers, the existence of a spread between the price at 
which the marketmaker is willing to buy (the bid price) and the price at 
which the marketmaker is willing to sell (the offer or ask price) can have a 
nontrivial impact on the serial correlation of price 'changes. 

For some purposes, such aspects of the market's mimstructure can be 
safely ignored, particularly when longer investment horizons are involved. 
For example, it is unlikely that bid-ask bounce (to be defined in Section 
3.2) is responsible for the negative autocorrelation in the five-year returns 
of US stock indexes such as the Standard and Poor's 500,' even though 
the existence of a bid-ask spread does induce negative autocorrelation in 
returns (see Section 3.2.1). 

However, for other purposes-the measurement of execution costs and 
market liquidity, the comparison of alternative marketmaking mechanisms, 
the impact of competition and the potential for collusion among market- 
makers-market microstructure is central. Indeed, market microstructure 
is now one of the most active research areas in economics and finance, span- 

' see  Section 2.5 in Chapter 2 and Section 7.2.1 in Chapter 7 for further discussion of 
long-horizon returns. 



84 3. Market Microstructure 3.1. Nonsynchronous Trading 85 

ning many markets and many  model^.^ To test some of these models, and also induce spurious own-autocorrelation in the daily returns of A: During 
to determine the importance of market microstructure effects for other re- periods of nontrading, A's observed return is zero and when A does trade, 
search areas, we require some empirical measures of market microstructure its observed return reverts to the cumulated mean return, and this mean- 
effects. We shall construct such measures in this chapter. reversion creates negative serial correlation in A's returns. These effects 

In Section 3.1, we present a simple model of the trading process to cap- have obvious implications for tests of predictability and nonlinearity in asset 
ture the effects of nonsynchronous trading. In Section 3.2, we consider the returns (see Chapters 2 and 12), as well as for quantifying the trade-offs 
effects of the bid-ask spread on the time-series properties of price changes, between risk and expected return (see Chapters 4-6). 
and in Section 3.3 we explore several techniques for modeling transactions Perhaps the first to recognize the importance of nonsynchronous prices 
data which pose several unique challenges including price discreteness and was Fisher (1966). More recently, explicit models of nontrading have been 
irregular sampling intervals. developed by Atchison, Butler, and Simonds (1987),C ohen, Maier, Schwartz, 

and Whitcomb (1978,1979), Cohen, Hawawini, Maier, Schwartz, and Whit- 
comb (1983b), Dimson (1979), Lo and MacKinlay (1988, 1990a, 1990c), 

3.1 Nonsynchronous Trading and Scholes and Williams (1977). Whereas earlier studies considered the 
effects of nontrading on empirical applications of the Capital Asset Pricing 

The nonsynchronous trading or nontrading effect arises when time series, usu- Model and the Arbitrage Pricing ~ h e o r ym, ~or e recent attention has been 
ally asset prices, are taken to be recorded at time intervals of one length focused on spurious autocorrelations induced by nonsynchronous trading.4 
when in fact they are recorded at time intervals of other, possibly irregular, Although the various models of nontrading may differ in their specifics, they 
lengths. For example, the daily prices of securities quoted in the financial all have the common theme of modeling the behavior of asset returns that 
press are usually closing prices, prices at which the last transaction in each are mistakenly assumed to be measured at evenly spaced time intervals when 
of those securities occurred on the previous business day. These closing in fact they are not. 
prices generally do not occur at the same time each day, but by referring 
to them as "daily" prices, we have implicitly and incorrectly assumed that 

3.1.1 A Model of Nonsynchronous Trading 
they are equally spaced at 24hour intervals. As we shall see below, such an 
assumption can create a false impression of predictability in price changes Since most empirical investigations of stock price behavior focus on returns 
and returns even if true price changes or returns are statistically indepen- or price changes, we take as primitive the (unobservable) return-generating 
dent. process of a collection of N securities. To capture the effects of nontrad- 

In particular, the nontrading effect induces potentially serious biases ing, we shall follow the nonsynchronous trading model of Lo and MacKinlay 
in the moments and co-moments of asset returns such as their means, vari- (1990a) which associates with each security i in each'period t an unobserved 
ances, covariances, betas, and autocorrelation and cross-autocorrelation co- or virtual continuously compounded return ri,. These virtual returns rep- 
efficients. For example, suppose that the returns to stocks A and B are resent changes in the underlying value of the security in the absence of any 
temporally independent but A trades less frequently than B. If news affect- trading frictions or other institutional rigidities. They reflect both company- 
ing the aggregate stock market arrives near the close of the market on one specific information and economy-wide effects, and in a frictionless market 
day, it is more likely that B's end-of-day price will reflect this information these returns would be identical to the observed returns of the security. 
than A's, simply because A may not trade after the news arrives. Of course, To model the nontrading phenomenon as a purely spurious statistical 
A will respond to this information eventually but the fact that it responds artifact-not an economic phenomenon motivated by private information 
with a lag induces spurious cross-autocorrelation between the daily returns and strategic considerations-suppose in each period t there is some proba- 
of A and B when calculated with closing prices. This lagged response will bility rri that security i does not trade and whether the security trades or not 

is independent of the virtual returns {rZt(]a nd all other random variables 
 h he literature is far too vast to give a complete citation list here. In addition to the 

citations listed in each of the sections below, readers interested in an introduction to market 
microstructure are encouraged to consult the following excellent monographs and conference ?See, for example, Cohen, Hawawini, Maier, Schwartz, and Whitcomb (1983a, b),D imson 
volumes that, together, provide a fairly complete treatment of the major issues and models in (1979), Scholes and Williams (1977),a nd Shanken (1987b). 
this literature: Cohen, Maier, Schwartz, and Whitcomb (1986),D avisand Holt (1993), Frankel, 4 ~ eAetc hison, Butler, and Simonds (1987), Cohen, Maier, Schwartz, and Whitcomb (1979, 
Galli, and Giovannini (1996), Kagel and Roth (1995), Lo (1995), O'Hara (1995), and SEC 1986), and Lo and MacKinlay (1988, 1988b, 1990a, 1990~) .  
(1994). 



86 3. Market Microstructure 3.1. Nonsynchronous Trading 87 

in this m ~ d e l ) .T~he refore, this nontrading process can be viewed as an c,t-kf or all i, t ,  and k.' Each period's virtual return is random and captures 
IID sequence of coin t o ~ s e sw, ~ith  different nontrading probabilities across movements caused by information arrival as well as idiosyncratic noise. The 
securities. By allowing cross-sectional differences in the random nontrad- particular nontrading and returncumulation process we assume captures 
ing processes, we shall be able to capture the effects of nontrading on the the lag with which news and noise is incorporated into security prices due to 
returns of portfolios of securities. infrequent trading. The dynamics of such a stylized model are surprisingly 

The observed return of security i, ri",d, epends on whether security i trades rich, and they yield several important empirical implications. 
in period t .  If security i does not trade in period t ,  let its observed return be To derive an explicit expression for the observed returns process and to 
zero-if no trades occur, then the closing price is set to the previous period's deduce its timeseries properties we introduce two related random variables: 
closing price, and hence ri", = = log 1 = 0 .  If, on the other 

sit 1 (no trade) with probability ni 
hand, security i does trade in period t ,  let its observed return be the sum of = 

0 (trade) with probability 1 - n ,  (3.1.2) 
the virtual returns in period t and in all prior consecutive periods in which i 
did not trade. 

For example, consider a sequence of five consecutive periods in which 1 with probability (1- ni )n:  
security i trades in periods 1, 2, and 5, and does not trade in periods 3 and 

= I (3.1.3) 
4. The above nontrading mechanism implies that: the observed return in 0 with probability 1 - (1-ni )n:  ' 
period 2 is simply the virtual return (ri4; = ri2);t he observed returns in period where Xit(0) r 1 - S Z t ,  { J i t }  is assumed to be independent of (4,)f or i # j 
3 and 4 are both zero (r: = rg = 0 ) ;a nd the observed return in period 5 
is the sum of the virtual returns from periods 3 to 5 (T; = ri3 + ri4 + and temporally IID for each i = 1,2,.  . . , N .  

ri5).' The indicator variable takes on the value one when security i does 
This captures the essential feature of nontrading as a source of spurious not trade in period t and is zero otherwise. XZt(ki)s  also an indicator variable 
autocorrelation: News affects those stocks that trade more frequently first and takes on the value one when security i trades in period t but has not 
and influences the returns of more thinly traded securities with a lag. In this traded in any of the k previous consecutive periods, and is zero otherwise. 
framework the impact of news on returns is captured by the virtual returns Since n i  is within the unit interval, for large k the variable Xit(k)w ill be zero 
process and the impact of the lag induced by nontrading is captured by the with high probability. This is not surprising since it is highly unlikely that 
observed returns process rio,. security i should trade today but never in the past. 

To complete the specification of this nontrading model, suppose that Having defined the Xit(k)'si t is now a simple matter to derive an explicit 
virtual returns are governed by a one-factor linear model: expression for observed returns rio,: 

where 5 is some zero-mean common factor and € i t  is zero-mean idiosyncratic 
noise that is temporally and cross-sectionally independent at all leads and If security i does not trade in period t ,  then which implies that Xit(k)=O 
lags. Since we wish to focus on nontrading as the sole source of autocorrela- for all k, and thus ri",=O. If i does trade in period t ,  then its observed return 
tion, we also assume that the common factor 5 is IID and is independent of is equal to the sum of today's virtual return rit and its past kt virtual returns, 

where the random variable kt is the number of past consecutive periods that 
5 ~ hceas e where trading is correlated with virtual returns is not without interest, but it is i has not traded. We call this the duration of nontrading, which may be 

inconsistent with the spirit of the nontrading as a kind of measurement error. In the presence expressed as 
of private information and strategic behavior, trading activity does typically depend on virtual 
returns (suitably defined), and strategic trading can induce serial correlation in observed 
returns, but such correlation can hardly be dismissed as "spurious". See Section 3.1.2 for 
further discussion. 

"his assumption may be relaxed to allow for statedependent probabilities, i.e.,a utocor- Although (3.1.4) will prove to be more convenient for subsequent calcula- 
related nontrading; see the discussion in Section 3.1.2. tions, kt may be used to give a somewhat more intuitive definition of the 

'period 1's return obviously depends on how many consecutive periods prior to period 1 
that the security did not trade. If it traded in period 0, then the period-1 return is simply equal 
to its virtual return; if it  did not trade in period 0 but did trade in period - 1 ,  then period 1's "hese strong assumptions are made primarily for expositional convenience and may be 
observed return is the sum of period 0's and period 1's virtual returns; etc. relaxed considerably. See Section 3.1.2 for further discussion. 



3. Market Microstructure 3.1. Nonsynchronous Trading 89 

observed returns process: is three. As expected, if the security trades every period so that ni = 0, both 
the mean and variance of kt are zero. 

Implicationsfor Zndiuidual Security Returns 
To see how nontrading can affect the time-series properties of the observed 

Whereas (3.1.4) shows that in the presence of nontrading the observed returns of individual securities, consider the moments of r i  which, in turn, 
returns process is a (stochastic) function of all past returns, the equivalent depend on the moments of xit(k).lo For the nontrading process (3.1.2)- 
relation (3.1.6) reveals that r,", may also be viewed as a random sum with a (3.1.3), the observed returns processes {r,",)( i  = 1, . . . , N) are covariance- 
random number of terms.g stationary with the following first and second moments: 

A third and perhaps most natural way to view observed returns is the 
following: 

with probability ni Var [r:] 2ni
= ai+  - 

w ;  
1 -ni  

with probability (1 

1 Tit + rit-I with probability (1- ni)2ni 

where a: = Var [r,,] and af2 = Var [S,].  
From (3.1.9) and (3.1. lo) it is clear that nontrading does not affect the 

mean of observed returns but does increase their variance if the security has a 
Expressed in this way, it is apparent that nontrading can induce spurious nonzero expected return. Moreover, (3.1.12) shows that having a nonzero 
serial correlation in observed returns because each r,", contains within it expected return induces negative serial correlation in individual security 
the sum of past k consecutive virtual returns for every k with some positive returns at all leads and lags which decays geometrically. The intuition for 
probability (1 - nJ2n;. this phenomenon follows from the fact that during nontrading periods the 

To see how the nontrading probability ni is related to the duration of observed return is zero and during trading periods the observed return 
nontrading, consider the mean and variance of kt: reverts back to its cumulated mean return, and this mean reversion yields 

negative serial correlation. When ~ , = 0t,h ere is no mean reversion hence 
no negative serial correlation in this case. 

1f n i=i  then security i goes without trading for one period at a time on aver- Maximal Spurious Autocorrelation 
These moments also allow us to calculate the maximal negative autocorre- 

age; if xi=: then the average number of consecutive periods of nontrading 
lation attributable to nontrading in individual security returns. Since the 
autocorrelation of observed returns (3.1.12) is a nonpositive continuous 

 his is similar in spirit to the Scholes and Williams (1977) subordinated stochastic process function of ni that is zero at ni=O and approaches zero as ni approaches 
representation of observed returns, although we do not restrict the trading times to take values 
in a fixed finite interval. With suitable normalizations it may be shown that our nontrading unity, it must attain a minimum for some ni in [0,1). Determining this lower 
model converges weakly to the continuous-time Poisson process ofScholes and Williams (1977). bound is a straightforward exercise in calculus, and hence we calculate it 
From (3.1.4) the observed returns process may also be considered an infinite-order moving only for the first-order autocorrelation and leave the higher-order cases to 
average ofvirtual returns where the MA coefficients are stochastic. This is in contrast to Cohen, the reader. 
Maier, Schwartz, and Whitcomb (1986, Chapter 6) in which observed returns are assumed to be 
a finite-order MA process with nonstochastic coefficients. Although our nontrading process is 
more general, their observed returns process includes a bid-ask spread component; ours does ''TO conserve space, we summarize the results here and refer readers to Lo and MacKinlay 
not. (1990a, 1990~f)o r further details. 



3. Market Microstructure 3.1. Nonsynchronous Trading 91  

Under (3.1.2)-(3.1.3) the minimum first-order autocorrelation of the is determined by the sign of B1B1. Also, the expression is not synimetric 
observed returns process {r:} with respect to nontrading probabilities n, is with respect to i and j: If ni = 0 and nl # 0, then there is spurious cross- 

autocovariance between r: and $+,b ut no cross-autocovariance between 
ro and r:+, for any n > 0." The intuition for this result is simple: Wherl 
I l  

security j exhibits nontrading, the returns to a constantly trading security i 
can forecast j due to the common factor J present in both returns. That 

where 6; - pi/a, ,  and the minimum is attained at j exhibits nontrading implies that future observed returns $+"w ill be a 
weighted average of all past virtual returns r j t+"-k  (with the 4,+,,(k)'sa s ran- 
dom weights), of which one term will be the current virtual return ? , Since 
the contemporaneous virtual returns rit and 5, are correlated (because of 

Over all values of n, E [O,1) and tiE  (-co, +co), we have the common factor), ri"c, an forecast rio,+n However, r; is itself unforecastable 
because rz = ri, for all t (since n; = 0) and rit is IID by assumption, thus ro 

1 
Inf Corr[r,:, = -- is uncorrelated with rz+, for any n > 0. I t  

{n,.trl 2'  The asymmetry of (3.1.11 ) yields an empirically testable restriction on 
the cross-autocovariances of returns. Since the only source of asymmetry 

which is the limit of (3.1.13) as (tii(n creases without bound, but is never 
attained by finite ti. in (3.1.11 ) is cross-sectional differences in the probabilities of nontrading, 

information regarding these probabilities may be extracted from sample 
Although the lower bound of -:s eems quite significant, it is virtually moments. Specifically, denote by ry the vector [ rfl r& . . . r i ,  1' of observed 

unattainable for any empirically plausible parameter values. For example, returns of the N securities and define the autocovariance matrix I', as 
if we consider a period to be one trading day, typical values for p ,  and 
a ,  are .05% and 2.576, respectively, implying a typical value of 0.02 for c,. 
According to (3.1.13), this would induce a spurious autocorrelation of at 

Denoting the (i,j )th element of I', by yV(n),w e have by definition 
most -0.037% in individual security returns and would require a nontrading 
probability of 97.2% to attain, which corresponds to an average nontrading 
duration of 35.4 days! 

These results also imply that nontrading-induced autocorrelation is 
magnified by taking longer sampling intervals since under the hypothe- If the nontrading probabilities ni differ across securities, I', is asymmetric. 
sized virtual returns process, doubling the holding period doubles but From (3.1 .l7) it is evident that 
only multiplies a ,  by a factor of a.T herefore more extreme negative au- 
tocorrelations are feasible for longer-horizon individual returns. However, 
this is not of direct empirical relevance since the effects of time aggrega- 
tion have been ignored. To see how, observe that the nontrading process Therefore relative nontrading probabilities may be estimated directly using 
(3.1.2)-(3.1.3)i s not independent of the sampling interval but changes in sample autocovariances f',. To derive estimates of the probabilities ni them- 
a nonlinear fashion. For example, if a period is taken to be one week, selves we need only estimate one such probability, say T I ,a nd the remaining 
the possibility of daily nontrading and all its concomitant effects on weekly probabilities may be obtained from the ratios (3.1.18). A consistent estima- 
observed returns is eliminated by assumption. A proper comparison of ob- tor of nl is readily constructed with sample means and autocovariances via 
served returns across distinct sampling intervals must allow for nontrading at (3.1.11). 
the finest time increment, after which the implications for coarser-sampled 
returns may be developed. We shall postpone further discussion of this and " ~ anlte rnative interpretation of this asymmetry may be found in the timeseries literature 

concerning Grangercausality (see Granger [1969]), in which 1;'; is said to Cranger-cause r,: if 
other issues of time aggregation until later in this section. the return to i predicts the return to j. In the above example, security i Grangemauses security 

j when j is subject to nontrading but i is not. Since our nontrading process may be viewed as 
Asymmetric Cross-Autocovariances a form of measurement error, the fact that the returns to one security may be exogenous with 
Several other important empirical implications of this nontrading model respect to the returns of another has been proposed under a different guise in Sims (1974, 

1977). 
are captured by (3.1.1 1)  . In ~articulart,h e sign of the cross-autocovariances 



3. Market Microstructure 3.1. Nonsynchronous Trading 93 

Implications fm Portfolio Returns for K = a, 6. The first and second moments of the portfolios' returns are 
Suppose securities are grouped by their nontrading probabilities and equal- 
weighted portfolios are formed based on this grouping so that portfolio A 
contains N, securities with identical nontrading probability n,, and similarly 
for portfolio B. Denote by r,", and rb",t he observed time-t returns on these two 
portfolios respectively, which are approximately averages of the individual 
returns: 

where the summation is over all securities i in the set of indices I, which 
comprise portfolio K.  The reason (3.1.19) is not exact is that both observed 
and virtual returns are assumed to be continuously compounded, and the 
logarithm of a sum is not the sum of the logarithms.12 However, if ri", takes 
on small values and is not too volatile-plausible assumptions for the short 
return intervals that nonsynchronous trading models typically focus on- where the symbol "A"i ndicates that the equality obtains only asymptotically. 
the approximation error in (3.1.19) is negligible. From (3.1.22) we see that observed portfolio returns have the same 

The timeseries properties of (3.1.19) may be derived from a simple mean as the corresponding virtual returns. In contrast to observed individ- 
asymptotic approximation that exploits the cross-sectional independence ual returns, the variance of rzt is lower asymptotically than the variance of 
of the disturbances €it. Similar asymptotic arguments can be found in the its virtual counterpart rats ince 
Arbitrage Pricing Theory (APT) literature (see Chapter 6); hence our as- 
sumption of independence may be relaxed to the same extent that it may 
be relaxed in studies of the APT in which portfolios are required to be 
"~elldiversified."'~I n such cases, as the number of securities in portfolios 
A and B (denoted by Na and Nb,r espectively) increases without bound, the 
following equalities obtain almost surely: where (3.1.28) follows from the law of large numbers applied to the last 

term in (3.1.27). Thus Var[rat] A B:c~,w hich is greater than or equal to 
Var [r;,] . 

Since the nontrading-induced autocorrelation (3.1.25) declines geo- 
metrically, observed portfolio returns follow a first-order autoregressive pro- 

where cess with autoregressive coefficient equal to the nontrading probability. In 
contrast to expression (3.1.11 ) for individual securities, the autocorrelations 
of observed portfolio returns do not depend explicitly on the expected re- 
turn of the portfolio, yielding a much simpler estimator for IT,: the nth 
root of the nth order autocorrelation coefficient. Therefore, we may easily 

''A precise interpretation of r,O, is the return to a portfolio whose value is calculated as 
an unweighted geometric average of the component securities' prices. The expected return estimate all nontrading probabilities by using only the sample first-order 
of such a portfolio will be lower than that of an equal-weighted portfolio whose returns are own-autocorrelation coefficients for the portfolio returns. 
calculated as the arithmetic means of the simple returns of the component securities. This Comparing (3.1.26) to (3.1.11) shows that the cross-autocovariance be- 
issue is examined in greater detail by Modest and Sundaresan (1983) and Eytan and Harpaz tween observed portfolio returns takes the same form as that of observed 
(1986) in the context of the Value Line Index which was an unweighted geometric average 
until 1988. individual returns. If there are differences across portfolios in the nontrad- 

I3see, for example, Chamberlain (1983a),C hamberlain and Rothschild (1983). and Wang ing probabilities, the autocovariance matrix for observed portfolio returns 
(1993). The essence of these weaker conditions is simply to allow a Law of Large Numbers to will be asymmetric. This may give rise to the types of lead-lag relations 
be applied to the average of the disturbances, so that "idiosyncratic risk" vanishes almost surely empirically documented by Lo and MacKinlay (1988) in sizesorted portfo- 
as the cross section grows. 



94 3. Market Microstructure 3.1. Nonsynchronous Trading 

lios. Ratios of the cross-autocovariances may be formed to estimate relative 
nontrading probabilities for portfolios, since 

In addition, for purposes of testing the overall specification of the non- 
trading model, these ratios give rise to many over-identifying restrictions, 
since 

where ti-  pi/oi as before. 
for any arbitrary sequence of distinct indices ~ 1KZ,,  .  . . , Although expected returns time-aggregate linearly, (3.1.33) shows that 

K,, a # b, r 5 Np, 
where Np is the number of distinct portfolios and yKLKJ=(n )C ov[r&, r;,+,]. variances do not. As a result of the negative serial correlation in rz, the 

variance of a sum is less than the sum of the variances. Time aggregation 
Therefore, although there are Ni distinct autocovariances in I?, the restric- does not affect the sign of the autocorrelations in (3.1.35) although their 
tions implied by the nontrading process yield far fewer degrees of freedom. 

magnitudes do decline with the aggregation value q. As in (3.1.12), the auto- 
correlation of time-aggregated returns is a nonpositive continuous function 

Time Aggregation of ni on [O, 1) which is zero at ni = 0 and approaches zero as ni approaches 
The discrete-time framework we have adopted so far does not require the unity, and hence it attains a minimum. 
specification of the calendar length of a "period." This advantage is more To explore the behavior of the first-order autocorrelation, we plot it as 
apparent than real since any empirical implementation of the nontrading a function of nii n Figure 3.1 for a variety of values of q and 6: q takes on the 
model (3.1.2)-(3.1.3) must either implicitly or explicitly define a period to values 5, 22, 66, and 244 to correspond to weekly, monthly, quarterly, and 
be a particular fixed calendar time interval. Once the calendar time interval annual returns, respectively, since q = 1 is taken to be one day, and 
has been chosen, the stochastic behavior of coarser-sampled data is restricted takes 

on the values 0.09,0. 16, and 0.21 to correspond to daily, weekly, and monthly 
by the parameters of the most finely sampled process. For example, if the returns, respectively.14F igure 3. l a  plots the first-order autocorrelation pl ( p )  
length of a period is taken to be one day, then the moments of observed for the four values of q with C = 0.09. The curve marked "q = 5" shows that 
monthly returns may be expressed as functions of the parameters of the the weekly first-order autocorrelation induced by nontrading never exceeds 
daily observed returns process. We derive such restrictions in this section. -5% and only attains that value with a daily nontrading probability in excess 

To do this, denote by rL(q) the observed return of security i at time t of 90%. 
where one unit of t-time is equivalent to q units of t-time, thus: Although the autocorrelation of coarser-sampled returns such as 

monthly or quarterly have more extreme minima, they are attained only 
rl; (q) - C r;. at higher nontrading probabilities. Also, time-aggregation need not always 

t=(r-l)q+l yield a more negative autocorrelation, as is apparent from the portion of 
the graphs to the left of, say, n = .80; in that region, an increase in the 

Then under the nontrading process (3.1.2)-(3.1.3), it can be shown that aggregation value q leads to an autocorrelation closer to zero. Indeed as q 
the time-aggregated observed returns processes {rL( q)} ( i  = 1, . . . , N) are increases without bound the autocorrelation (3.1.35) approaches zero for 
covariance-stationaryw ith the following first and second moments (see LO fixed n;,a nd thus nontrading has little impact on longer-horizon returns. 
and MacKinlay [ 1990aI)  : 

I4values for 6 were obtained by taking the ratio of the sample mean to the sample standard 
deviation for daily, weekly, and monthly equal-weighted stock returns indexes for the sample 
period from 1962 to 1987 as reported in Lo and MacKinlay (1988, Tables la-c). Although 
these values may be more representative of stock indexes rather than individual securities, 
nevertheless for the sake of illustration they should suffice. 



3.1. Nonsynchronous Trading 

The effects of increasing .$ are traced out in Figures 3.lb and 3. lc. Even 
if we assume = 0.21 for daily data, a most extreme value, the nontrading- 
induced autocorrelation in weekly returns is at most -8% and requires a 
daily nontrading probability of over 90%. From (3.1.8) we see that when 
n, = .90 the average duration of nontrading is nine days! Since no security 
listed on the New York or American Stock Exchanges is inactive for two 
weeks (unless it has been delisted), we infer from Figure 3.1 that the impact 
of nontrading for individual short-horizon stock returns is negligible. 

Time Aggregation Fw Pmtfolios 
Similar time-aggregated analytical results can be derived for observed port- 
folio returns. Denote by r: (q) the observed return of portfolio A at time t 
where one unit of t-time is equivalent to q units of t-time; thus 

where r t: is given by (3.1.19). Then under (3.1.2)-(3.1.3) the observed 
portfolio returns processes {r&( q)} and {r; (q)} are covariance-stationary 
with the following first and second moments as N, and Nb increase without 
bound: 

na(l-n~)(1-nb)2+n~(l-n~)(l-n,)2 
[q - (l-no)(l-nb) 1- for n = 0 

(3.1.42) 
(1-n,.-)(,l.-,n b) [ % ng-q+l 

I-,h I n b  a.abu; for n > o 
for K = a, 6 ,  q > 1, and arbitrary portfolios a, 6 ,  and time r .  



98 3. Market Microstructure 3.2. The Bid-Ask S F a d  99 

Equation (3.1.40) shows that time aggregation also affects the autocor- tomorrow depends on whether or not a trade occurs today. Although this 
relation of observed portfolio returns in a highly nonlinear fashion. In specification does admit compact and elegant expressions for the moments 
contrast to the autocorrelation for time-aggregated individual securities, of the observed returns process, we shall leave their derivation to the reader 
(3.1.40) approaches unity for any fixed q as n, approaches unity; therefore (see Problem 3.3). However, a brief summary of the implications for the 
the maximal autocorrelation is one. time-series properties of observed returns may be worthwhile: (1) Individ- 

To investigate the behavior of the portfolio autocorrelation we plot it ual security returns may be positively autocorrelated and portfolio returns 
as a function of the portfolio nontrading probability n in Figure 3.ld for may be negatively autocorrelated, but these possibilities are unlikely given 
q = 5, 22, 66, and 55. Besides differing in sign, portfolio and individ- empirically relevant parameter values; (2) It is possible, but unlikely, for 
ual autocorrelations also differ in absolute magnitude, the former being autocorrelation matrices to be symmetric; and (3) Spurious index autocor- 
much larger than the latter for a given nontrading probability. If the non- relation induced by nontrading is higher (or lower) when there is positive 
trading phenomenon is extant, it will be most evident in portfolio returns. (or negative) persistence in nontrading. In principle, property (3) might 
Also, portfolio autocorrelations are monotonically decreasing in q so that be sufficient to explain the magnitude of index autocorrelations in recent 
time aggregation always decreases nontrading-induced serial dependence stock market data. However, several calibration experiments indicate the 
in portfolio returns. This implies that we are most likely to find evidence of degree of persistence in nontrading required to yield weekly autocorrela- 
nontrading in short-horizon returns. We exploit both these implications in tions of 30% is empirically implausible (see Lo and MacKinlay [1990c] for 
the empirical analysis of Section 3.4.1. details). 

One final direction for further investigation is the possibility of depen- 
dence between the nontrading and virtual returns processes. If virtual re- 

3.1.2E xtensions and Generalizations turns are taken to be new information then the extent to which traders 
Despite the simplicity of the model of nonsynchronous trading in Section exploit this information in determining when (and what) to trade will show 
3.1.1, its implications for observed time series are surprisingly rich. The itself as correlation between rit and Jjt. Many strategic considerations are 
framework can be extended and generalized in many directions with little involved in models of information-based trading, and an empirical analysis 
difficulty. of such issues promises to be as challenging as it is exciting.16 

It is a simple matter to relax the assumption that individual virtual re- However, if it is indeed the case that return autocorrelation is induced 
turns are IID by allowing the common factor to be autocorrelated and the by information-based nontrading, in what sense is this autocorrelation spu- 
disturbances to be cross-sectionally correlated. For example, allowing J1 rious? The premise of the extensive literature on nonsynchronous trading 
to be a stationary AR(1) is conceptually straightforward, although the cal- is that nontrading is an outcome of institutional features such as lagged ad- 
culations become somewhat more involved. This specification will yield a justments and nonsynchronously reported prices. But if nonsynchronicity 
decomposition of observed autocorrelations into two components: one due is purposeful and informationally motivated, then the serial dependence it 
to the common factor and another due to nontrading. induces in asset returns should be considered genuine, since it is the result 

Allowing cross-sectional dependence in the disturbances also compli- of economic forces rather than measurement error. In such cases, purely 
cates the moment calculations but does not create any intractabilities.15 statistical models of nontrading are clearly inappropriate and an economic 
Indeed, generalizations to multiple factors, time-series dependence of the model of strategic interactions is needed. 
disturbances, and correlation between factors and disturbances are only lim- 
ited by the patience and perseverance of the reader; the necessary moment 
calculations are not intractable, but merely tedious. 3.2 The Bid-Ask Spread 

Dependence can be built into the nontrading process itself by assuming 
that the &'s are Markov chains, so that the conditional probability of trading One of the most important characteristics that investors look for in an or- 

ganized financial market is liquidity, the ability to buy or sell significant 

'%w e discussed earlier, some form of cross-sectional weak dependence must be imposed 
so that the asymptotic arguments of the portfolio results still obtain. Of course, such an '%orne good illustrations of the kind of trading behavior that can arise from strategic 
assumption may not always be appropriate as, for example, in the case of companies within the considerations are contained in Admati and Pfleiderer (1988,1989),B ertsimas and Lo (1996), 
same industry, whose residual risks we might expect to be positively correlated. Therefore, the Easley and O'Hara (1987, l9O), Kyle (l985),a nd Wang (1993, 1994). 
asymptotic approximation will be most accurate for well-diversified portfolios. 



100 3. Market Microstructure 3.2. The Bid-Ask Spread 101 

quantities of a security quickly, anonymously, and with relatively little price 3.2.1 Bid-Ask Bounce 
impact. To maintain liquidity, many organized exchanges use marketmak- To account for the impact of the bid-ask spread on the time-series properties 
ers, individuals who stand ready to buy or sell whenever the public wishes of asset returns, Roll (1984) proposes the following simple model. Denote 
to sell or buy. In return for providing liquidity, marketmakers are granted by P,* the time-t fundamental value of a security in a frictionless economy, 
monopoly rights by the exchange to post different prices for purchases and and denote by s the bid-ask spread (see Glosten and Milgrom [1985], for 
sales: They buy at the bid price Pb and sell at a higher ask price Pa. This example). Then the observed market price P, may be written as 
ability to buy low and sell high is the marketmaker's primary source of com- 
pensation for providing liquidity, and although the bid-ask spread Pa - Pbi s 
rarely larger than one or two ticks-the NYSE Fact Book: 1994 Data reports 
that the spread was $0.25 or less in 90.8% of the NYSE bid-ask quotes from I +1 with probability i (buyer-initiated) 
1994--over a large number of trades marketmakers can earn enough to It IID (3.2.2) 
compensate them for their services. -1  with probability i (seller-initiated) 

The diminutive size of typical spreads also belies their potential im- 
portance in determining the time-series properties of asset returns. For where It is an order-type indicator variable, indicating whether the transac- 
example, Phillips and Smith (1980) show that most of the abnormal re- tion at time t is at the ask (buyer-initiated) or at the bid (seller-initiated) 
turns associated with particular options trading strategies are eliminated price. The assumption that P,* is the fundamental value of the security 
when the costs associated with the bid-ask spread are included. Blume implies that E[I,] = 0, hence Pr(It=l) = Pr(I,= - 1) = i. Assume for 
and Stambaugh (1983) argue that the bid-ask spread creates a significant the moment that there are no changes in the fundamentals of the security; 
upward bias in mean returns calculated with transaction prices. More re- hence P,*=  P* is fixed through time. Then the process for price changes 
cently, Keim (1989) shows that a significant portion of the so-called January Apt is given by 
effect-the fact that smaller-capitalization stocks seem to outperform larger 
capitalization stocks over the few days surrounding the turn of the year- 
may be attributable to closing prices recorded at the bid price at the end 
of December and closing prices recorded at the ask price at the begin- and under the assumption that It is IID the variance, covariance, and auto- 
ning of January. Even if the bid-ask spread remains unchanged during this correlation of Apt may be readily computed 
period, the movement from bid to ask is enough to yield large portfolio 
returns, especially for lower-priced stocks for which the percentage bid-ask 
spread is larger. Since low-priced stocks also tend to be low-capitalization 
stocks, Keim's (1989) results do offer a partial explanation for the January 
effect." 

The presence of the bid-ask spread complicates matters in several ways. 
Instead of one price for each security, there are now three: the bid price, 1 
the ask price, and the transaction price which need not be either the bid Corr[ Aptpi,  AP, ] = -- (3.2.7) 
or the ask (although in some cases it is), nor need it lie in between the two 2 '  
(although in most cases it does). How should returns be calculated, from Despite the fact that fundamental value P,* is fixed, Apt exhibits volatility 
bid-to-bid, ask-to-bid, etc.? Moreover, as random buys and sells arrive at and negative serial correlation as the result of bid-ask bounce. The intuition 
the market, prices can bounce back and forth between the ask and the bid is clear: If P* is fixed so that prices take on only two values, the bid and 
prices, creating spurious volatility and serial correlation in returns, even if the ask, and if the current price is the ask, then the price change between 
the economic value of the security is unchanged. the current price and the previous price must be either 0 or s and the price 

change between the next price and the current price must be either 0 or -s. 
The same argument applies if the current price is the bid, hence the serial 

" ~ e i m(1 989) also documents the relation between other calendar anomalies (the weekend correlation between adjacent price changes is nonpositive. This intuition 
effect, holiday effects, etc.) and systematic movements between the bid and ask prices. 



102 3. Market Microstructure 3.2. The Bid-Ask Spread 103 

applies more generally to cases where the order-type indicator It is not IID," accounting for the effects of the bid-ask spread on the time-series properties 
hence the model is considerably more general than it may seem. of asset returns. 

The larger the spread s, the higher the volatility and the first-order 
autocovariance, both increasing proportionally so that the first-order auto- 3.2.2 Components of the Bid-Ask Spread 
correlation remains constant at -;. Observe from (3.2.6) that the bid-ask 
spread does not induce any higher-order serial correlation. Although Roll's model of the bid-ask spread captures one important aspect 

Now let the fundamental value P : change through time, but suppose of its effect on transaction prices, it is by no means a complete theory of 
that its increments are serially uncorrelated and independent of It.'' Then the economic determinants and the dynamics of the spread. In particular, 

Roll (1984) takes s as given, but in practice the size of the spread is the 
-(3.:2.

b 5) still applies, but the first-order autocorrelation (3.2.7) is no longer 
ecause of the additional variance of APT in the denominator. Specifi- single most important quantity that marketmakers control in their strategic 

cally if a2 (aP*)is  the variance of APT, then interactions with other market participants. In fact, Glosten and Milgrom 
(1985) argue convincingly that s is determined endogenously and is unlikely 
to be independent of P* as we have assumed in Section 3.2.1. 

Other theories of the marketmaking process have decomposed the 
spread into more fundamental components, and these components often 

Although (3.2.5) shows that a given spread s implies a first-order autoco- behave in different ways through time and across securities. Estimating the 
variance of -s2/4, the logic may be reversed so that a given autocovariance separate components of the bid-ask spread is critical for properly imple- 
coefficient and value of p imply a particular value for s. Solving for s in menting these theories with transactions data. In this section we shall turn 
(3.2.5) yields to some of the econometric issues surrounding this task. 

s = 2J - C~V[AP, -A~p, t ] , (3.2.9) There are three primary economic sources for the bid-ask spread: order- 
processing costs, inventory costs, and adverse-selection costs. The first two 

hence s may be easily estimated from the sample autocovariances of price consist of the basic setup and operating costs of trading and recordkeeping, 
changes (see the discussion in Section 3.4.2 regarding the empirical imple- and the carrying of undesired inventory subject to risk. Although these costs 
mentation of (3.2.9) for further details). have been the main focus of earlier ~iterature,~it'  is the adverse-selection 

Estimating the bid-ask spread may seem superfluous given the fact that component that has received much recent a t tent i~n.~A'  dverse selection 
bid-ask quotes are observable. However, Roll (1984) argues that the quoted costs arise because some investors are better informed about a security's 
spread may often differ from the effective spread, i.e., the spread between value than the marketmaker, and trading with such investors will, on av- 
the actual market prices of a sell order and a buy order. In many instances, erage, be a losing proposition for the marketmakes. Since marketmakers 
transactions occur at prices within the bid-ask spread, perhaps because mar- have no way to distinguish the informed from the uninformed, they are 
ketmakers do not always update their quotes in a timely fashion, or because forced to engage in these losing trades and must be rewarded accordingly. 
they wish to rebalance their own inventory and are willing to "better" their Therefore, a portion of the marketmaker's bid-ask spread may be viewed 
quotes momentarily to achieve this goal, or because they are willing to pro- as compensation for taking the other side of potential information-based 
vide discounts to customers that are trading for reasons other than private trades. Because this information component can have very different statis- 
information (see Eikeboom [1993], Glosten and Milgrom [1985], Goldstein tical properties from the order-processing and inventory components, it is 
[1993], and the discussion in the next section for further details). Roll's critical to distinguish between them in empirical applications. To do so, 
(1984) model is one measure of this effective spread, and is also a means for Glosten (1987) provides a simple asymmetric-information model that cap- 

tures the salient features of adverse selection for the components of the 

g or bid-ask spread, and we shall present an abbreviated version of his elegant 
example, serial correlation in I, (of either sign) does not change the fact that bid- 

ask bounce induces negative serial correlation in price changes, although it does affect the analysis here (see, also, Glosten and Harris [I9881 and Stoll [1989]). 
magnitude. See Choi, Salandro, and Shastri (1988) for an explicit analysis of this case. 

'%oil (1984) argues that price changes must be serially uncorrelated in an informationally 20 See, for example, Amihud and Mendelson (1980), Bagehot (1971), Dernsetz (1968), Ho 
efficient market. However, Leroy (1973), Lucas (1978), and others have shown that this need and Stdl (1981),S toll (1978),a nd Tinie (1972). 
not be the case. Nevertheless, for short-horizon returns, e.g., daily or intradaily returns, it 21 See Bagehot (lf171), Copeland and Galai (1983), Easley and O'Hara (1987). Glosten 
is difficult to pose an empirically ~lausiblee quilibrium model of asset returns that exhibits (1987),G losten and Harris (1988),G losten and Milgrom (l985),a nd Stoll (1989). 
significant serial correlation. 



104 3. Market Microstructure 3.2. The Bid-Ask Spread 105 

Glosten s' Decomposition An immediate implication of (3.2.16)a nd (3.2.17)i s that only a portion of 
the total spread, Ca+Cb, covers the basic costs of marketmaking, so that 

Denote by Pb and Pa the bid and ask prices, respectively, and let P be the 
"true" or common-information the quoted spread Aa+Ab+C,+Cb can be larger than Stoll's (1985) "ef- 

market price, the price that all investors with- 
fective" spread-the spread between purchase and sale prices that occur 

out private information (uninformed investors) agree upon. Under risk- 
neutrality, the common-information price is given by P = E[P*1 521 strictly within the quoted bid-ask spread-the difference being the adverse- 

where 
R selection component A,+Ab. This accords well with the common practice 

denotes the common or public information set and P* denotes the price 
of marketmakers giving certain customers a better price than the quoted 

that would result if everyone had access to all information. The bid and ask 
bid or ask on certain occasions, presumably because these customers are 

prices may then be expressed as the following sums: 
~erceivedt o be trading for reasons other than private information, e.g., 
liquidity needs, index-portfolio rebalancing, etc. 

Implications for Transaction Prices 
To derive the impact of these two components on transaction prices, denote 
by Fn the price at which the nth transaction is consummated, and let 

where A,+Ab is the adverse-selection component of the spread, to be de- 
termined below, and Ca+Cb includes the order-processing and inventory 
components which Glosten calls the p s pr ojit component and takes as 
exogenous.22 If uninformed investors observe a purchase at the ask, then where I, (Ib)i s an indicator function that takes on the value one if the trans- 
they will revise their valuation of the asset from P to P+Aa to account for action occurs at the ask (bid) and zero otherwise. Substituting (3.2.16)- 
the possibility that the trade was information-motivated, and similarly, if a (3.2.17) into (3.2.18)t hen yields 
sale at the bid is observed, then P will be revised to P-Ab. But how are A, 
and Ab determined? 

Glosten assumes that all potential marketmakers have access to com- 
mon information only, and he defines their updating rule in response to 
transactions at various possible bid and ask prices as 

Ca if buyer-initiated trade 
a ) =  E[ P* 1 R U { investor buys at x) ] (3.2.13) Cn = (3.2.22) 

if seller-initiated trade 

b(y) = E[ P* 1 R U { investor sells at y) 1. (3.2.14) a = (+ 1   if buyer-initiated trade ' 

- 1 if seller-initiated trade (3.2.23) 

A, and Ab are then given by the following relations: where A is the event in which the transaction occurs at the ask and B is 

A, = a(P,) - P,  Ab = P - b(Pb). (3.2.15) the event in which the transaction occurs at the bid. Observe that P, is the 
common information price after the nth transaction. 

Although (3.2.20)i s a decomposition that is frequently used in this liter- 
Under suitable restrictions for a( . )a nd b(.) ,a n equilibrium among compet- ature, Glosten's model adds an important new feature: correlation between 
ing marketmaken will determine bid and ask prices so that the expected P, and a.I f P is the common information price before the nth transaction 
profits from marketmaking activities will cover all costs, including Ca+Cb and P n  is the common information price afterwards, Glosten shows that 
and A,+Ab; hence 

Cov[Pn, a l p ]  = E[AIP] where A = A, if & = + I  (3.2.24) 
Ab if a = - 1 .  

That P n  and a must be correlated follows from the existence of adverse 
selection. If &= + 1 ,  the possibility that the buyer-initiated trade is infor- 

2 2 ~ eAer nihud and Mendelson (1980);C ohen, Maier, Schwartz, and Whitcomb (1981);H o mation-based will cause an upward revision in P, and for the same reason, 
and Stoll (1981);a nd Stoll (1978) for models of these costs. 



106 3. Market Microstructure 3.3. Modeling Transactions Data 107 

4,= - 1 will cause a downward revision in P. There is only one case in which component is economically important is largely an empirical issue that has 
Pn and 4, are uncorrelated: when the adverse-selection component of the yet to be determined decisively,24n evertheless Glosten's (1987) model shows 
spread is zero. that adverse selection can have very different implications for the statistical 

properties of transactions data than other components of the bid-ask spread. 
Implications for Transaction Price Dynamics 
To derive implications for the dynamics of transactions prices, denote by rn  
the revisions in Pn-l due to the arrival of new public information between 3.3 Modeling Transactions Data 
trades n-1 and n. Then the nth transaction price may be written as 

One of the most exciting recent developments in empirical finance is the 
availability of lowcost transactions databases: historical prices, quantities, 
bid-ask quotes and sizes, and associated market conditions, transaction by 

Taking the first difference of (3.2.20) then yields transaction and time-stamped to the nearest second. For example, the 
NYSE's Trades and Quotes (TAQ database contains all equity transactions 
reported on the Consolidated Tape from 1992 to the present, which includes 
all transactions on the NYSE, AMEX, NASDAQ, and the regional exchanges. 
The Berkeley Options Database provides similar data for options transac- 
tions, and transactions databases for many other securities and markets are 

which shows that transaction price changes are comprised of a gross-profits being developed as interest in market microstructure issues continues to 
component which, like Roll's (1984) model of the bid-ask spread, exhibits grow. 
reversals, and an adverse-selection component that tends to be permanent. The advent of such transactions databases has given financial economists 
Therefore, Glosten's attribution of the effective spread to the gross-profits the means to address a variety of issues surrounding the fine structure of 
component is not coincidental, but well-motivated by the fact that it is the trading process or price discovery. For example, what are determinants 
this component that induces negative serial correlation in returns, not the of the bid-ask spread, and is adverse selection a more important factor than 
adverseselection component. Accordingly, Glosten (1987) provides alter- inventory costs in explaining marketmaking behavior?25 Does the very act 
native relations between spreads and return covariances which incorporate of trading move prices, and if so, how large is this price impact effect and how 
this distinction between the adverse-selection and gross-profits components. does it vary with the size of the trade?26 Why do prices tend to fall more 
In particular, under certain simpliQing assumptions Glosten shows that23 often on whole-dollar multiples than on half-dollar multiples, more often 

on half-dollar multiples than on quarterdollar multiples, e t ~ . ? ~W'h at are 
the benefits and costs of other aspects of a market's microstructure, such as 
margin requirements, the degree of competition faced by dealers, the fre- 

where quency that orders are cleared, and intraday volatility?28A lthough none of 

24~ecenattt empts to quantify the relative contributions of order-processing/inventory costs 
and adverse selection costs to the bid-ask spread include: Affleck-Graves, Hegde, and Miller 
(1994), Glosten and Harris (1988),George, Kaul, and Nimalendran (1991), Huang and St011 
(1995a), and Stoll (1989). See Section 3.4.2 for further discussion. 

and where &, R, are the per-period market and true returns, respectively, 25 See Arnihud and Mendelson (1980), Bagehot (1971), Copeland and Galai (1983), Dem- 

and Fk is the continuously compounded per-period market return. Setz (19681, Easley and O'Hara (1987), Glosten (1987), Glosten and Harris (1988), Glosten 
and Milgrom (1985), Ho and Stoll (1981),S toll (1978, 1989), and Tin i~(1 972). 

These relations show that the presence of adverse selection ( y c 1) has an 26 See Bertsimas and Lo (1996), Chan and Lakonishok (1993b,1995),a nd Keim and Mad- 
additional impact on means and covariances of returns that is not captured hamn (1995a,b, 1996). 
by other models of the bid-ask spread. Whether or not the adverse-selection "See Ball, Torous, and Tschoegl (1985); Christie, Harris, and Schultz (1994); Christie 

and Schultz (1994); Goodhart and Curcio (1990); Harris (1991); Niederhoffer (1965, 1966); 
Niederhoffer and Osborne (1966); and Osborne (1962). 

'%pecifically, he assumes that: ( 1 )  True returns are independent of all past history; (2) 28 See Cohen, Maier, Schwartz, and Whitcomb (1986),H arris, Sofianos, and Shapiro (1994), 
The spread is symmetric about the true price; and (3) The gross-profit component does not Hasbrouck (1991a, b), Madhavan and Smidt (1991). and Stoll and Whalev (1990). 
cause conditional drift in prices. 



108 3. Market Microstructure 3.3. Mo&lzng Transactions Data 

these questions are new to the recent literature, the kind of answers we can Table 3.1. Summary statisticsfor daily return ofJiue NYSE stocks. 
provide have changed dramatically, thanks to transactions data. Even the 
event study, which traditionally employs daily returns data, has been applied Statistic AAC APD CBS CCB KAB 
recently to transactions data to sift out the impact of news announcements 
within the day (see, for example, Barclay and Litzenberger [I9881)  . 

The richness of these datasets does not come without a price-trans- 
actions datasets are considerably more difficult to manipulate and analyze 
because of their sheer size. For example, in 1994 the NYSE consummated 
over 49 million transactions, and for each transaction, the NYSE's Trades 
and Quotes (TAQ)d atabase records several pieces of information: transac- 
tion price, time of trade, volume, and various condition codes describing 
the trade. Bid-ask quotes and depths are also recorded. Even for indi- 
vidual securities, a sample size of 100,000 observations for a single year of 
transactions data is not unusual. 

3.3.1 Motivation 

Transactions data pose a number of unique econometric challenges that Summary statistics for daily returns data from January 2, 1990, to December 31, 1992, for five 
do not easily fit into the framework we have developed so far. For exam- W E st ocks: AAC = Anacomp; APD = Air Products and Chemicals; CBS = Columbia 
ple, transactions data are sampled at irregularly spaced random intervals- Broadcasting System; CCB = Capital Cities ABC; KAB = Kaneb Services. 
whenever trades occur-and this presents a number of problems for stan- 
dard econometric models: observations are unlikely to be identically dis- 
tributed (since some observations are very closely spaced in time while others discreteness is less problematic for coarser-sampled data, which may be well- 
may be separated by hours or days), it is difficult to capture seasonal effects approximated by a continuous-state process. But it becomes more relevant 
(such as time-ofday regularities) with simple indicator functions, and fore- for transaction price changes, since such finely sampled price changes typ- 
casting is no longer a straightforward exercise because the transaction times ically take on only a few distinct values. For example, the NYSE Fact Book: 
are random. 1994 Data reports that in 1994, 97.4% of all transactions on the NYSE oc- 

Also, transaction prices are always quoted in discrete units or t i c k  curred with no change or a one-tick price change. Moreover, price changes 
currently $0.125 for equities, $0.0625 for equity options, $0.05 for futures greater than 4 ticks are extremely rare, as documented in Hausman, Lo, 
contracts on the Standard and Poor's 500 index, $0.03125 for US Treasury and MacKinlay (1992). 
bonds and notes, and so on. While there are no a prim-i theoretical reasons 
to rule out continuous prices, the transactions costs associated with quot- Disrreteness and Prices 
ing and processing such prices make them highly impractical.2gO f course, Discreteness affects both prices and returns, but in somewhat different ways. 

respect to prices, several studies have documented the phenomenon 
2g~espi tteh e indivisibilities that accompany price discreteness, there seems to be general of @ce clustering, the tendency for prices to fall more frequently on certain 

agreement among economists and practitioners alike that the efficiency gains from discrete values than on others.30 For example, Figure 3.2a displays the histograms 
prices far outweigh the potential costs of indivisible trading lots. However, an unresolved of the fractional part of the daily closing prices of the following five NYSE 
issue is the optimal degree of discreteness, which balances the costs of indivisibilities against the 
benefits of discreteness. For example, on the NYSE, the minimum price movement of stocks stocks during the three-year period from January 2, 1990, to December 31, 
with prices greater than or equal to $1 is one tick, but this minimum price variation was set 1992 (see Table 3.1 for summary statistics): Anacomp (AAC), Air Products 
years ago before the advent of high-speed digital computers and corresponding electronic and Chemicals (APD), Columbia Broadcasting System (CBS), Capital Cities 
trading mechanisms. It is unclear whether or not an eighth of a dollar is the optimal degree 
of discreteness today. Indeed, recent discussions between the NYSE and the US Securities 
and Exchange Commission seem to indicate a move towards decimalization under which prices 90 

See, for example, Ball, Torous, and Tschoegl(1985); Goodhart and Curcio (1990); Harris 
and quotes are denominated in cents. See Ball, Torous, and Tschoegl (1985); Brennan and (l991);N iederhoffer (1965,1966);  Niederhoffer and Osborne (1966);  and Osborne (1962). 
Copeland (1988); Harris (1991);a nd the SEC's (1994) Market 2000study for further discussion. 



110 3. Market Microstructure 

ABC (CCB), and Kaneb Services (KAB).  The histogram for CBS is a partic- 
ularly good illustration of the classic price-clustering pattern: Prices tend 
to fall more frequently on wholedollar multiples than on half-dollar mul- 
tiples, more frequently on half-dollars than on quarter-dollars, and more 
frequently on even eighths than on odd eighths. Price-clustering is even 
more pronounced for transactions data. 

The importance of these patterns of discreteness has been highlighted 
by the recent controversy, and litigation surrounding the publication of Price Fraction for AAC Price Change (ti&) fr,rA&6 

two empirical studies by Christie and Schultz (1994) and Christie, Harris, 
and Schultz (1994). They argue that the tendency for bid-ask quotes on 
NASDAQ stocks to cluster more frequently on even eighths than on odd 
eighths is an indication of tacit collusion among NASDAQ dealers to main- 
tain wider spreads. Of course, there are important differences between the 
NASDAQ's market structure and those of other organized exchanges, and 
more detailed analysis is required to determine if such differences can ex- - ti . i i5  ,240 375 .5b0 .6h5 .7k0 ~ $ 5  

plain the empirical regularities documented by Christie and Schultz (1994) Price Fraction for APD Price Change (ticks) ~ O I A P D  

and Christie, Harris, and Schultz (1994). Although the outcome of this 
controversy is yet to be decided, all parties concerned would agree that 
discreteness can have a tremendous impact on securities markets.31 

Discreteness and Returns 
The empirical relevance of discreteness for returns depends to a large extent 
on the holding period and the price level, for reasons that we shall discuss o .I 25 ,250 375 ,500 .6i5 ,740 .sj5 

below. For transactions data, discreteness is considerably more problematic Price Fraction for CBS Price Change (ticks) for CBS 

because the price change from one transaction to the next is typically only 
one or two ticks. For example, if the minimum price variation is an eighth of 
a dollar, a stock currently priced at $10 a share can never yield a transaction 
return between zero and f1 .25%. In fact, in this case, the transaction return 
must fall on a discrete "grid" of integer multiples of 1.25%. For higher- 
priced stocks, this grid is considerably finer. For example, the transactions 
return for a $50 stock will fall on a grid of integer multiples of 0.25%. - 8 - 6 - 4 - 2 0  2 4 6 8 

Price Fraction for CCB Price Change (ticks) for CCB 
Moreover, as the price level varies through time, the collection of transaction 120 
returns obtained may seem less discrete because the grid corresponding 100 
to the entire dataset will be the superposition of the grids at each price 2 80 
level. Therefore, if price levels are high and volatile, or if the timespan of U 

g f31 
the dataset is long (which implies higher price-variability under a random 2 40 

walk model for prices), the discreteness of transaction returns will be less 20 

apparent. 0 

Table 3.2 contains a concrete example of this intuition. It reports the Price Fraction for KAB Price Change (ticks) for KAB 
relative frequencies of transaction price changes for the five stocks in Fig- 

310therc ontributions to the NASDAQ controversy include Chan, Christie, and Schultz @3p.2. ~H istogram of Daily Price Fractions and Price Changes for Five NYSE Stocks from 
(1995), Furbush and Smith (1976), Godek (1996), Grossman, Miller, Fischel, Cone, and Ross January 2, 1990 to December 31, 1992 
(199.5), Huang and Stoll (1995b),K andel and Marx (1996), and Kleidon and Willig (1995). 



3. Market Microstructure 

Table 3.2. Relative frequencies of p-ice changesf m tick data ofJive stocks. 

Number -4 
Stock of Trades -3 -2 - 1 0 +1 +2 +3 2 +4 

AAC 18,056 0.02 0.03 0.17 12.44 74.34 12.58 0.18 0.04 0.18 
2-History of AAC Returns, P = $3.353 

APD 26,905 0.32 0.41 3.22 13.48 64.40 14.23 3.14 0.41 0.39 

CBS 21,315 2.24 6.64 7.35 7.26 52.42 7.93 7.42 6.31 2.43 

CCB 23,128 15.72 0.70 1.69 3.90 55.11 4.56 1.89 0.58 15.85 

KAB 21,008 0.01 0.00 0.16 11.77 75.79 12.04 0.15 0.00 0.07 

2-History of APD Returns, P = $55.878 
Relative frequency count, in percent, for all 1991 transaction price changes in ticks for five 
NYSE stocks: AAC=Anacomp;A PD=Air Products and Chemicals; CBS=Columbia Broadcasting 
System; CCB=Capital Cities ABC; KAB=Kaneb Services. 

ure 3.2 using all of the stocks' transactions during the 1991 calendar year. 2-History of CBS Returns, P = $173.924 
The lower-priced stocks-KAB and AAG-have very few transaction price 
changes beyond the -1 tick to +1 tick range; these three values account 
for 99.6% and 99.3% of all the trades for KAB and AAC, respectively. In 
contrast, for a higher-priced stock like CCB, with an average price of $468 
during 1991, the range from -1 tick to +1 tick accounts for 63.6% of its 
trades. While discreteness is relatively less pronounced for CCB, it is never- 
theless still present. Even when we turn to daily data, the histograms of daily 2-History of CCB Returns, P = $467.844 

price changes in Figure 3.2b show that discreteness can still be important, 
especially for lower-priced stocks such as KAB and AAC. 

Moreover, discreteness may be more evident in the conditional and joint 
distribution of high frequency returns, even if it is difficult to detect in the 
unconditional or marginal distributions. For example, consider the graphs 
in Figure 3.3a in which pairs of adjacent daily simple returns (R,, Rt+l)a re 2-History of KAB Returns, P = 54.665 
plotted for each of the five stocks in Table 3.1 over the three-year sample (a)  (b) (4 
period. These m-histories (here, m = 2) are often used to detect structure in 
nonlinear dynamical systems (see Chapter 12). The scales of the two axes F'gum 3.3. 2-Histories of Daily Stock Return for Five NYSE Stocks from January 2, 1991 
are identical for all five stocks to make cross-stock comparisons meaningful, to D e e 31 , 1992 
and range from -5% to 5% in Figure 3.3a, -1 0% to 10% in Figure 3.3b, 
and -20% to 20% in Figure 3 .3~ .  

Figure 3.3a shows that there is considerable structure in the returns of ture than the lower-priced stocks but more than the higher-priced stocks. 
the lower-priced stocks, KAB and AAC; this is a radially symmetric structure Figures 3.3b and 3 . 3 ~sh ow that changing the scale of the plots can often 
that is solely attributable to discreteness. In contrast, no structure is evident reduce and, in the case of APD, completely obscure the regularities associ- 
in the 2-histories of the higher-priced stocks, CBS and CCB. Since APD's ated with discreteness. For further discussion of these 2-histories, see Crack 
initial price is in between those of the other four stocks, it displays less s t r~c-  

A -..- 



114 3. Market Microstructure 3.3. Modeling Transactions Data 115 

These empirical observations have motivated several explicit models of 
price discreteness, and we shall discuss the strengths and weaknesses of each 
of these models in the following sections. 

3.3.2 Rounding and BummeMr odels 
where the first method rounds down, the second rounds up, and the third 

Several models of price discreteness begin with a "true" but unobserved rounds to the nearest multiple of d.  For simplicity, we shall consider only 
continuous-state price process P,, and obtain the observed price process Pe (3.3.3), although our analysis easily extends to the other two methods. 
by discretizing P, in some fashion (see, for example, Ball [1988], Cho and At the heart of the discreteness issue is the difference between the return 
Frees [1988], and Gottlieb and Kalay [I9851)  . Although this may be a conve- Xt based on continuous-state prices and the return X," based on discretized 
nient starting point, the use of the term "true" price for the continuous-state prices. To develop a sense of just how different these two returns can be, 
price process in this literature is an unfortunate choice of terminology-it we shall construct an upper bound for the quantity IX," - X,I = Iq - & I ,  
implies that the discrete observed price is an approximation to the true price where Rt and q denote the simple net return of the continuous-state and 
when, in fact, the reverse is true: continuous-state models are approxima- discretized price processes, respectively. Let x and y be any two arbitrary 
tions to actual market prices which are discrete. When the approximation nonnegative real numbers such that y > 1, and observe that 
errors inherent in continuous-state models are neglected, this can yield mis- 
leading inferences, especially for transactions data3* 

Rounding Errors Subtracting x/y  from (3.3.6) then yields 
To formalize this notion of approximation error, denote by X, the gross 
return of the continuous-state process Pt between t-1 and t,  i.e., Xt = 
Pt /P t - l .  We shall measure the impact of discreteness by comparing X, to 
the gross returns process X," - P,"/PP_Ic orresponding to a discretized price 
process P,". which implies the inequality 

The most common method of discretizing P, is to round it to a multiple 
of d ,  the minimum price variation increment. To formalize this, we shall 
require the jlow and ceiling functions 

Assuming that Pt > d for all t ,  we may set x = P, /d ,  y -- P,-1 / d  and substitute 
LxJ = greatest integer 5 x (floor function) (3.3.1) these expressions into (3.3.8) to obtain the following upper bound: 
[xl = least integer > x (ceiling function), (3.3.2) 

IRp 6,-1 
- RtI < Max [ Xt , 1 - 6,-i 1 = L(X,, (3.3.9) 

for any real number x.33 Using (3.3.1) and (3.3.2), we can express the three 1 - 
most common methods of discretizing Pt compactly as where = d/PtP1is  defined to be the grid size at time t- 1 . 

Although the upper bound (3.3.9) is a strict inequality, it is in fact the 
k t up per bound, i.e., for any fixed d and any E > 0, there always exists some 
combination of PtV1a nd Xt for which - &I exceeds L(6, Xt, Pt-1) - c. 
Therefore, (3.3.9) measures the worst-case deviation of Re from R,, and it 

3 2 ~ hqeu estion ofwhich price is the "true" price may not be crucial for the statistical aspects is the tightest of all such measures. 
of models of discreteness-after all, whether one is an approximation to the other or vice-versa Note that (3.3.9) does not yield a uniform upper bound in rt, since L 
affects only the sign of the approximation error, not its absolute magnitude-but it is central 
to the motivation and interpretation of the results (see the discussion at the end of Section depends on r,: 
3.3.2 for examples). Therefore, although we shall adopt the terminology of this literature for 
the moment, the reader is asked to keep this ambiguity in mind while reading this section. 

3 % ~fru rther properties and applications of these integer functions, see Graham, Knuth, 
and Patashnik (1989, Chapter 3).  



116 3. Market Microstructure 3.3. Modeling Transactions Data 117 

Nevertheless, it still provides a useful guideline for the impact of discreteness Tables 3.3a-c report numerical values of (3.3.9) for price levels ptP1=  
on returns as prices and returns vary. For example, (3.3.9) formalizes the $1, $5, $10, $50, $100, and $200, and for values of /.L and a corresponding 
intuition that discreteness is less problematic for higher-priced stocks, since to annual means and standard deviations for simple returns ranging from 
L is an increasing function of and, therefore, a decreasing function 10% to 50% each, respectively, and then rescaled to represent daily returns 
of Pt-*. in Table 3.3a, monthly returns in Table 3.3b, and annual returns in Table 

It is important to keep in mind that (3.3.9) is only an upper bound, 3 .3~ .  
and while it does provide a measure of the worst-case discrepancy between Table 3.3a shows that for stocks priced at $1, the expected upper bound 
& and q,i t is not a measure of the discrepancy itself.34 This distinction is for the discreteness bias is approximately 14 percentage points, a substantial 
best understood by grappling with the fact that the expected upper bound bias indeed. However, this expected upper bound declines to approximately 
E[L(Xt, St-l)16t-l] is an increasing function of the mean and variance of Xt- 0.25 percentage points for a $50 stock and is a negligible 0.06 percentage 
the larger the expected return and volatility, the larger is the average value of points for a $200 stock. These upper bounds provide the rationale for 
the upper bound. This seems paradoxical because it is generally presumed the empirical examples of Figures 3.3a-c and the common intuition that 
that discreteness is less problematic for longer-horizon returns, but these discreteness has less of an impact on higher-priced stocks. Table 3.3a also 
have higher means and variances by construction. The paradox is readily shows that for daily returns, changes in the mean and standard deviation of 
resolved by observing that although the expected upper bound increases returns have relatively little impact on the magnitudes of the upper bounds. 
as the mean and variance increase, the probability mass of IR," - & I  near Tables 3.3b and 3 . 3 i~nd icate that the potential magnitudes of discrete- 
the upper bound may actually decline. Therefore, although the expected ness bias are relatively stable, increasing only slightly as the return-horizon 
worst-case discrepancy increases with the mean andvariance, the probability increases. Whereas the expected upper bound is about 2.5 percentage 
that such discrepancies are realized is smaller. Also, as we shall see below, points for daily returns when Pt-l = $5, it ranges from 2.8% to 3.9% for 
the expected upper bound seems to be relatively insensitive to changes in annual returns. This implies that as a fraction of the typical holding period 
the mean and variance of Xt, so that when measured as a percentage of the return, discreteness bias is much less-important as the return horizon in- 
expected return E[Xt], the expected upper bound does decline for longer- creases. Not surprisingly, changes in the mean and standard deviation of 
horizon returns. returns have more impact with an annual return-horizon. 

By specifymg a particular process for P,, we can evaluate the expectation 
of L(.) to develop some sense for the magnitudes of expected discreteness Rounding Models 
bias E[IR,"-  &I] that are possible. For example, let Pt follow a geometric Even if E[IR,"-  Rtl] is small, the statistical properties of PO,  can still differ in 
random walk with drift p and diffusion coefficient a so that log Pt/Pt-l are subtle but important ways from those of P,. If discreteness is an unavoidable 
IID normal random variables with mean p and variance a'. In this case, we aspect of the data at hand, it may be necessary to consider a more explicit 
have statistical model of the discrete price process. As we suggested above, a 

rounding model can allow us to infer the parameters of the continuous-state 
process from observations of the rounded process. In particular, in much of 
the rounding literature it is assumed that P, follows a geometric Brownian 
motion dP = pPdt + aPdW, and the goal is to estimate /.L and a from 
the observed price process P,". Clearly, the standard volatility estimator 3 
based on continuously compounded observed returns will be an inconsistent 
estimator of a , c onverging in probability to JE [(log PY,,-  log Pf)2] rather 
than to J E [ ( ~ O ~ P ,-+ l~og  Pt)*].  Moreover, it can be shown that 8 will be 

where @(.) is the normal C D F . ~ ~  an overestimate of a in the presence of price-discreteness (see Ball [1988, 
Table I] and Gottlieb and Kalay [1985, Table I] for approximate magnitudes 

34~deallyw, e would like to characterize [Re-  &I directly, but it is surprisingly difficult to do of this upward bias). Ball (1988),C ho and Frees (1988), Gottlieb and Kalay 
so with any degree of generality. However, see the discussion below regarding the roundingand 
barrier model-under specific parametric assumptions for X I ,m ore precise characterizations 
of the discreteness bias are available. This is no accident, since M a x [ X I ,1 -81 may be rewritten as M a x [ X t  - (1-6). 01 + 1-6; hence 

'"ate the upper bound may be recast as the payoff of a call option on Xt with strike price 1 .  
the similarity between (3.3.1 1 )  and the Black-Scholes callaption pricing formula. 



Table ?.?a. Expected upper bounds fm discreteness bias: daily return. Table ?.?b. Expected upper boundsfor discreteness bias: monthly returns. 

Pt-1 = $100 
10% 0.1270 0.1283 0.1296 0.1308 0.1319 
20% 0.1276 0.1286 0.1298 0.1309 0.1320 
30% 0.1282 0.1290 0.1300 0.1311 0.1321 
40% 0.1288 0.1 295 0.1304 0.1313 0.1322 
50% 0.1295 0.1300 0.1307 0.1315 0.1324 

Pf-1 = $200 
10% 0.0635 0.0641 0.0647 0.0653 0.0659 
20% 0.0637 0.0643 0.0648 0.0654 0.0659 
30% 0.0640 0.0645 0.0650 0.0655 0.0660 
40% 0.0644 0.0647 0.0651 0.0656 0.0661 
50% 0.0647 0.0649 0.0653 0.0657 0.0661 

Expected upper bounds for discreteness bias in simple returns 1%- &I x 100 under a geometric 
Expected upper bounds for discreteness bias in simple returns IT- & I x 100 under a geometric random walk for prices Pf with driftand diffusion parameters g and a calibrated to annual mean 
random walk for prices Pt with drift and diffusion parameters g and a calibrated to annual mean and standard deviation of simple returns m and s, respectively, each ranging from 10% to 50%, 
and standard deviation of simple returns m and s, respectively, each ranging from 10% to 50%, and then rescaled to match monthly data, i.e., ~ 1 1 2a,  / n . D iscretized prices P," = LPtIdJd, 
and then rescaled to match daily data, i.e., g / 3 6 0 ,  a/&%. Discretized prices P," = LPtIdJ d ,  d = 0.125, are used to calculate returns f$' = ( P ; / P L I )-  1 .  
d = 0.125, are used to calculate returns R; = (P,"/Pf-I-)   1 .  



3.3. Modeling Transactions Data 121 

(1985),a nd Harris (1990) all provide methods for estimating a consistently 
from the observed price process ~ e . ~ ~  

Table 3.3~. Expected upper boundsfor discreteness bias: annual returns. Barrier Mo&ls 
A slightly different but closely related set models of price discreteness has 
been proposed by Cho and Frees (1988) and Marsh and Rosenfeld (1986) 
which we shall call bummerm odels. In these models, the continuous-state 
"true" price process Pt is also a continuous-time process, and trades are 
observed whenever P, reaches certain levels or barriers. 

Marsh and Rosenfeld (1986) place these barriers at multiples of an 
eighth, so that conditional on the most recent trade at, say 40:, the waiting 
time until the next trade is the first-passage time of P, to two barriers, one 
at 40; and the other at 40; (assuming that Pt has positive drift). 

Cho and Frees (1988) focus on gross returns instead of prices and define 
stopping times t, as 

Therefore, according to their model a stock which has just traded at time 
t,-1 at $10.000 a share will trade next at time t, when the unobserved 
continuous-state gross returns process Pt/$lO.OOO reaches either 1.125 or 
111.125, or when Pt reaches either $10.125 or $8.888. If Pt reaches $8.888, 
the stock will trade next when Pt reaches either $10.000 or $7.901, and so 
on. 

This process captures pricediscreteness of a very different nature since 
the price increments defined by the stopping times are not integer multi- 
ples of any fixed quantity (for example, the lower' barrier 111.125 does not 
correspond to a oneeighth price decline). However, such an unnatural def- 
inition of discreteness does greatly simplify the characterization of stopping 
times and the estimation of the parameters of Pt, since the firstdifference 
oft ,  is IID. 

Under the more natural specification of price discreteness, not consid-. 
ered by Cho and Frees (1988), the stopping time becomes 

Expected upper bounds for discreteness bias in simple returns IRY - R11 x 100 under a geometric 
random walk for prices Pt with drift and diffusion parameters and a calibrated to annual mean which reduces to the Marsh and Rosenfeld (1986) model in which the in- 
and standard deviation of simple returns rn and s, respectively, each ranging from 10% to 50%. crements of stopping times are not IID. 
Discretized prices PP = LPt/d]d ,  d  = 0.125, are used to calculate returns Rf (P: /PE1)-  1. 

96~oweverse, e the discussion at the end of Section 3.3.2 for some caveats about the moti- 
m i o n  for these models. 



122 3. Market Microstmcture 3.3. Modeling Transactions Data 123 

Limitations ordered probit analysis is a generalization of the linear regression model 
to cases where the dependent variable is discrete. As such, among the 

Although all of the previous rounding and barrier models do capture price 
existing models of stock price discreteness--e.g., Ball (1988) ,C ho and Frees 

discreteness and admit consistent estimators of the parameters of the unob- 
(1988) ,G ottlieb and Kalay (1985) ,H arris (1990) ,a nd Marsh and Rosenfeld 

served continuous-state price process, they suffer from at least three impor- 
tant limitations. (1986)--ordered probit is the only specification that can easily capture the 

First, for unobserved price processes other than geometric Brownian impact of "explanatory" variables on price changes while also accounting 

motion, these models and their corresponding parameter estimators be- for price discreteness and irregular transaction intervals. 

come intractable. 
Second, the rounding and barrier models focus exclusively on prices The Basic Speczjication 

and allow no role for other economic variables that might influence price Specifically, consider a sequence of transaction prices P ( h ) ,P ( t l ) ,. . . , P(tn)  

behavior, e.g., bid-ask spreads, volatility, trading volume, etc. sampled at times h,  t l ,  . . . , tn,a nd denote by 6,Y2 ,.  . . , Yn the correspond- 

Third, and most importantly, the distinction between the "true" and ing price changes, where Yk - P(tk) - P(tk-1) is assumed to be an integer 

observed price is artificial at best, and the economic interpretation of the multiple of some divisor, e.g., a tick. Let Y: denote an unobservable con- 

two quantities is unclear. For example, Ball (1988) ,C ho and Frees (1988) ,  tinuous random variable such that 

Gottlieb and Kalay (1985) ,a nd Harris (1990) all provide methods for es- 
timating the volatility of a continuous-time price process from discrete ob- 
served prices, never questioning the motivation of this arduous task. If the where the ( 9 x 1 )  vector Xk = [ Xlk . .. Xgk 1' is a vector of explani atory 
continuous-time price process is an approximation to actual market prices, variables that determines the conditional mean of Yk+ and "INID" indicates 
why is the volatility of the approximating process of interest? One might that the ck's are independently but not identically distributed, an impor- 
argue that derivative pricing models such as the Black-Scholes/Merton for- tant difference from standard econometric models which we shall return to 
mulas depend on the parameters of such continuous-time processes, but shortly. Note that subscripts are used to denote transaction time, whereas 
those models are also approximations to market prices, prices which ex- time arguments tk denote calendar or clock time, a convention we shall follow 
hibit discreteness as well. Therefore, a case must be made for the economic throughout Section 3.3.3. 
relevance of the parameters of continuous-state price processes to properly The heart of the ordered probit model is the assumption that observed 
motivate the statistical models of discreteness in Section 3.3.2. price changes Yka re related to the continuous variables Y l  in the following 

In the absence of a well-articulated model of "true" price, it seems un- manner: 
natural to argue that the "true" price is continuous, implying that observed sl if Y,* E Al 
discrete market prices are somehow less genuine. After all, the economic 
definition of price is that quantity of numeraire at which two mutually con- s:, if Y: E A2 
senting economic agents are willing to consummate a trade. Despite the fact Yk = . . (3.3.15) 

..  ..  that institutional restrictions may require prices to fall on discrete values, 
as long as both buyers and sellers are aware of this discreteness in advance s, if Y: E A,, 
and are still willing to engage in trade, then discrete prices corresponding 
to market trades are "true" prices in every sense. where the sets A, form apartition of the state space S* of Y f ,i .e., S* = Upl Aj 

and Ai n Aj = 0 for i # j, and the sj's are the discrete values that comprise 
the state space S of Yk. 

3.3.3 The Ordered Probit Model The motivation for the ordered probit specification is to uncover the 
TO address the limitations of the rounding and barrier models, Hausman, mapping between S* and S and relate it to a set of economic variables. In 
Lo, and MacKinlay (1992) propose an alternative in which price changes are Hausman, Lo, and MacKinlay (1992) ,t he 3's are defined as: 0 ,  -i,+ $, 
modeled directly using a statistical model known as orderedprobit,a  technique 
used most frequently in empirical studies of dependent variables that take on variable is discrete and is naturally ordered since college education always follows high school 
only a finite number of values possessing a natural ordering" Heuristically, (see Maddala [ 19831 for further details). The ordered probit model was developed by Aitchison 

and Silvey (1957) and Ashford (1959),a nd generalized to nonnormal disturbances by Gurland, 
Lee, and Dahm (1960). For more recent extensions, see Maddala (1983), McCullagh (1980), 

 or example, the dependent variable might be the level of education, as measured by and Thisted (1991 \ .  
three categories: less than high school, high school, and college education. The dependent 



124 3. Market Mimstructure 3.3. Modeling Transactions Data 

-%,+ ia,n d so on. For simplicity, the statespace partition of S* is usually 
defined to be intervals: 

Although the observed price change can be any number of ticks, posi- 
tive or negative, we assume that m in (3.3.15) is finite to keep the number of 
unknown parameters finite. This poses no difficulties since we may always 
let some states in S represent a multiple (and possibly countably infinite) 
number of values for the observed price change. For example, in the empir- Figure 3.4. The Ordered Probit Model 
ical application of Hausman, Lo, and MacKinlay (1992), sl is defined to be 
a price change of -4 ticks or less, % to be a price change of +4 ticks or more, 
and s2 to s~ to be price changes of -3 ticks to +3 ticks, respectively. This Brownian motion with variance proportional to Atk - tk - tk-1, CT; must be 
parsimony is obtained at the cost of losing price resolution. That is, under a linear function of Atk which varies from one transaction to the next. 
this specification the ordered probit model does not distinguish between More generally, to allow for more general forms of conditional het- 
price changes of +4 and price changes greater than +4, since the ++tick eroskedasticity, let us assume that a; is a linear function of a vector of pre- 
outcome and the greater than ++tick outcome have been grouped together determined variables Wk = [ Wlk . . . WLk I'  SO that 
into a common event. The same is true for price changes of -4 ticks and 
price changes less than -4. This partitioning is illustrated in Figure 3.4 
which superimposes the partition boundaries {ail on the density function 
of Yk+,a nd the sizes of the regions enclosed by the partitions determine the 
probabilities xi of the discrete events. 

Moreover, in principle the resolution may be made arbitrarily finer by where (3.3.20) replaces the corresponding hypothesis in (3.3.14) and the 
simply introducing more states, i.e., by increasing m. As long as (3.3.14) is conditional volatility coefficients (q]ar e squared in (3.3.21) to ensure that 
correctly specified, increasing price resolution will not affect the estimated the conditional volatility is nonnegative. In this more general framework, 
p asymptotically (although finite-sample properties may differ). However, the arithmetic Brownian motion model of Marsh and Rosenfeld (1986) can 
in practice the data will impose a limit on the fineness of price resolution be easily accommodated by setting 
simply because there will be no observations in the extreme states when m 
is too large, in which case a subset of the parameters is not identified and 
cannot be estimated. 

The Conditional Distribution of Price Changes In this case, Wk contains only one variable, Atk (which is also the only 
Observe that the E ~ ' Si n (3.3.14) are assumed to be nonidentically dis- variable contained in Xk). The fact that the same variable is included in 
tributed, conditioned on the Xk's The need for this somewhat nonstandard both Xk and Wk does not create perfect multicollinearity since one vector 
assumption comes from the irregular and random spacing of transactions affects the conditional mean of Y,* while the other affects the conditional 
data. If, for example, transaction prices were determined by the model in variance. 
Marsh and Rosenfeld (1986) where the Yk*'sa re increments of arithmetic 



126 3. Market Microstructure 3.3. Modeling Transactions Data 127 

The dependence structure of the observed process Yk is clearly induced conditional heteroskedasticity in the ordered logit model, we have chosen 
by that of Yk* and the definitions of the Ajls, since the normal distribution. 

Given the partition boundaries, a higher conditional mean XiP implies 
a higher probability of observing a more extreme positive state. Of course, 
the labeling of states is arbitrary, but the ordered probit model makes use 

As a consequence, if Xk and Wk are temporally independent, the observed of the natural ordering of the states. The regressors allow us to separate 
process Yk is also temporally independent. Of course, these are fairly re- the effects of various economic factors that influence the likelihood of one 
strictive assumptions and are certainly not necessary for any of the statistical state versus another. For example, suppose that a large positive value of 
inferences that follow. We require only that the ek's be conditionally inde- X1 usually implies a large negative observed price change and vice versa. 
pendent, so that all serial dependence is captured by the Xk's and the Wk's. Then the ordered probit coefficient B1 will be negative in sign and large in 
Consequently, the independence of the ek'sd oes not imply that the Yt's are magnitude (relative to ak, of course). 
independently distributed because no restrictions have been placed on the By allowing the data to determine the partition boundaries a, the co- 
temporal dependence of the Xk9so r Wk's. efficients p of the conditional mean, and the conditional variance a:, the 

The conditional distribution of observed price changes Yk, conditioned ordered probit model captures the empirical relation between the unob- 
on Xk and Wk,i s determined by the partition boundaries and the particular servable continuous state space S* and the observed discrete state space S 
distribution of rk. For normal rk's, the conditional distribution is as a function of the economic variables Xk and Wk. 

Maximum Likelihood Estimation 
Let Ik(i) be an indicator variable which takes on the value one if the re- 
alization of the kth observation Yk is the ith state si, and zero otherwise. 
Then the log-likelihood function L for the vector of price changes Y = 
[ Yl Y2 . . .  Y, I f ,  conditional on the explanatory variables X = 
[X1X2 . . .  X n I 1 a n d W =  [W1W 2 . . .  W,I1,isgivenby 

where ffk(Wki)s written as an argument of Wk to show how the conditioning + zk(m)l og [ 1 - 8 ( - x p ) ].  (3.3.28) 
variables enter theconditional distribution, and 8 (.) is the standard normal ~kcwk) 

cumulative distribution function. 
To develop some intuition for the ordered probit model, observe that Although C Ti~s a llowed to vary linearly with Wk, there are some constraints 

the probability of any particular observed price change is determined by that must be placed on the parameters to achieve identification since, for 
example, doubling the a's, the P's, and a k l eaves the likelihood unchanged. 

where the conditional mean lies relative to the partition boundaries. There- 
A typical identification assumption is to set yo = 1. We are then left with 

fore, for a given conditional mean XLP, shifting the boundaries will alter 
three issues that must be resolved before estimation is possible: (i) the 

the probabilities of observing each state (see Figure 3.4). 
number of states m; (ii) the specification of the regressors Xk;a nd (iii) the 

In fact, by shifting the boundaries appropriately, ordered probit can fit specification of the conditional variance a;. 
any arbitrary multinomial distribution. This implies that the assumption of In choosing m, we must balance price resolution against the practical 
normality underlying ordered probit plays no special role in determining the constraint that too large an m will yield no observations in the extreme states 
probabilities of states; a logistic distribution, for example, could have served ~1 and s,. For example, if we set m to 101 and define the states sl and slol 
equally well. However, since it is considerably more difficult to capture 



128 3. Market Microstructure 3.4. Recent Empirical Findings 129 

symmetrically to be price changes of -50 ticks and +50 ticks, respectively, tional simplicity are unique to this framework. Under the assumptions of 

we would find no Yk9sa mong typical NYSE stock transactions falling into Section 3.1.1, the presence of nonsynchronous trading 
either of these states, and it would be impossible to estimate the parameters 1. does not affect the mean of either observed individual or portfolio re- 
associated with these two states. Perhaps the easiest method for determining 

turns. 
m is to use the empirical frequency distribution of the dataset as a guide, 2. increases the variance of observed individual security returns that have 
setting m as large as possible, but not so large that the extreme states have 

nonzero means. The smaller the mean, the smaller the increase in the 
no observations in them.38 

variance of observed returns. 
The remaining two issues must be resolved on a case-by-case basis since 3. decreases the variance of observed portfolio returns when portfolios 

the specification for the regressors and a: are dictated largely by the par- 
are welldiversified and consist of securities with common nontrading 

ticular application at hand. For forecasting purposes, lagged price changes 
probability. 

and market indexes may be appropriate regressors, but for estimating a 
4. induces geometrically declining negative serial correlation in observed 

structural model of marketmaker monopoly power, other variables might 
individual security returns that have nonzero means. The smaller the 

be more appropriate. 
absolute value of the mean, the closer is the autocorrelation to zero. 

5. induces geometrically declining positive serial correlation in observed 
portfolio returns when portfolios are welldiversified and consist of se- 

3.4 Recent Empirical Findings curities with a common nontrading probability, yielding an AR(1) for 
the observed returns process. 

The empirical market microstructure literature is an extensive one, strad- 6. induces geometricallyd eclining cross-autocorrelation between observed 
dling both academic and industry publications, and it is difficult if not im- returns of securities i and j which is of the same sign as /?,ST This 
possible to provide even a superficial review in a few pages. Instead, we cross-autocorrelation is generally asymmetric: The covariance of current 
shall present three specific market microstructure applications in this sec- observed returns to i with future bbserved returns to j is need not be 
tion, each in some depth, to give readers a more concrete illustration of the same as the covariance of current observed returns to j with future 
empirical research in this exciting and rapidly growing literature. Section observed returns to i. The asymmetry arises from the fact that different 
3.4.1 provides an empirical analysis of nonsynchronous trading in which securities may have different nontrading probabilities. 
the magnitude of the nontrading bias is measured using daily, weekly, and 7. induces geometrically declining positive cross-autocorrelation between 
monthly stock returns. Section 3.4.2 reviews the empirical analysis of ef- observed returns of portfolios A and B when portfolios are welldiver- 
fective bid-ask spreads based on the model in Roll (1984). And Section sified and consist of securities with common nontrading probabilities. 
3.4.3 presents an application of the ordered probit model to transactions This cross-autocorrelation is also asymmetric and arises from the fact 
data. that securities in different portfolios may have different nontrading 

probabilities. 
8. induces positive serial dependence in an equal-weighted index if the 

3.4.1 Nonsynchmnous Trading 
betas of the securities are generally of the same sign, and if individual 

Before considering the empirical evidence for nontrading effects we summa- returns have small means. 
rize the qualitative implications of the nontrading model of Section 3.1.1. 9. and time aggregation increases the maximal nontrading-induced neg- 
Although many of these implications are consistent with other models of ative autocorrelation in observed individual security returns, but this 
nonsynchronous trading, the sharp comparative static results and exposi- maximal negative autocorrelation is attained at nontrading probabili- 

ties increasingly closer to unity as the degree of aggregation increases. 
10. and time aggregation decreases the nontrading-induced autocorrela- 

3 8 ~ oerx ample, Hausman, Lo, and MacKinlay (1992) set m = 9 for the larger stocks, tion in observed portfolio returns for all nontrading probabilities. 
implying extreme states of -4 ticks or less and +4 ticks or more, and set m = 5 for the smaller 
stocks, implying extreme states of -2 ticks or less and +2 ticks or more. Note that although Since the effects of nonsynchronous trading are more apparent in se- 
the definition of states need not be symmetric (state 51 can be -6 ticks or less, implying that curities grouped by nontrading-  -p robabilities than in individual stocks, our 
state SJ is +2 ticks or more), the symmetry of the histogram of price changes in their dataset empirical application uses the returns of ten size-sorted portfolios for daily, 
suggests a symmetric definition of the sl's. 



130 3. Market Microstructure 3.4. Recent Empirical Findings 131 

weekly, and monthly data from 1962 to 1994. We use market capitaliza- Table 3.4. Autocorrelation matrices for size-sorted pmtfolio returns. 
tion to group securities because the relative thinness of the market for 
any given stock is highly correlated with the stock's total market value; 
hence stocks with similar market values are likely to have similar nontrading 
probabilities.39 We choose to form ten portfolios to maximize the homo- 
geneity of nontrading probabilities within each portfolio while still main- Daily 
taining reasonable diversification so that the asymptotic approximation of 
(3.1.20) might still obtain.40 

Daily Nontrading Probabilities Implicit in Autocorrelations 
Table 3.4 reports first-order autocorrelation matrices for the vector of 
four of the ten size-sorted portfolio returns using daily, weekly, and monthly 

Weekly 
data taken from the Center for Research in Security Prices (CRSP) database. 
Portfolio 1 contains stocks with the smallest market values and portfolio 10 
contains those with the largest.41 From casual inspection it is apparent 
that these autocorrelation matrices are not symmetric. The second column 
of matrices is the autocorrelation matrices minus their transposes, and it 
is evident that elements below the diagonal dominate those above it. This 
confirms the lead-lag pattern reported in Lo and MacKinlay (1990~).  Monthly 

The fact that the returns of large stocks tend to lead those of smaller 
stocks does suggest that nonsynchronous trading may be a source of cor- 
relation. However, the magnitudes of the autocorrelations for weekly and Sample first-order autocorrelation matrix f 1 for the (4x 1) subvector [ r: r4' r7 ' rlooI '  of observed 
monthly returns imply an implausible level of nontrading. This is most evi- returns to ten equal-weighted sizesorted portfolios using daily, weekly, and monthly NYSE- 

AMEX common stock returns data from the CRSP files for the time period July 3, 1962 to 
dent in Table 3.5, which reports estimates of daily nontrading probabilities December 30, 1994. Stocks are assigned to portfolios annually using the market value at the 
implicit in the weekly and monthly own-autocorrelations of Table 3.4. end of the prior year. If this market value is missing the end of year marketvalue is used. If both 

For example, using (3.1.40) the daily nontrading probability implied by market values are missing the stock is not included. Only securities with complete daily return 

an estimated weekly autocorrelation of 37% for portfolio 1 is estimated to histories within a given month are included in the daily returns calculations. r: is the return to 
the portfolio containing securities with the smallest market values and rh is the return to the 

be 71.796.~U~s ing (3.1.8) we estimate the average time between trades to portfolio of securities with the largest. There are approximately equal numbers of securities in 
each portfolio. The entry in the ith row and jth column is the correlation between r$ and 

TO gauge the degree of asymmetry in these autocorrelation matrices, the difference rA ~-  r^ Il  
J g ~ n loyr dinary common shares are included in this analysis. Excluded are American is also reported. 

Depository Receipts (ADRs) and other specialized securities where using market value to char- 
acterize nontrading is less meaningful. 

4 0 ~ hret urns to these portfolios are continuously compounded returns of individual simple 
returns arithmetically averaged. We have repeated the correlation analysis for continuously be 2.5 days! The corresponding daily nontrading probability is 86.6% using 
compounded returns of portfolios whose values are calculated as  unweighted geometric av- monthly returns, implying an average nontrading duration of 6.5 days. 
erages of included securities' prices. The results for these portfolio returns are practically 
identical to those for the continuously compounded returns of equal-weighted portfolios. For comparison Table 3.5 also reports estimates of the nontrading prob 

41w e  report only a subset of four portfolios for the sake of brevity. abilities using daily data and using trade information from the CRSP files. In 
42~tandaredr rors for autocorrelation-based probability and nontrading duration estimates the absence of time aggregation own-autocorrelations of portfolio returns 

are obtained by applying a first-order Taylor expansion (see Section A.4 of the Appendix) to are consistent estimators of nontrading probabilities; thus the entries in the 
(3.1.8) and (3.1.40) using heteroskedasticity- and autocorrelationconsistent standard errors column of Table 3.5 labelled "+, (q = 1)" are simply taken from the diagonal 
for daily, weekly, and monthly first-order autocorrelation coefficients. These latter standard 
errors are computed by regressing returns on a constant and lagged returns, and using Newey of the autocovariance matrix in Table 3.4. 
and West's (1987) procedure to calculate heteroskedasticity- and autocorrelationconsistent For the smaller securities, the point estimates yield plausible nontrading 
standard errors for the slope coefficient (which is simply the first-order autocorrelation coef- durations, but the estimated durations decline only marginally for larger- 
ficient of returns). 



3. Market Microstructure 3.4. Recent Empirical Findings 

Table 3.5. Estimattes of daily nontrading probabilities. Table 3.6. Nontrading-implied weekly indm autocorrelations. 

Estimator of n, Implied Index pl (%) Implied Index pl (%) 
(BI = 1,81o=1 ) (PI = 1.5, BIO=  0.5) 

Negative Share Price 1.4 1.8 

Daily Autocorrelation 4.8 5.9 

Implied first-order autocorrelation PI  of weekly returns of an equal-weighted portfolio of ten 
size-sorted portfolios (which approximates an equal-weighted portfolio of all securities), using 
two different estimators of daily nontradingprobabilities for the portfolios: the average fraction 
of negative share prices reported by CRSP, and daily nontrading probabilities implied by first- 
order autocorrelations of daily returns. Since the index autocorrelation depends on the betas 
of the ten portfolios, it is computed for two sets of betas, one in which all betas are set to 1.0 
and another in which the betas decline linearly from PI = 1.5 to Plo = 0.5. The sample weekly 

Estimates of daily nontrading probabilities implicit in ten weekly and monthly sizesorted port- autocorrelation for an equal-weighted portfolio of the ten portfolios is 0.21. Results are based 
folio return autocorrelations. Entries in the column labelled "I?," are averages of the fraction of on data from July 3,1962 to December 30, 1994. 
securities in portfolio K that did not trade on each trading day, where the average is computed 
over all trading days from July 3,1962 to December 30,1994. Entries in the "I?, (q = 1)" column 
are the first-order autocorrelation coefficients of daily portfolio returns, which are consistent Nonynchronous Trading and Index Autocorrelation 
estimators of daily nontrading probabilities. Entries in the "I?,(q = 5)" and "?,(q = 22)" Denote by r i ,  the observed return in period t to an equal-weighted portfolio 
columns are estimates of daily nontrading probabilities obtained from first-order weekly and of all N securities. Its autocovariance and autocorrelation are readily shown 
monthly portfolio return autocorrelation coefficients, using the time aggregation relations of 
Section 3.2 (q  = 5 for weekly returns and q = 22 for monthly returns since there are 5 and 22 to be 
trading days in a week and a month, respectively). Entries in columns labelled "E[kl]" are esti- 
mates of the expected number of consecutive days without trading implied by the probability 
estimates in columns to the immediate left. Standard errors are reported in parentheses; all 
are heteroskedasticity- and autocorrelationconsistent. 

where I?, is the contemporaneous covariance matrix of r,O and i is an (N x 1) 
vector of ones. If the betas of the securities are generally of the same sign and 
if the mean return of each security is small, then r i ,  is likely to be positively 

size portfolios. A duration of nearly one fourth of a day is much too large autocorrelated. Alternatively, if the cross-autocovariances, are positive and 
for securities in the largest portfolio. More direct evidence is provided in dominate the negative own-autocovariances, the equal-weighted index will 
the column labelled 7i,, which reports the average fraction of securities in exhibit positive serial dependence. Can this explain Lo and MacKinlay's 
a given portfolio that do not trade during each trading day.4"his average (1988b) strong rejection of the random walk hypothesis for the CRSP weekly 
is computed over all trading days from July 3, 1962 to December 30, 1994 equal-weighted index, which exhibits a first-order autocorrelation over 20%? 
(8179 observations). Comparing the entries in this column with those in With little loss in generality we let N = 10 and consider the equal- 
the others shows the limitations of nontrading as an explanation for the weighted portfolio of the ten size-sorted portfolios, which is an approxi- 
autocorrelations in the data. Nontrading may be responsible for some of mately equal-weighted portfolio of all securities. Using (3.1.36) we may 
the time-series properties of stock returns but cannot be the only source of calculate the weekly autocorrelation of r i ,  induced by particular daily non- 
autocorrelation. trading probabilities n, and beta coefficients B,. To do this, we need to 

select empirically plausible values for IT, and #?,i,  = 1, 2, . . . , 10. This is 
done in Table 3.6 using two different methods of estimating the IT,'sa nd 

43Thisi nformation is provided in the CRSP daily files in which the closing price of a security two different assumptions for the B,'s. 
is reported to be the negative of the average of the bid and ask prices on days when that security The first row corresponds to weekly autocorrelations computed with 
did not trade. Standard errors for  roba ability estimates are based on the daily time series of the nontrading probabilities obtained from the fractions of negative share 
the fraction of no-trades. The standard errors are heteroskedasticity- and autocorrelation- prices reported by CRSP (see Table 3.5). The first entry, 1.4%,i s the first- 
consistent. 



134 3. Market Microstructure 3.4. Recent Empirical Findings 135 

order autocorrelation of the weekly equal-weighted index assuming that all This convention seems difficult to justify on economic grounds-negative 
twenty portfolio betas are 1. O, and the second entry, 1.8%,i s computed un- spreads are typically associated with marketmaking activity, i.e., the provi- 
der the alternative assumption that the betas decline linearly from = 1.5 sion of liquidity, yet this seems to have little connection with the presence of 

for the portfolio of smallest stocks to Blo = 0.5 for the portfolio of the positive serial correlation in returns. A more plausible alternative interpre- 

largest. The second row reports similar autocorrelations implied by non- tation of cases where (3.4.4) is complex-valued is that the Roll (1984) model 

trading probabilities estimated from daily autocorrelations using (3.1.41). is misspecified and that additional structure must be imposed to account 

The largest implied first-order autocorrelation for the weekly equal- for the positive serial correlation (see, for example, George et al. [1991], 

weighted returns index reported in Table 3.6 is only 5.9%. Using direct Glosten and Harris [19881, Huang and Stoll [1995al, and Stoll [1989]). 

estimates of nontrading via negative share prices yields an autocorrelation of Roll estimates the effective spreads of NYSE and AMEX stocks year by 

less than 2%. These magnitudes are still considerably smaller than the 21% year using daily returns data from 1963 to 1982, and finds the overall average 

sample autocorrelation of the equal-weighted index return. In summary, effective spread to be 0.298% for NYSE stocks and 1.74% for AMEX stocks 

the recent empirical evidence provides little support for nontrading as an (recall that AMEX stocks tend to be lower-priced; hence they ought to have 

important source of spurious correlation in the returns of common stock larger percentage spreads). However, these figures must be interpreted with 

over daily and longer frequencies.44 caution since 24,358 of the 47,414 estimated effective spreads were negative, 
suggesting the presence of substantial specification errors. Perhaps another 
symptom of these specification errors is the fact that estimates of the effective 

3.4.2 Estimating the Effective Bid-Ask Spread spread based on weekly data differ significantly from those based on daily 
In implementing the model of Section 3.2.1, Roll (1984) argues that the data. Nevertheless, the magnitudes of these effects are clearly important for 
percentage bid-ask spread s, may be more easily interpreted than the ab- empirical applications of transactions data. 
solute bid-ask spread s, and he shows that the first-order autocovariance of Glosten and Harris (1988) refine and estimate Glosten's (1987) decom- 
simple returns is related to s, in the following way: position of the bid-ask spread using transactions data for 250 NYSE stocks 

and conclude that the permanent adverse-selection component is indeed 
present in the data. Stoll (1989) develops a similar decomposition of the 
spread, and using transactions data for National Market System securities 
on the NASDAQ system from October to December of 1984, he concludes 
that 43% of the quoted spread is due to adverse selection, 10% is due to 
inventory-holding costs, and the remaining 47% is due to order-processing 

where s, is defined as a percentage of the geometric average of the average bid costs. George, Kaul, and Nimalendran (1991) allow the expected return of 
and ask prices Paa nd Pb. Using the approximation in (3.4.2),t he percentage the unobservable "true" price (P: in the notation of Section 3.2.1) to vary 
spread may be recovered as through time, and using daily and weekly data for NYSE and AMEX stocks 

from 1963 to 1985 and NASDAQstocks and from 1983 to 1987, they obtain a 
much smaller estimate for the portion of the spread attributable to adverse 
selection-8% to 13%-with the remainder due to order-processing costs, 

Note that (3.4.4) and (3.2.9) are only well-defined when the return auto- and no evidence of inventory costs. Huang and Stoll (1995a) propose a 
covariance is negative, since by construction the bid-ask bounce can only more general model that contains these other specifications as special cases 
induce negative first-order serial correlation. However, in practice, posi- and estimate the components of the spread to be 21% adverse-selection 
tive serial correlation in returns is not uncommon, and in these cases, Roll costs, 14% inventory-holding costs, and 65% order-processing costs using 
simply defines the spread to be (see footnotes a and b of his Table I): 1992 transactions data for 19 of the 20 stocks in the Major Market Index. 

The fact that these estimates vary so much across studies makes it dif- 
ficult to regard any single study as conclusive. The differences come from 
two sources: different specifications for the dynamics of the bid-ask spread, 

44~ortdoukhR,i chardson, and Whitelaw (199.5), Mech (1993) and Sias and Starks (1994) and the use of different datasets. There is clearly a need for a more detailed 
present additional empirical results on nontrading as a source of autocorrelation. While the and comprehensive analysis in which all of these specifications are applied 
papers do not agree on the level of autocorrelation induced by nontrading, all three papers 
conclude that nontrading cannot completely account for the observed autocorrelations. 



136 3. Market Microstructure 3.4. Recent Empirical Findings 137 

to a variety of datasets to gauge the explanatory power and stability of each To provide some intuition for this enormous dataset, we report a few 
model. summary statistics in Table 3.7. Our sample contains considerable price 

dispersion, with the low stock price rangingfrom $3.125 for NAV to $104.250 
for IBM, and the high ranging from $7.875 for NAV to $129.500 for IBM. 

3.4.3 Transactions Data At $219 million, HNH has the smallest market capitalization in our sample, 
and IBM has the largest with a market value of $69.8 billion. 

In Hausman, Lo, and MacKinlay (1992), three specific aspects of transac- The empirical analysis also requires some indicator of whether a trans- 
tions data are examined using the ordered probit model of Section 3.3.3: action was buyer-initiated or seller-initiated, otherwise the notion of price 
(1) Does the particular sequence of trades affect the conditional distribution impact is ill-defined-a 100,000-share block-purchase has quite a different 
of price changes, e.g., does the sequence of three price changes +1, -1, +1 price impact from a 100,000-share block-sale. Obviously, this is a difficult 
have the same effect on the conditional distribution of the next price change task because for every trade there is always a buyer and a seller. What we 
as the sequence -1, +1, +l? (2) Does trade size affect price changes, and hope to capture is which of the two parties is more anxious to consummate 
if so, what is the price impact per unit volume of trade from one transac- the trade and is therefore willing to pay for it by being closer to the bid price 
tion to the next? (3) Does price discreteness matter? In particular, can or the ask price. Perhaps the most obvious indicator is whether the trans- 
the conditional distribution of price changes be modeled as a simple linear action occurs at the ask price or at the bid price; if it is the former then the 
regression of price changes on explanatory variables without accounting for transaction is most likely a "buy" and if it is the latter then the transaction 
discreteness at all? is most likely a "sell." Unfortunately, a large number of transactions occur 

To address these three questions, Hausman, Lo, and MacKinlay (1992) at prices strictly within the bid-ask spread, so that this method for signing 
estimate the ordered probit model for 1988 transactions data of over a trades will leave the majority of trades indeterminate. 
hundred stocks. To conserve space, we focus only on their smaller and Hausman, Lo, and MacKinlay (1992) use the well-known algorithm of 
more detailed sample of six stocks-International Business Machines Cor- signing a transaction as a buy if the transaction price is higher than the mean 
poration (IBM),  Quantum Chemical Corporation (CUE), Foster Wheeler of the prevailing bid-ask quote (the most recent quote that is set at least five 
Corporation (FWC), Handy and Harman Company (HNH), Navistar In- seconds prior to the trade); they classify it as a sell if the price is lower. If 
ternational corporation (NAV), and American Telephone and Telegraph the price equals the mean of the prevailing bid-ask quote, they classify the 
Incorporated (T). For these six stocks, they focus only on intraday transac- trade as an in&terminate trade. This method yields far fewer indeterminate 
tion price changes since it has been welldocumented that overnight returns trades than classifying according to transactions at the bid or at the ask. 
differ substantially from intraday returns (see, for example, Amihud and Unfortunately, little is known about the relative merits of this method of 
Mendelson [1987], Stoll and Whaley [1990], and Wood, McInish, and Ord classification versus others such as the tick test (which classifies a transaction 
[1985]). They also impose several other filters to eliminate "problem" trans- as a buy, a sell, or indeterminate if its price is greater than, less than, or equal 
actions and quotes, which yielded sample sizes ranging from 3,174 trades to the previous transaction's price, respectively),s imply because it is virtually 
for HNH to 206,794 trades for IBM. impossible to obtain the data necessary to evaluate these alternatives. 

They also use bid and ask prices in their analysis, and since bid-ask quotes 
are reported only when they are revised, some effort is required to match The Empirical Specification 
quotes to transactions. A natural algorithm is to match each transaction 

TOe stimate the parameters of the ordered probit model via maximum likeli- 
price to the most recently reported quote prior to the transaction; however, hood, three specification decisions must be made: (i) the number of states 
Bronfman (1991) and Lee and Ready (1991) have shown that prices of m, (ii) the explanatory variables &, and (iii) the parametrization of the 
trades that precipitate quote revisions are sometimes reported with a lag, so 
that the order of quote revision and transaction price is reversed in official In choosing m, we must balance price resolution against the practical 
records such as the Consolidated Tape. To address this issue, Hausman, constraint that too large an mwill yield no observations in the extreme states 
Lo, and MacKinlay (1992) match transaction prices to quotes that are set 

SI and s,. For example, if we set m to 101 and define the states sl and slol 
at lcartfrve seconds F o r  to the transaction-the evidence in Lee and Ready sYmmetrically to be price changes of -50 ticks and +50 ticks, respectively, 
(1991) suggests that this will account for most of the missequencing. This we would find no Yk9sa mong our six stocks falling into these two states. 
is only one example of the kind of unique challenges that transactions data Using the empirical distribution of the data as a guide, Hausman, Lo, and 
pose. 



138 3. Market Miuostructure 3.4. Recent Empirical Findings 139 

nian motion model), the effects of bid-ask bounce (since many transactions 
Table 3.7. Summary statistics fm transactions data o f six stocks. are merely movements from the bid price to the ask price or vice versa), the 

size of the transaction (so price impact can be determined as a function of 
Variable IBM CUE FWC HNH NAV T the quantity traded), and the impact of "systematic" or marketwide move- 

- - 

Low Price ments on the conditional distribution of an individual stock's price changes. 
High Price These aspects call for the following explanatory variables: 
Market Value ($Billions) 

Atk: The time elapsed between transactions k-1 and k, in seconds. 
% Trades at Prices: ABk-l: The bid-ask spread prevailing at time tk-1, in ticks. 

z Midquote Yk-1: Three lags [I = 1, 2, 31 of the dependent variable Yk.R ecall that for 
= Midquote 

Midquote m = 9, price changes less than -4 ticks are set equal to -4 ticks (state 
sl), and price changes greater than +4 ticks are set equal to +4 ticks 

Price Change, Yk 
(state $), and similarly for m = 5. 

Mean: Vk-l: Three lags [I = 1, 2, 31 of the dollar volume of the (k- 1)th trans- 
Std. Dev.: 

action, defined as the price of the (k-1)th transaction (in dollars, not 
Time Between Trades, A t k  ticks) times the number of shares traded (denominated in hundreds of 

Mean: shares); hence dollar volume is denominated in hundreds of dollars. 
Std. Dev.: 

To reduce the influence of outliers, if the share volume of a trade ex- 
Bid-Ask Spread, ABk ceeds the 99.5 percentile of the empirical distribution of share volume 

Mean: for that stock, it is set equal to the 99.5 percentile. 
Std. Dev.: SP500k-l: Three lags [I = 1 ,  2, 31 of five-minute continuously com- 

S&P500 Futures Return pounded returns of the Standard and Poor's (S&P) 500 index futures 
Mean: price, for the contract maturing in the closest month beyond the month 
Std. Dev.: in which transaction k - 1 occurred, where the return is computed with 

Buy-Sell Indicator, IBSk the futures price recorded one minute before the nearest round minute 
Mean: +OtoT tk- [ and the price recorded five minutes before this. 
Std. Dev.: IBSk-[: Three lags [ l  = 1, 2, 31 of an indicator variable that takes the 

Signed Transformed Volume value +1 if the (k - 1)th transaction price is greater than the average of 
Mean: the quoted bid and ask prices at time tk-l, the value - 1 if the (k-1)th 
Std. Dev.: transaction price is less than the average of the bid and ask prices at 

Median Trading Volume ($) time tk-1, and zero otherwise, i.e., 

Summary statistics for transaction prices and corresponding ordered probit explanatory vari- 
ables of International Business Machines Corporation (IBM, 206,794 trades), Quantum Chem- 
ical Corporation (CUE, 26,927 trades), Foster Wheeler Corporation (FWC, 18,199 trades), 
Handy and Harman Company (HNH, 3,174 trades), Navistar International Corporation (NAV, 
96,127 trades), and American Telephone and Telegraph Company (T, 180,726 trades), for the 
period from January 4,1988 to December 30,1988. 

MacKinlay (1992) set m = 9 for the larger stocks, implying extreme states 
of -4 ticks or less and +4 ticks or more, and set m = 5 for the two smaller 
stocks, FWC and HNH, implying extreme states of -2 ticks or less and +2 
ticks or more. 

The explanatory variables & are selected to capture several aspects of The variable Atk is included in & to allow for clock-time effects on the 
transaction price changes: clock-time effects (such as the arithmetic Brow- 



140 3. Market Mimstructure 3.4. Recent Empirical Findings 141 

conditional mean of Y i .  If prices are stable in transaction time rather Table 3.8a. Estimates of ordered h b i t  partition boundaries. 
than clock time, this coefficient should be zero. Lagged price changes 
are included to account for serial dependencies, and lagged returns of the Parameter IBM CUE FWC HNH NAV T 
S&P500 index futures price are included to account for market-wide effects 
on price changes. 

To measure the price impact of a trade per unit volume, the term 
Tvwkpli)s  included, which is dollar volume transformed according to the 
Box and Cox (1964) transformation T,(.): 

where v E [0, 11 is also a parameter to be estimated. The Box-Cox trans- 
formation allows dollar volume to enter into the conditional mean nonlin- 
early, a particularly important innovation since common intuition suggests 
that price impact may exhibit economies of scale with respect to dollar vol- 
ume; i.e., although total price impact is likely to increase with volume, the 
marginal price impact probably does not. The Box-Cox transformation cap- 
tures the linear specification ( v  = 1) and concave specifications up to and 
including the logarithmic function (v = 0). The estimated curvature of 
this transformation will play an important role in the measurement of price 
impact. 

The transformed dollar volume variable is interacted with IBSkPI,a n Maximum likelihood estimates of the partition boundaries of the ordered probit model for 
transaction price changes of International Business Machines Corporation (IBM, 206,794 

indicator of whether the trade was buyer-initiated (IBSk=l),s eller-initiated trades), Quantum Chemical Corporation (CUE, 26,927 trades), Foster Wheeler Corporation 
[IBSk= - 11, or indeterminate (IBSk=O). A positive Pll would imply that (FWC, 18,199 trades), Handy and Harman Company (HNH, 3,174 trades), Navistar Interna- 
buyer-initiated trades tend to push prices up and seller-initiated trades tend tional Corporation (NAY 96,127 trades), and American Telephone and Telegraph Company 

to drive prices down. Such a relation is predicted by several information- (T, 180,726 trades), for the period from January 4, 1988 to December 30, 1988. 

based models of trading, e.g., Easley and O'Hara (1987). Moreover, the 
magnitude of Pll is the per-unit volume impact on the conditional mean 
of Yk*,w hich may be readily translated into the impact on the conditional the coefficients of the explanatory variables PI, . . . , Pl3, and the Box-Cox 
probabilities of observed price changes. The sign and magnitudes of P12 parameter v. The 5-state specification requires the estimation of only 20 
and Pl3 measure the persistence of price impact. parameters. 

Finally, to complete the specification the conditional variance  :a - 
y:+ C  :y wk must be parametrized. To allow for clock-time effects Atk is The Maximum Likelihood Estimates 
included, and since there is some evidence linking bid-ask spreads to the in- Tables 3.8a and 3.10b report the maximum likelihood estimates of the or- 
formation content and volatility of price changes (see, for example, Glosten dered probit model for the six stocks. Table 3.8a contains the estimates of the 
[1987], Hasbrouck [l988,1991a,b],a nd Petersen and Umlauf [1990])  , the boundary partitions a, and Table 3.8b contains the estimates of the "slope" 
lagged spread ABkpl is also included. And since the parameter vectors a, coefficients 0. Entries in each of the columns labeled with ticker symbols 
0, and -y are unidentified without additional restrictions, yo2 is set to one. are the parameter estimates for that stock; z-statistics, which are asyrnptot- 
This yields the specification ically standard normal under the null hypothesis that the corresponding 

coefficient is zero, are contained in parentheses below each estimate. 
Table 3.8a shows that the partition boundaries are estimated with high 

precision for all stocks and, as expected, the z-statistics are much larger for 
In summary, the 9-state specification requires the estimation of 24 parame- those stocks with many more observations. Note that the partition bound- 
ters: the partition boundaries a,,.  . . , as, the variance parameters yl and y2, 



3.4. Recent Empirical Findings 143 

aries are not evenly spaced, e.g., las-a41 = 1.765, whereas la4-a51=  2.670 
(it can be shown that these two values are statistically different). One im- 

Table 3.8b. Estimates of ordered pobit "slope" coefJicients. plication is that the eighths-barrier model of discrete prices, e.g., that of 
Marsh and Rosenfeld (1986), is not consistent with these transactions data. 

Parameter IBM CUE FWC HNH NAV T Another implication is that the estimated conditional probabilities of price 
changes need not look normal, but may (and do) display a clustering phe- 
nomenon similar to the clustering of the unconditional distribution of price 
changes on even eighths. 

Table 3.8b shows that the conditional means of the YL's for all six stocks 
are only marginally affected by A tk. Moreover, the z-statistics are minuscule, 
especially in light of the large sample sizes. However, A t  does enter into 
the 02 expression significantly-in fact, since all the parameters for ok2a re 
significant, homoskedasticity may be rejected-and hence clock-time is im- 
portant for the conditional variances, but not for the conditional means of 
Yl. Note that this does not necessarily imply the same for the conditional 
distribution of the Yk7sw, hich is nonlinearly related to the conditional distri- 
bution of the Yl's. For example, the conditional mean of the Yk7sm ay well 
depend on the conditional variance of the Yk+'s,S O that clock-time can still 
affect the conditional mean of observed price changes even though it does 
not affect the conditional mean of Yl. 

Order Flow, Discreteness, and Price Impact 
More striking is the significance and sign of the lagged price change coeffi- 
cients 8 2 ,  b3,a nd $4 ,  which are negative for all stocks, implying a tendency 
towards price reversals. For example, if the past three price changes were 
each one tick, the conditional mean of Yk+ changes by b 2 + b 3 + b 4 .  However, 
if the sequence of price changes was I/-1/ 1, then the effect on the condi- 
tional mean is b2-b3+b4a, quantity closer to zero for each of the security's 
parameter estimates. 

Note that these coefficients measure reversal tendencies beyond that 
induced by the presence of a constant bid-ask spread as in Roll (1984). 
The effect of bid-ask bounce on the conditional mean should be captured 
by the indicator variables IBSkPI ,I BSkP2,a nd IBSkPJ. In the absence of 
all other information (such as market movements or past price changes), 
these variables pick up any price effects that buys and sells might have on 
the conditional mean. As expected, the estimated coefficients are generally 
negative, indicating the presence of reversals due to movements from bid 

Maximum likelihood estimates of the "slope" coefficients of the ordered probit model for trans- to ask or ask to bid prices. Hausman, Lo, and MacKinlay (1992) compare 
action price changes of International Business Machines Corporation (IBM, 206,794 trades), 
Quantum Chemical Corporation (CUE, 26,927 trades), Foster Wheeler Corporation (FWC, their magnitudes formally and conclude that the conditional mean of price 
18,199 trades), Handy and Harman Company (HNH, 3,174 trades), Navistar International changes is path-dependent with respect to past price changes-the sequence of 
Corporation (NAV, 96,127 trades), and American Telephone and Telegraph Company (T, price changes or orderJlow matters. 
180,726 trades), for the period from January 4, 1988 to December 30, 1988. Using these parameter estimates, Hausman, Lo, and MacKinlay (1992) 

are also able to address the second two questions they put forward. Price 



144 3. Market Microstructure Problems 145 

impact-the effect of a trade on the market price-can be quantified with 3.2 Under the nontrading process defined by (3.1.2)-(3.1.3), and assum- 
relatively high precision, it does increase with trade size although not lin- ing that virtual returns have a linear one-factor structure (3.1. I ) ,s how how 
early, and it differs from stock to stock. The more liquid stocks such as IBM nontrading affects the estimated beta of a typical security. Recall that a secu- 
tend to have relatively flat price-impact functions, whereas less liquid stocks rity's beta is defined as the slope coefficient of a regression of the security's 
such as HNH are more sensitive to trade size (see, in particular, Hausman, returns on the return of the market portfolio. 
Lo, and MacKinlay [1992, Figure 41 ). 

Also, discreteness does matter, in the sense that the conditional distribu- 3.3 Suppose that the trading process ( S i t )  defined in (3.1.2) were not ID, 
tion of price changes implied by the ordered probit specification can capture but followed a two-state Markov chain instead, with transition probabilities 
certain nonlinearities-priceclustering on even eighths versus odd eighths, given by Si t  

for example-that other techniques such as ordinary least squares cannot. 0 1 
While it is still too early to say whether the ordered probit model will 

have broader applications in market microstructure studies, it is currently 
the only model that can capture discreteness, irregular trade intervals, and 
the effects of economic variables on transaction prices in a relatively parsi- 3.3.1 Derive the unconditional mean, variance, first-order autocovari- 
monious fashion. ance, and steady-state distribution of S i t  as functions of ni and ~ri. 

3.5 Conclusion 3.3.2 Calculate the mean, variance, and autocorrelation function of the 
observed returns process ri", under (3.5.1). How does serial correlation in 
sit affect the moments of observed returns? 

There are many outstanding economic and econometric issues that can now 
be resolved in the market microstructure literature thanks to the plethora of 3.3.3 Using daily returns for any individual security, estimate the param- 
newly available transactions databases. In this chapter we have touched on eters rri and 17; assuming that the virtual returns process is IID. Are the 
only three of the issues that are part of the burgeoning market microstruc- estimates empirically plausible? 
ture literature: nonsynchronous trading, the bid-ask spread, and modeling 
transactions data. However, the combination of transactions databases and 3.4 Extend the Roll (1984) model to allow for a serially correlated order- 
ever-increasing computing power is sure to create many new directions of type indicator variable. In particular, let It be a two-state Markov with -1 
research. For example, the measurement and control of trading costs has and 1 as the two states, and derive expressions for the moments of Apt in 
been of primary concern to large institutional investors, but there has been terms of s and the transition probabilities of It. How do these results differ 
relatively little academic research devoted to this important topic because from the IID case? How would you reinterpret Roll's (1984) findings in 
the necessary data were unavailable until recently. Similarly, measures of light of this more general model of bid-ask bounce? 
market transparency, liquidity, and competitiveness all figure prominently 3.5 How does price discreteness affect the sampling properties of the mean, 
in recent theoretical models of security prices, but it has been virtually im- standard deviation, and first-order autocorrelation estimators, if at all? Hint: 
possible to implement any of these theories until recently because of a lack Simulate continuous-state prices with various starting price levels, round 
of data. The experimental markets literature has also contributed many to the nearest eighth, calculate the statistics of interest, and tabulate the 
insights into market microstructure issues but its enormous potential is only relevant sampling distributions. 
beginning to be realized. Given the growing interest in market microstruc- 
ture by academics, investment professionals and, most recently, policymak- 3.6 The following questions refer to an extract of the NYSE's TAQDatabase 
ers involved in rewriting securities markets regulations, the next few years which consists of all transactions for IBM stock that occurred on January 4th 
are sure to be an extremely exciting and fertile period for this area. and 5th, 1988 (2,748 trades). 

3.6.1 Construct a histogram for IBM's stock price. Do you see any ev- 
idence of price clustering? Construct a histogram for IBM's stock price 
changes. Is there any pricechange clustering? Construct the following 

3.1 Derive the mean, variance, autocovariance, and autocorrelation func- two histograms and compare and contrast: the histogram of price changes 
tions (3.1.9)-(3.1.12)o f the observed returns process { r z }f or the nontrading conditional on prices falling on an even eighth, and the histogram of price 
model of Section 3.1. Hint: Use the representation (3.1.4). changes conditional on prices falling on an odd eighth. Using these his- 



146 3. Market Microstructure Problems 147 

tograms, comment on the importance or unimportance of discrete prices which he will switch completely into bonds until experiencing a sequence 
for statistical inference. of six consecutiue advances. Implement this rule for an initial investment 

of $100,000 with the transactions data, but do it two ways: (1) use the 
3.6.2 What is the average time between trades for IBM? Construct a average of the bid-ask spread for purchases or sales; (2) use the ask price 
95% confidence interval about this average. Using these quantities and for purchases and the bid price for sales. How much do you have left at 
the central limit theorem, what is the probability that IBM does not trade the end of two days of trading? You may assume a zero riskfree rate for 
in any given one-minute interval? Divide the trading day into one-minute this exercise. 
intervals, and estimate directly the unconditional and conditional probabil- 
ities of nontrading, where the conditional probabilities are conditioned 
on whether a trade occurred during the previous minute (hint: think 
about Markov chains). Is the nontrading process independent? 

3.6.3 Plot price and volume on the same graph, with time-of-day as the 
horizontal axis. Are there any discernible patterns? Propose and perform 
statistical tests of such patterns and other patterns that might not be visible 
to the naked eye but are motivated by economic considerations; e.g., block 
trades are followed by larger price changes than nonblock trades, e t ~ . ~ ~  

3.6.4 Devise and estimate a model that measures price impact, i.e., the 
actual cost of trading n shares of IBM. Feel free to use any statistical 
methods at your disposal-there is no single right answer (in particular, 
ordered probit is not necessarily the best way to do this). Think carefully 
about the underlying economic motivation for measuring price impact. 

3.7 The following questions refer to an extract of the NYSE's TAQDatabase 
which consists of bid-ask quote revisions and depths for IBM stock that were 
displayed during January 4th and 5th, 1988 (1,327 quote revisions). 

3.7.1 Construct a histogram for IBM's bid-ask spread. Can you conclude 
from this that the dynamics of the bid-ask spread are unimportant? Why 
or why not? You may wish to construct various conditional histograms to 
properly answer this question. 

3.7.2 Are there any discernible relations between revisions in the bid-ask 
quotes and transactions? That is, do revisions in bid-ask quotes "cause" 
trades to occur, or do trades motivate revisions in the quotes? Propose 
and estimate a model to answer this question. 

3.7.3 How are changes in the bid and ask prices related to volume, if at 
all? For example, do quote revisions cause trades to occur, or do trades 
motivate revisions in the quotes? Propose and estimate a model to answer 
this question. 

3.7.4 Consider an asset allocation rule in which an investor invests fully 
in stocks until experiencing a sequence of three consecutiue declines, after 

4"he NYSE defines a block trade as any trade consisting of 10,000 shares or more. 

A 



Event-Study Analysis 

ECONOMISTS ARE FREQUENTLY ASKED to measure the effect of an economic 
event on the value of a firm. On the surface this seems like a difficult 
task, but a measure can be constructed easily using financial market data 
in an event study. The usefulness of such a study comes from the fact 
that, given rationality in the marketplace, the effect of an event will be 
reflected immediately in asset prices. Thus the event's economic impact 
can be measured using asset prices observed over a relatively short time 
period. In contrast, direct measures may require many months or even 
years of observation. 

The general applicability of the event-study methodology has led to 
its wide use. In the academic accounting and finance field, event-study 
methodology has been applied to a variety of firmspecific and economy- 
wide events. Some examples include mergers and acquisitions, earnings an- 
nouncements, issues of new debt or equity, and announcements of macroe- 
conomic variables such as the trade deficit.' However, applications in other 
fields are also abundant. For example, event studies are used in the field of 
law and economics to measure the impact on the value of a firm of a change 
in the regulatory en~ironmenta,n~d  in legal-liability cases event studies are 
used to assess damages.3 In most applications, the focus is the effect of an 
event on the price of a particular class of securities of the firm, most often 
common equity. In this chapter the methodology will be discussed in terms 
of common stock applications. However, the methodology can be applied 
to debt securities with little modification. 

Event studies have a long history. Perhaps the first published study is 
Dolley (1933). Dolley examined the price effects of stock splits, studying 
nominal price changes at the time of the split. Using a sample of 95 splits 

'we will further discuss the first three examples later in the chapter. McQueen and Roley 
(1993) provide an illustration using macroeconomic news announcements. 

'see Schwert (1981). 
'See Mitchell and Netter (1994). 

149 



150 4. Event-Study Analysis 4.1.  Outline of an Euent Study 151 

from 1921 to 1931, he found that the price increased in 57 of the cases and as having seven steps: 
the price declined in only 26 instances. There was no effect in the other 12 1. Event definition. The initial task of conducting an event study is to de- 
cases. Over the decades from the early 1930s until the late 1960s the level fine the event of interest and identify the period over which the security 
of sophistication of event studies increased. Myers and Bakay (1948),B aker prices of the firms involved in this event will be examined-the event 
(1956, 1957, 1958), and Ashley (1962) are examples of studies during this window. For example, if one is looking at the information content of 
time period. The improvements include removing general stock market an earnings announcement with daily data, the event will be the earn- 
price movements and separating out confounding events. In the late 1960s ings announcement and the event window might be the one day of the 
seminal studies by Ball and Brown (1968) and Fama, Fisher, Jensen, and announcement. In practice, the event window is often expanded to 
Roll (1969) introduced the methodology that is essentially still in use today. two days, the day of the announcement and the day after the announce- 
Ball and Brown considered the information content of earnings, and Fama, ment. This is done to capture the price effects of announcements which 
Fisher,J ensen, and Roll studied the effects of stock splits after removing the occur after the stock market closes on the announcement day. The pe- 
effects of simultaneous dividend increases. riod prior to or after the event may also be of interest and included 

In the years since these pioneering studies, several modifications of the separately in the analysis. For example, in the earnings-announcement 
basic methodology have been suggested. These modifications handle com- case, the market may acquire information about the earnings prior to 
plications arising from violations of the statistical assumptions used in the the actual announcement and one can investigate this possibility by 
early work, and they can accommodate more specific hypotheses. Brown examining pre-event returns. 
and Warner (1980, 1985) are useful papers that discuss the practical im- 2. Selection criteria. After identifying the event of interest, it is necessary 
portance of many of these modifications. The 1980 paper considers imple- to determine the selection criteria for the inclusion of a given firm in 
mentation issues for data sampled at a monthly interval and the 1985 paper the study. The criteria may involve restrictions imposed by data avail- 
deals with issues for daily data. ability such as listing on the NYSE or AMEX or may involve restrictions 

This chapter explains the econometric methodology of event studies. such as membership in a specific industry. At this stage it is useful to 
Section 4.1 briefly outlines the procedure for conducting an event study. summarize some characteristics of the data sample (e.g., firm market 
Section 4.2 sets up an illustrative example of an event study. Central to capitalization, industry representation, distribution of events through 
any event study is the measurement of the abnormal return. Section 4.3 time) and note any potential biases which may have been introduced 
details the first step--measuring the normal performance-and Section 4.4 through the sample selection. 
follows with the necessary tools for calculating the abnormal return, mak- 3. Normal and abnomal returns. To appraise the event's impact we require 
ing statistical inferences about these returns, and aggregating over many a measure of the abnormal return. The abnormal return is the actual 
event observations. In Sections 4.3 and 4.4 the discussion maintains the ex post return of the security over the event window minus the normal 
null hypothesis that the event has no impact on the distribution of returns. return of the firm over the event window. The normal return is defined 
Section 4.5 discusses modifying the null hypothesis to focus only on the as the return that would be expected if the event did not take place. For 
mean of the return distribution. Section 4.6 analyzes of the power of an each firm i and event date t we have 
event study. Section 4.7 presents a nonparametric approach to event stud- 
ies which eliminates the need for parametric structure. In some cases theory 
provides hypotheses concerning the relation between the magnitude of the 
event abnormal return and firm characteristics. In Section 4.8 we consider where €2, &, and E(&) are the abnormal, actual, and normal returns, 
cross-sectional regression models which are useful to investigate such hy- respectively, for time period t .  Xt is the conditioning information for 
potheses. Section 4.9 considers some further issues in event-study design the normal performance model. There are two common choices for 
and Section 4.10 concludes. modeling the normal return-the constant-mean-return model where Xt 

is a constant, and the market model where Xt is the market return. The 

4.1 Outline of an Event Study constant-mean-return model, as the name implies, assumes that the 
mean return of a given security is constant through time. The market 
model assumes a stable linear relation between the market return and 

At the outset it is useful to give a brief outline of the structure of an event the security return. 
study. While there is no unique structure, the analysis can be viewed 



4.3. Moo!& for Measuring Nmmal Per fmance  
152 4. fient-Study Analysis 

each firm and quarter, three pieces of information are compiled: the date 
4. Estimation pfocedure. Once a normal performance model has been se- 

lected, the parameters of the model must be estimated using a subset of the announcement, the actual announced earnings, and a measure of 
the expected earnings. The source of the date of the announcement is 

of the data known as the estimation window. The most common choice, 
when feasible, is to use the period prior to the event window for the esti- Datastream, and the source of the actual earnings is Compustat. 

mation window. For example, in an event study using daily data and the If earnings announcements convey information to investors, one would 
expect the announcement impact on the market's valuation of the firm's 

market model, the market-model parameters could be estimated over 
equity to depend on the magnitude of the unexpected component of the 

the 120 days prior to the event. Generally the event period itself is not 
announcement. Thus a measure of the deviation of the actual announced 

included in the estimation period to prevent the event from influencing 
earnings from the market's prior expectation is required. We use the mean 

the normal performance model pardmeter estimates. 
quarterly earnings forecast from the Institutional Brokers Estimate System 

5. Testingpocedure. With the parameter estimates for the normal perfor- 
(I/B/E/S) to proxy for the market's expectation of earnings. I/B/E/S com- 

mance model, the abnormal returns can be calculated. Next, we need 
piles forecasts from analysts for a large number of companies and reports 

to design the testing framework for the abnormal returns. Important 
summary statistics each month. The mean forecast is taken from the last 

considerations are defining the null hypothesis and determining the 
month of the quarter. For example, the mean third-quarter forecast from 

techniques for aggregating the abnormal returns of individual firms. 
6. September 1990 is used as the measure of expected earnings for the third 

Empirical results. The presentation of the empirical results follows the 
formulation of the econometric design. In addition to presenting the quarter of 1990. 

basic empirical results, the presentation of diagnostics can be fruitful. In order to examine the impact of the earnings announcement on the 

Occasionally, especially in studies with a limited number of event obser- value of the firm's equity, we assign each announcement to one of three 

vations, the empirical results can be heavily influenced by one or two categories: good news, no news, or bad news. We categorize each an- 

firms. Knowledge of this is important for gauging the importance of nouncement using the deviation of the actual earnings from the expected 

the results. earnings. If the actual exceeds expected by more than 2.5% the announce- 

7. ment is designated as good news, and if the actual is more than 2.5% less 
Interpretation and conclusions. Ideally the empirical results will lead to 
insights about the mechanisms by which the event affects security prices. than expected the announcement is designated as bad news. Those an- 

Additional analysis may be included to distinguish between competing nouncements where the actual earnings is in the 5% range centered about 

explanations. the expected earnings are designated as no news. Of the 600 announce- 
ments, 189 are good news, 173 are no news, and the remaining 238 are bad 
news. 

With the announcements categorized, the next step is to specify the 
4.2 An Example of an Event Study sampling interval, event window, and estimation window that will be used 

to analyze the behavior of firms' equity returns. For this example we set the 
The Financial Accounting Standards Board (FASB) and the Securities Ex- sampling interval to one day; thus daily stock returns are used. We choose a 
change Commission strive to set reporting regulations so that financial state- 41day event window, comprised of 20 pre-event days, the event day, and 20 
ments and related information releases are informative about the value of Postevent days. For each announcement we use the 250-tradingday period 
the firm. In setting standards, the information content of the financial dis- prior to the event window as the estimation window. After we present the 
closures is of interest. Event studies provide an ideal tool for examining the methodology of an event study, we use this example as an illustration. 
information content of the disclosures. 

In this section we describe an example selected to illustrate the event- 
study methodology. One particular type of disclosure-quarterly earnings 4.3 Models for Measuring Normal Performance 
announcements-is considered. We investigate the information content of 
quarterly earnings announcements for the thirty firms in the Dow Jones A number of approaches are available to calculate the normal return of a 
Industrial Index over the five-year period from January 1988 to December given security. The approaches can be loosely grouped into two categories- 
1993. These announcements correspond to the quarterly earnings for the Statistical and economic. Models in the first category follow from statistical 
last quarter of 1987 through the third quarter of 1993. The five years of assumptions concerning the behavior of asset returns and do not depend on 
data for thirty firms provide a total sample of 600 announcements For 



154 4. Event-Study Analysis 4.3. Models for Measuring Normal Performance 155 

any economic arguments. In contrast, models in the second category rely 4.3.2M arket Model 
on assumptions concerning investors' behavior and are not based solely on The market model is a statistical model which relates the return of any 
statistical assumptions. It should, however, be noted that to use economic given security to the return of the market portfolio. The model's linear 
models in practice it is necessary to add statistical assumptions. Thus the specification follows from the assumed joint normality of asset returns.4 
potential advantage of economic models is not the absence of statistical For any security i we have 
assumptions, but the opportunity to calculate more precise measures of the 
normal return using economic restrictions. 

For the statistical models, it is conventional to assume that asset re- 
turns are jointly multivariate normal and ind'ependently and identically dis- 
tributed through time. Formally, we have: where G.ta nd ata re the period-t returns on security i and the market 

portfolio, respectively, and rit is the zero mean disturbance term. ai ,  pi, 
(A l )  Let Rt be an ( N x  1) vector of asset returns for calendar time period t. R, is and :0 are the parameters of the market model. In applications a broad- 

independently multivariate normally distributed with mean p and covariance matrix based stock index is used for the market portfolio, with the S&P500 index, 
for all t. the CRSP value-weighted index, and the CRSP equal-weighted index being 

popular choices. 
The market model represents a potential improvement over the con- 

This distributional assumption is sufficient for the constant-mean-return 
model and the market model to be correctly specified and permits the de- stant-mean-return model. By removing the portion of the return that is 

related to variation in the market's return, the variance of the abnormal 
velopment of exact finitesample distributional results for the estimators 
and statistics. Inferences using the normal return models are robust to return is reduced. This can lead to increased ability to detect event effects. 

The benefit from using the market model will depend upon the R~ of the 
deviations from the assumption. Further, we can explicitly accommodate 

market-model regression. The higher the R*, the greater is the variance re- 
deviations using a generalized method of moments framework. 

duction of the abnormal return, and the larger is the gain. See Section 4.4.4 
for more discussion of this point. 

4.3.1 Constant-Mean-Return Model 

Let pi, the ith element of p, be the mean return for asset i. Then the 4.3.3 Other Statistical Models 
constant-mean-return model is A number of other statistical models have been proposed for modeling 

the normal return. A general type of statistical model is the factor model. 
fL = ~i + t i t  (4.3.1) Factor models potentially provide the benefit of reducing the variance of 

the abnormal return by explaining more of the variation in the normal 
E[&l = 0 Var[Citl = 02, return. Typically the factors are portfolios of traded securities. The market 

model is an example of a one-factor model, but in a multifactor model one 
where Gt,t he ith element of Rt,i s the period-t return on security i, titis  the might include industry indexes in addition to the market. Sharpe (1970) 
disturbance term, and :a is the (i, i) element of a. and Sharpe, Alexander, and Bailey (1995) discuss index models with factors 

Although the constant-mean-return model is perhaps the simplest based on industry classification. Another variant of a factor model is a 
model, Brown and Warner (1980, 1985) find it often yields results simi- procedure which calculates the abnormal return by taking the difference 
lar to those of more sophisticated models. This lack of sensitivity to the between the actual return and a portfolio of firms of similar size, where size 

model choice can be attributed to the fact that the variance of the abnormal is measured by market value of equity. In this approach typically ten size 
groups are considered and the loading on the size portfolios is restricted 

return is frequently not reduced much by choosing a more sophisticated 
model. When using daily data the model is typically applied to nominal 
returns. With monthly data the model can be applied to real returns or 4 ~ h sepe cification actually requires the asset weights in the market portfolio to remain 

constant. However, changes over time in the market portfolio weights are small enough that 
excess returns (the return in excess of the nominal riskfree return generally h e y  have little effect on  empirical work. 
measured using the US Treasury bill) as well as nominal returns. 



4.4. Measuring and Analyzing Abnormal Returns 
156 4. EventStudy Analysis 

Time Line: 
to unity. This procedure implicitly assumes that expected return is directly 
related to the market value of equity. estimation ] ( event ] ( post-event 

In practice the gains from employing multifactor models for event stud- window window window I 
ies are limited. The reason for this is that the marginal explanatory power of 
additional factors beyond the market factor is small, and hence there is little 
reduction in the variance of the abnormal return. The variance reduction 
will typically be greatest in cases where the sample firms have a common 
characteristic, for example they are all members of one industry or they are 
all firms concentrated in one market capitalization group. In these cases Figure 4.1. T i mL ine for an Event Study 

the use of a multifactor model warrants consideration. 
Sometimes limited data availability may dictate the use of a restricted 

model such as the markd-adjusted-returnm odel. For some events it is not feasi- 
ble to have a preevent estimation period for the normal model parameters, cross-section of mean returns, as shown by Fama and French (1996a) and 
and a market-adjusted abnormal return is used. The market-adjusted-return others, so a properly chosen APT model does not impose false restrictions 
model can be viewed as a restricted market model with ai constrained to be on mean returns. On the other hand the use of the APT complicates the 
0 and pic onstrained to be 1. Since the model coefficients are prespecified, implementation of an event study and has little practical advantage relative 
an estimation period is not required to obtain parameter estimates. This to the unrestricted market model. See, for example, Brown and Weinstein 
model is often used to study the underpricing of initial public offerings.5 (1985). There seems to be no good reason to use an economic model rather 
A general recommendation is to use such restricted models only as a last than a statistical model in an event study. 
resort, and to keep in mind that biases may arise if the restrictions are false. 

4.3.4 Economic Models 4.4 Measuring and Analyzing Abnormal Returns 

Economic models restrict the parameters of statistical models to provide In this section we consider the problem of measuring and analyzing abnor- 
more constrained normal return models. Two common economic models mal returns. We use the market model as the normal performance return 
which provide restrictions are the Capital Asset Pricing Model (CAPM) and model, but the analysis is virtually identical for the constant-mean-return 
exact versions of the Arbitrage Pricing Theory (APT). The CAPM, due to model. 
Sharpe (1964) and Lintner (1965b), is an equilibrium theory where the We first define some notation. We index returns in event time using 
expected return of a given asset is a linear function of its covariance with r .  Defining r = 0 as the event date, t = T, + 1 to r = T2 represents 
the return of the market portfolio. The APT, due to Ross (1976),i s an asset the event window, and r = To + 1 to r = TI constitutes the estimation 
pricing theory where in the absence of asymptotic arbitrage the expected window. Let L1 = TI - To and & = T2-  TI be the length of the estimation 
return of a given asset is determined by its covariances with multiple factors. window and the event window, respectively. If the event being considered 
Chapters 5 and 6 provide extensive treatments of these two theories. is an announcement on a given date then T2 = TI + 1 and = 1. If 

The Capital Asset Pricing Model was commonly used in event studies applicable, the postevent window will be from r = T2+  1 to r = T3 and its 
during the 1970s. During the last ten years, however, deviations from the length is LQ = T3 - T2. The timing sequence is illustrated on the time line 
CAPM have been discovered, and this casts doubt on the validity of the in Figure 4.1. 
restrictions imposed by the CAPM on the market model. Since these re- We interpret the abnormal return over the event window as a measure 
strictions can be relaxed at little cost by using the market model, the use of of the impact of the event on the value of the firm (or its equity). Thus, the 
the CAPM in event studies has almost ceased. methodology implicitly assumes that the event is exogenous with respect to 

Some studies have used multifactor normal performance models mo- the change in market value of the security. In other words, the revision in 
tivated by the Arbitrage Pricing Theory. The APT can be made t~ fit the value of the firm is caused by the event. In most cases this methodology is 

appropriate, but there are exceptions. There are examples where an event 
is triggered by the change in the market value of a security, in which case 

%ee Ritter (1990) for an example. 



158 4. Event-Study Analysis 4.4. Measuring and Analyzing Abnormal Returns 159 

the event is endogenous. For these cases, the usual interpretation will be properties of abnormal returns. First we consider the abnormal return 
properties of a given security and then we aggregate across securities. 

incorrect. 
It is typical for the estimation window and the event window not to over- 

lap. This design provides estimators for the parameters of the normal return 4.4.2 Statistical Properties of Abnormal Returns 
model which are not influenced by the event-related returns. Including the 
event window in the estimation of the normal model parameters could lead Given the market-model parameter estimates, we can measure and analyze 

to the event returns having a large influence on the normal return mea- the abnormal returns. Let i: be the (& x 1) sample vector of abnormal 

sure. In this situation both the normal returns and the abnormal returns returns for firm i from the event window, TI + 1 to T2. Then using the 

would reflect the impact of the event. This would be problematic since the market model to measure the normal return and the OLS estimators from 

methodology is built around the assumption that the event impact is cap- (4.4.3),w e have for the abnormal return vector: 

tured by the abnormal returns. In Section 4.5 we consider expanding the 
null hypothesis to accommodate changes in the risk of a firm around the 
event. In this case an estimation framework which uses the event window 
returns will be required. 

where Rf = [&T,+I . . .&T? 1' is an (& x 1) vector of event-window returns, 
4.4.1 Estimation of the Market Model Xt = [ L  R*,] is an (& x2) matrix with a vector of ones in the first column 

and the vector of market return observations R: = . . . &r2]' in the 
Recall that the market model for security i and observation t in event time second column, and e i  = [Gi& I '  is the (2x 1) parameter vector estimate. 
is 

fir = ai + + Conditional on the market return over the event window, the abnormal re- 
€ir. (4.4.1) turns will be jointly normally distributed with a zero conditional mean and 

The estimation-window observations can be expressed as a regression sys- conditional covariance matrix Via s shown in (4.4.8) and (4.4.9), respec- 
tem, tively. 

R~ = xiei+  E i ,  

where Ri = . . . &,I' is an (Llx  1) vector of estimation-window re- 
turns, Xi = [ L  R,] is an (L1x 2) matrix with a vector of ones in the first col- 
umn and the vector of market return observations R, = [&To+l . . . &TI]' 
in the second column, and ei=  [a ip i]' is the (2 x 1) parameter vector. X has 
a subscript because the estimation window may have timing that is specific 
to firm i. Under general conditions ordinary least squares (OLS) is a consis- 
tent estimation procedure for the market-model parameters. Further, given 
the assumptions of Section 4.3, OLS is efficient. The OLS estimators of the 
market-model parameters using an estimation window of Ll observations 
are 

I is the (L2x  &) identity matrix. 
From (4.4.8) we see that the abnormal return vector, with an expecta- 

tion of zero, is unbiased. The covariance matrix of the abnormal return 
vector from (4.4.9) has two parts. The first term in the sum is the variance 
due to the future disturbances an$ the second term is the additional vari- 
ance due to the sampling error in O i .  This sampling error, which is common 

We next show how to use these OLS estimators to measure the statistical 



160 4. Event-Study Analysis 4.4. Measuring and Analyzing Abnormal Returns 161 

for all the elements of the abnormal return vector, will lead to serial corre- degrees of freedom. From the properties of the Student t distribution, 
lation of the abnormal returns despite the fact that the true disturbances the expectation of S a i ( r 1 . r 2) is 0 and the variance is ( M IF.o r a large 

LI-4  
are independent through time. As the length of the estimation window L1 estimation window (for example, Ll > 30), the distribution of SCARi(rl,r 2) 
becomes large, the second term will approach zero as the sampling error of will be well approximated by the standard normal. 
the parameters vanishes, and the abnormal returns across time periods will The above result applies to a sample of one event and must be extended 
become independent asymptotically. for the usual case where a sample of many event observations is aggregated. 

Under the null hypothesis, Ho, that the given event has no impact on To aggregate across securities and through time, we assume that there is 
the mean or variance of returns, we can use (4.4.8)a nd (4.4.9)a nd the joint not any correlation across the abnormal returns of different securities. This 
normality of the abnormal returns to draw inferences. Under Ho, for the will generally be the case if there is not any clustering, that is, there is not 
vector of event-window sample abnormal returns we have any overlap in the event windows of the included securities. The absence of 

any overlap and the maintained distributional assumptions imply that the 
abnormal returns and the cumulative abnormal returns will be independent 
across securities. Inferences with clustering will be discussed later. 

Equation (4.4.10) gives us the distribution for any single abnormal return The individual securities' abnormal returns can be averaged using 2.: 
observation. We next build on this result and consider the aggregation of from (4.4.7). Given a sample of N events, defining E* as the sample average 
abnormal returns. of the N abnormal return vectors, we have 

4.4.3 Aggregation of A b n m a l  Returns 

The abnormal return observations must be aggregated in order to draw 
overall inferences for the event of interest. The aggregation is along two 
dimensions-through time and across securities. We will first consider ag- 
gregation through time for an individual security and then will consider 
aggregation both across securities and through time. 

We introduce the cumulative abnormal return to accommodate multi- We can aggregate the elements of this average abnormal returns vector 
ple sampling intervals within the event window. Define CARi(rl, r2) as the through time u-sing the same approach as we did for an individual security's 
cumulative abnormal return for security i from rl to 52 where Tl < rl 5 vector. Define CAR(rl, r2)a s the cumulative average abnormal return from 
r2 5 T2. Let 7 be an (&x  1) vector with ones in positions rl - TI to r2 - Tl rl to 52 where Tl < rl 5 r2 5 T2 and 7 again represents an (& x 1) vector 
and zeroes elsewhere. Then we have with ones in positions rl - Tl to r2 - Tl and zeroes elsewhere. For the 

cumulative average abnormal return we have 

Var[mi (r l ,  rd1 = $(51, rg) = ~ ' V i 7 .  (4.4.12) 

It follows from (4.4.10) that under Ho, 

m i ( r 1 , r 2) - N(o,o ?(rl, rd). (4.4.13) - 
Equivalently, to obtain CAR(rl, r2),w e can aggregate using the sample 

cumulative abnormal return for each security i. For N events we have 
We can construct a test of Ho for security i from (4.4.13)u sing the standard- 
ized cumulative abnormal return, 

where 8:(rl, Q) is calculated with :3 from (4.4.4)s ubstituted for 0:. Under 
the null hypothesis the distribution of sm,(sQl), is  Student t with L1 - 2 



162 4. Event-Study Analysis 4.4. Measuring and Analyzing Abnonnal Returns 163 

In (4.4.16), (4.4.18), and (4.4.20) we use the assumption that the event constant-mean-return model will lead to a reduction in the abnormal re- 
windows of the N securities do not overlap to set the covariance terms to turn variance. This point can be shown by comparing the abnormal return 
zero. Inferences about the cumulative abnormal returns can be drawn using variances. For this illustration we take the normal return model parameters 

as given. 
The variance of the abnormal return for the market model is 

since under the null hypothesis the expectation of the abnorm- al returns 
2 

is zero. In practice, since a 2 ( r 1t,  2)is  unknown, we can use 5 ( t l ,t 2)=  
1 xE1& :(ttl2, )as  a consistent estimator and proceed to test Ho using 

where R: is the R~ of the market-model regression for security i. 
For the constant-mean-return model, the variance of the abnormal re- 

This distributional result is for large samples of events and is not exact 
turn titis  the variance of the unconditional return, Var[R,], that is, 

because an estimator of the variance appears in the denominator. 
A second method of aggregation is to give equal weighting to the indi- 

vidual SCARi'%D efining SCAR(t l ,t 2a)s  the average over N securities from 
event time 51 to t p ,  we have Combining (4.4.25) and (4.4.26) we have 

Since R~~li es between zero and one, the variance of the abnormal return 
Assuming that the event windows of the N securities do not overlap in using the market model will be less than or equal to the abnormal return 
calendar time, under Ho, SCAR(t l ,t 2)w ill be normally distributed in large variance using the constant-mean-return model. This lower variance for 
samples with a mean of zero and variance (N$24,). We can test the null the market model will carry over into all the aggregate abnormal return 
hypothesis using measures. As a result, using the market model can lead to more precise 

inferences. The gains will be greatest for a sample of securities with high 
market-model R2 statistics. 

In principle further increases in R2 could be achieved by using a multi- 
When doing an event study one will have to choose between using J1 or J2 factor model. In practice, however, the gains in R2 from adding additional 

for the test statistic. One would like to choose the statistic with higher power, factors are usually small. 
and this will depend on the alternative hypothesis. If the true abnormal 
return is constant across securities then the better choice will give more 4.4.5 CARS for the Earnings-Announcement Example 
weight to the securities with the lower abnormal return variance, which is 
what J2 does. On the other hand if the true abnormal return is larger for The earnings-announcement example illustrates the use of sample abnor- 

securities with higher variance, then the better choice will give equal weight mal returns and sample cumulative abnormal returns. Table 4.1 presents 

to the realized cumulative abnormal return of each security, which is what J the abnormal returns averaged across the 30 firms as well as the averaged 
cumulative abnormal return for each of the three earnings news categories. 

does. In most studies, the results are not likely to be sensitive to the choice 
TWOn ormal return models are considered: 

of J1 versus J2 because the variance of the CAR is of a similar magnitude the market model and, for 
comparison, the constant-mean-return model. Plots of the cumulative ab- 

across securities. 
normal returns are also included, with the CARs from the market model 
in Figure 4.2a and the CARs from the constant-mean-return model in Fig- 

4.4.4 Sensitivity to Nomal Return Model ure 4.2b. 

We have developed results using the market model as the normal return The results of this example are largely consistent with the existing lit- 

model. As previously noted, using the market model as opposed to the erature on the information content of earnings. The evidence strongly 



4.4. Measuring and Analyzing Abnormal Returns 

_ .'--. 
Tdle 4.1. Abnormal return for an went study of the infmmation content of earnings an- 0.02 1 ,-- 

l-'\_,_.._-' - _ _ - -  
nouncements. Good-News Firms 

- - 

 arki it Model Constant-Mean-Return Model 
Event 

Good News No News Bad News Good News N o  News Bad News 

Figurn 4 . 2 ~ .  Plot of Cumulative Market-Model A b n m l  Return for Earning Announce- 
mats 

,--,..,  ,--. -
\. .< -  

I - . - - '  __-- 
Good-News Firms 

Event Time 

The sample consists of a total of 600 quarterly announcements for the thirty companies in the 
DowJones Industrial Index for the five-year period January 1989 to December 1993. Two mod- F*rn 4.2b. Plot of Cumulative Constant-Mean-Return-Model Abnormal Return for Earn- 
els are considered for the normal returns, the market model using the CRSP value-weighted ing Announcements 
index and the constant-mean-return model. The announcements are categorized into three 
groups, good news, no news, and bad news. ?* is the sample average abnormal return for the 
specified day in event time and is the sample average cumulative abnormal return for day supports the hypothesis that earnings announcements do indeed convey in- 
-20 to the specified day. Event time is measured in days relative to the announcement date. formation useful for the valuation of firms. Focusing on the announcement 

day (day zero) the sample average abnormal return for the good-news firm 



166 4. Event-Study Analysis 4.5. Modzjjing the Null Hypothesis 167 

using the market model is 0.965%. Since the standard error of the oneday overlap, the covariances between the abnormal returns may differ from 
good-news average abnormal return is 0.104%, the value of Jl is 9.28 and zero, and the distributional results presented for the aggregated abnormal 
the null hypothesis that the event has no impact is strongly rejected. The returns are not applicable. Bernard (1987) discusses some of the problems 
story is the same for the bad-news firms. The event day sample abnormal related to clustering. 
return is -0.679%, with a standard error of O.O98%, leading to Jl equal to When there is one event date in calendar time, clustering can be ac- 
-6.93 and again strong evidence against the null hypothesis. As would be commodated in two different ways. First, the abnormal returns can be 
expected, the abnormal return of the no-news firms is small at -0.091% aggregated into a portfolio dated using event time, and the security level 
and, with a standard error of 0.098%, is less than one standard error from analysis of Section 4.4 can be applied to the portfolio. This approach allows 
zero. There is also some evidence of the announcement effect on day one. for cross correlation of the abnormal returns. 
The average abnormal returns are 0.251% and -0.204% for the good-news A second way to handle clustering is to analyze the abnormal returns 
and the bad-news firms respectively. Both these values are more than two without aggregation. One can test the null hypothesis that the event has no 
standard errors from zero. The source of these day-one effects is likely to be impact using unaggregated security-by-security data. The basic approach is 
that some of the earnings announcements are made on event day zero after an application of a multivariate regression model with dummy variables for 
the close of the stock market. In these cases the effects will be captured in the event date; it is closely related to the multivariate F-test of the CAPM pre- 
the return on day one. sented in Chapter 5. The approach is developed in the papers of Schipper 

The conclusions using the abnormal returns from the constant-mean- and Thompson (1983,1985),M alatesta and Thompson (1985), and Collins 
return model are consistent with those from the market model. However, and Dent (1984). It has some advantages relative to the portfolio approach. 
there is some loss of precision using the constant-mean-return model, as the First, it can accommodate an alternative hypothesis where some of the firms 
variance of the average abnormal return increases for all three categories. have positive abnormal returns and some of the firms have negative abnor- 
When measuring abnormal returns with the constant-mean-return model mal returns. Second, it can handle cases where there is partial clustering, 
the standard errors increase from 0.104% to 0.130% for good-news firms, that is, where the event date is not the same across firms but there is overlap 
from 0.098% to 0.124% for nc-news firms, and from 0.098% to 0.131% in the event windows. This approach also has some drawbacks, however. In 
for bad-news firms. These increases are to be expected when considering many cases the test statistic has poor finite-sample properties, and often it 
a sample of large firms such as those in the Dow Index since these stocks has little power against economically reasonable alternatives. 
tend to have an important market component whose variability is eliminated 
using the market model. 

The CAR plots show that to some extent the market gradually learns 4.5 Modifying the Null Hypothesis 
about the forthcoming announcement. The average CAR of the good-news 
firms gradually drifts up in days -20 to -1, and the average CAR of the Thus far we have focused on a single null hypothesis-that the given event 
bad-news firms gradually drifts down over this period. In the days after the has no impact on the behavior of security returns. With this null hypothesis 
announcement the CAR is relatively stable, as would be expected, although either a mean effect or a variance effect represents a violation. However, 
there does tend to be a slight (but statistically insignificant) increase for the in some applications we may be interested in testing only for a mean effect. 
bad-news firms in days two through eight. In these cases, we need to expand the null hypothesis to allow for changing 

(usually increasing) variances. 
To accomplish this, we need to eliminate any reliance on past returns 

4.4.6I nfmences with Clustering in estimating the variance of the aggregated cumulative abnormal returns. 
In analyzing aggregated abnormal returns, we have thus far assumed that Instead, we use the cross section of cumulative abnormal returns to form 
the abnormal returns on individual securities are uncorrelated in the cross an estimator of the variance. Boehmer, Musumeci, and Poulsen (1991) 
section. This will generally be a reasonable assumption if the event windows discuss this methodology, which is best applied using the constant-mean- 
of the included securities do not overlap in calendar time. The assumption return model to measure the abnormal return. 
allows us to calculate the variance of the aggregated sample cumulative The cross-sectional approach to estimating the- varian ce can be applied 
abnormal returns without concern about covariances between individual to both the average cumulative abnormal return (CAR( t l ,t 2)a)n d the av- 
sample CARS, since they are zero. However, when the event windows do erage standardized cumulative abnormal return (SCAR(tl ,t 2).) U sing the 



168 4. Event-Study Analysis 4.6. Analysis of Power 169 

cross section to form estimators of the variances we have Given an alternative hypothesis H.4 and the CDF of Ji for this hypothesis, 
we can tabulate the power of a test of size a! using 

With this framework in place, we need to posit specific alternative hy- 
potheses. Alternatives are constructed to be consistent with event studies 
using data sampled at a daily interval. We build eight alternative hypotheses 
using four levels of abnormal returns, 0.5%, 1.0%, 1.5%, and 2.0%, and two 
levels for the average variance of the cumulative abnormal return of a given 

For these estimators of the variances to be consistent we require the security over the sampling interval, 0.0004 and 0.0016. These variances cor- 
abnormal returns to be uncorrelated in the cross section. An absence of respond to standard deviations of 2% and 4%, respectively. The sample size, 
clustering is sufficient for this requirement. Note that cross-sectional ho- that is the number of securities for which the event occurs, is varied from 
moskedasticity is not required for consistency. Given these variance estima- 1 to 200. We document the power for a test with a size of 5% (a! = 0.05) 
tors, the null hypothesis that the cumulative abnormal returns are zero can givingvalues of -1.96 and 1.96 for W1( (r/2) and W' (1 - a!/2), respectively. 
then be tested using large sample theory given the consistent estimators of In applications, of course, the power of the test should be considered when 
the variances in (4.5.2) and (4.5.1). selecting the size. 

One may also be interested in the impact of an event on the risk of a The power results are presented in Table 4.2 and are plotted in Figures 
firm. The relevant measure of risk must be defined before this issue can 4.3a and 4.3b. The results in the left panel of Table 4.2 and in Figure 4.3a 
be addressed. One choice as a risk measure is the market-model beta as are for the case where the average variance is 0.0004, corresponding to a 
implied by the Capital Asset Pricing Model. Given this choice, the market standard deviation of 2%. This is an appropriate value for an event which 
model can be formulated to allow the beta to change over the event window does not lead to increased variance and can be examined using a oneday 
and the stability of the beta can be examined. See Kane and Unal (1988) event window. Such a case is likely to give the event-study methodology its 
for an application of this idea. highest power. The results illustrate that when the abnormal return is only 

0.5% the power can be low. For example, with a sample size of 20 the power 
of a 5% test is only 0.20. One needs a sample of over 60 firms before the 

4.6 Analysis of Power power reaches 0.50. However, for a given sample size, increases in power 
are substantial when the abnormal return is larger. For example, when the 

To interpret an event study, we need to know what is our ability to detect abnormal return is 2.0% the power of a 5% test with 20 firms is almost 1.00 

the presence of a nonzero abnormal return. In this section we ask what is with a value of 0.99. The general results for a variance of 0.0004 is that 

the likelihood that an event-study test rejects the null hypothesis for a given when the abnormal return is larger than 1% the power is quite high even 

level of abnormal return associated with an event, that is, we evaluate the for small sample sizes. When the abnormal return is small a larger sample 

power of the test. size is necessary to achieve high power. 

We consider a two-sided test of the null hypothesis using the cumulative- In the right panel of Table 4.2 and in Figure 4.3b the power results 
are presented for the case where the average variance of the cumulative 

abnormal-return-based statistic J1 from (4.4.22). We assume that the- abno r- 
abnormal return is 0.0016, corresponding to a standard deviation of 4%. 

ma1 returns are uncorrelated across securities; thus the variance of CAR is 
This case corresponds roughly to either a multiday event window or to a 

a2(r1,r p),w here a2(r1,r 2) = 1 /N2 xL1a 2(rl, r2) and N is the sample size. 
oneday event window with the event leading to increased variance which 

Under the null hypothesis the distribution of Jl is standard normal. For a 
is accommodated as part of the null hypothesis. Here we see a dramatic 

two-sided test of size a! we reject the null hypothesis if Jl < @-'(a!/2) or if decline in the power of a 5% test. When the CAR is 0.5% the power is only 
JI > W1( 1 -(u/2) where a(.)i s the standard normal cumulative distribution 
function (CDF).  0.09 with 20 firms and only 0.42 with a sample of 200 firms. This magnitude 



4. Event-Study Analysis 4.6. Analysis of Power 

Table 4.2. Power of event-study test statistic Jl to rqect the nud l hypothesis that the abnormal 
return is zero. 

Sample Abnormal Return Abnormal Return 
Size 0.5% 1.0% 1.5% 2.0% 0.5% 1.0% 1.5% 2.0% 

1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 , 1 1 1 1 ~  

O 10 20 30 40 50 60 70 80 90 100 
Sample Size 

(a) 

O 10 20 30 40 50 60 70 80 90 100 
Sample Size 

Figure 4.3. Power of Event-Study Test Statistic J1 to Reject the Null Hypothesis that the 
Abnormal Return Is Zero, When the Square Root of the Average Variance of the Abnormal 
Return Amss Finns is (a)  2 % and (6) 4 % 

The power is reported for a test with a size of 5%. The sample size is the number of event 
observations included in the study, and a is the square root of the average variance of the there is a sample size of 30, the power is 0.54. Generally if the abnormal 
abnormal return across firms. 

return is large one will have little difficulty rejecting the null hypothesis of 
no abnormal return. 

of abnormal return is difficult to detect with the larger variance of 0.0016. We have calculated power analytically using distributional assumptions. 

In contrast, when the CAR is as large as 1.5% or 2.0% the 5% test still has If these distributional assumptions are inappropriate then our power calcu- 

reasonable power. For example, when the abnormal return is 1.5% and lations may be inaccurate. However, Brown and Warner (1985) explore this 



4. Event-Study Analysis 4.8. Cross-Sectional Moakls 

issue and find that the analytical computations and the empirical power are event day zero. The framework can be easily altered for events occurn 
very close. over multiple days. 

It is difficult to reach general conclusions concerning the the ability Drawing on notation previously introduced, consider a sample of 
of event-study methodology to detect nonzero abnormal returns. When abnormal returns for each of N  securities. To implement the rank test 
conducting an event study it is necessary to evaluate the power given the is necessary for each security to rank the abnormal returns from 1 to 1 
parameters and objectives of the study. If the power seems sufficient then Define K,, as the rank of the abnormal return of security i for event tin 
one can proceed, otherwise one should search for ways of increasing the period t. Recall that t ranges from TI+  1 to T2 and r = 0 is the event da 
power. This can be done by increasing the sample size, shortening the event The rank test uses the fact that the expected rank under the null hypothes 
window, or by developing more specific predictions of the null hypothesis. is y.Th e test statistic for the null hypothesis of no abnormal return 01 

event day zero is: 

4.7 Nonparametric Tests 

The methods discussed to this point are parametric in nature, in that specific 
assumptions have been made about the distribution of abnormal returns. 
Alternative nonparametric approaches are available which are free of spe- 
cific assumptions concerning the distribution of returns. In this section we Tests of the null hypothesis can be implemented using the result that the 
discuss two common nonparametric tests for event studies, the sign test and asymptotic null distribution of J4 is standard normal. Corrado (1989) gives 
the rank test. further details. 

The sign test, which is based on the sign of the abnormal return, re- Typically, these nonparametric tests are not used in isolation but in 
quires that the abnormal returns (or more generally cumulative abnormal conjunction with their parametric counterparts. The nonparametric tests 
returns) are independent across securities and that the expected propor- enable one to check the robustness of conclusions based on parametric 
tion of positive abnormal returns under the null hypothesis is 0.5. The 

tests. Such a check can be worthwhile as illustrated by the work of Campbell 
basis of the test is that under the null hypothesis it is equally probable that and Wasley (1993). They find that for daily returns on NASDAQ stocks 
the CAR will be positive or negative. If, for example, the null hypothesis the nonparametric rank test provides more reliable inferences than do the 
is that there is a positive abnormal return associated with a given event, 

p standard parametric tests. 
the null hypothesis is Ho: 5 0.5 and the alternative is HA:p  r 0.5 where 
p = Pr(CARi >_ 0.0).  To calculate the test statistic we need the number of 
cases where the abnormal return is positive, N + ,  and the total number of 4.8 Cross-Sectional Models 
cases, N. Letting J3 be the test statistic, then asymptotically as N  increases 
we have Theoretical models often suggest that there should be an association be- 

Ween the magnitude of abnormal returns and characteristics specific to 
the event observation. To investigate this association, an appropriate tool 
is a cross-sectional regression of abnormal returns on the characteristics of 
interest. To set up the model, define y as an ( N x 1 ) vector of cumulative 

For a test of size ( 1  - a ) ,H o is rejected if J3 > @-'(a). abnormal return observations and X as an ( N x K )  matrix of characteris- 
A weakness of the sign test is that it may not be well specified if the tics. The first column of X is a vector of ones and each of the remaining 

distribution of abnormal returns is skewed, as can be the case with daily ( K  - 1 )  columns is a vector consisting of the characteristic for each event 
data. With skewed abnormal returns, the expected proportion of positive observation. Then, for the model, we have the regression equation 
abnormal returns can differ from one half even under the null hypothesis. 
In response to this possible shortcoming, Corrado (1989) proposes a non- 
parametric rank test for abnormal performance in event studies. We briefly 
A P C C A ~ Ph i= t p ~ nt  f the nllll hvnntht.qi9 that there is no abnormal return on 



174 4. Event-Study Analysis 4.9. Further Issues 175 

For the OLS estimator we have investors rationally use firm characteristics to forecast the likelihood of the 
event occurring. In these cases, a linear relation between the firm charac- 
teristics and the valuation effect of the event can be hidden. Malatesta and 
Thompson (1985) and Lanen and Thompson (1988) provide examples of 

Assuming the elements of Q are cross-sectionally uncorrelated and homo- this situation. 
skedastic, inferences can be derived using the usual OLS standard errors. Technically, the relation between the firm characteristics and the degree 
Defining a$ as the variance of the elements of Q we have of anticipation of the event introduces a selection bias. The assumption 

that the regression residual is uncorrelated with the regressors, E [X'Q]=  0, 
breaks down and the OLS estimators are inconsistent. Consistent estimators 
can be derived by explicitly allowing for the selection bias. Acharya (1988, 

Using the unbiased estimator for 4, 1993) and Eckbo, Maksimovic, and Williams (1990) provide examples of 
this. Prabhala (1995) provides a good discussion of this problem and the 
possible solutions. He argues that, despite misspecification, under weak 
conditions, the OLS approach can be used for inferences and the t-statistics 
can be interpreted as lower bounds on the true significance level of the 

where 6 = y - ~ 9we, ca n construct t-statistics to assess the statistical signifi- 
estimates. 

cance of the elements of e. Alternatively, without assuming homoskedastic- 
ity, we can construct heteroskedasticity-consistent z-statistics using 

4.9 Further Issues 

A number of further issues often arise when conducting an event study. We 
discuss some of these in this section. 

where x: is the ith row of X and 6 ,i s the ith element of 6. This expression 
for the standard errors can be derived using the Generalized Method of Mo- 4.9.1 Role of the Sampling Interval 
ments framework in Section A.2 of the Appendix and also follows from the 
results of White (1980). The use of heteroskedasticity-consistents tandard If the timing of an event is known precisely, then the ability to statistically 
errors is advised since there is no reason to expect the residuals of (4.8.1) identify the effect of the event will be higher for a shorter sampling interval. 
to be homoskedastic. The increase results from reducing the variance of the abnormal return 

Asquith and Mullins (1986) provide an example of this approach. The without changing the mean. We evaluate the empirical importance of this 
two-day cumulative abnormal return for the announcement of an equity issue by comparing the analytical formula for the power of the test statistic 
offering is regressed on the size of the offering as a percentage of the value J1 with a daily sampling interval to the power with a weekly and a monthly 
of the total equity of the firm and on the cumulative abnormal return in interval. We assume that a week consists of five days and a month is 22 days. 
the eleven months prior to the announcement month. They find that the The variance of the abnormal return for an individual event observation is 
magnitude of the (negative) abnormal return associated with the announce- assumed to be (4%)2o n a daily basis and linear in time. 
ment of equity offerings is related to both these variables. Larger pre-event In Figure 4.4, we plot the power of the test of no event-effect against 
cumulative abnormal returns are associated with less negative abnormal the alternative of an abnormal return of 1% for 1 to 200 securities. As 
returns, and larger offerings are associated with more negative abnormal one would expect given the analysis of Section 4.6, the decrease in power 
returns. These findings are consistent with theoretical predictions which going from a daily interval to a monthly interval is severe. For example, 
they discuss. with 50 securities the power for a 5% test using daily data is 0.94, whereas 

One must be careful in interpreting the results of the cross-sectional re- the power using weekly and monthly data is only 0.35 and 0.12, respectively. 
gression approach. In many situations, the event-window abnormal return The clear message is that there is a substantial payoff in terms of increased 

will be related to firm characteristics not only through the valuation effects power from reducing the length of the event window. Morse (1984) presents 
detailed analysis of the choice of daily versus monthly data and draws the 

of the event but also through a relation between the firm characteristics 
same conclusion. 

and the extent to which the event is anticipated. This can happen when 



4. Event-Study Analysis 4.9. Further Issues 177 

I ~ I ~ l l I r I '  Ball and Torous (1988) investigate this issue. They develop a maximum- 
- 

- - _ _ _ - - - _  likelihood estimation procedure which accommodates eventdate uncer- 
- tainty and examine results of their explicit procedure versus the informal 
- procedure of expanding the event window. The results indicate that the 
- informal procedure works well and there is little to gain from the more 

elaborate estimation framework. 
- 

One-Week Interval 

_----- -- - 4.9.3 Possible Biases 

Event studies are subject to a number of possible biases. Nonsynchrono~~ 
trading can introduce a bias. The nontrading or nonsynchronous trading 

I  I I  I I I I I I I I I I  I I  

O 1  ' 2 0  40 ' 6b effect arises when prices are taken to be recorded at time intervals of one 
' 80 100 120 140 160 180 200 length when in fact they are recorded at time intervals of other possibly 

Sample Size irregular lengths. For example, the daily prices of securities usually em- 
ployed in event studies are generally "closing" prices, prices at which the 

Figure 4.4. Power of Event-Study Test Statistic J1 to Rqect the Null Hypothesis that the last transaction in each of those securities occurred during the trading day. 
Abnormal Return is Zero, fm Dqferent Sampling Intervals, Wzen the Square Root of the These closing prices generally do not occur at the same time each day, but by 
Average Variance ofthe Abnormal Return Across Firm Is 4 % for the Daily Interval calling them "daily" prices, we have implicitly and incorrectly assumed that 

they are equally spaced at 24hour intervals. As we showed in Section 3.1 
of Chapter 3, this nontrading effect induces biases in the moments and 
cemoments of returns. 

A sampling interval of one day is not the shortest interval possible. The influence of the nontrading effect on the variances and covariances 
With the increased availability of transaction data, recent studies have used of individual stocks and portfolios naturally feeds into a bias for the market- 
observation intervals of duration shorter than one day. The use of intra- model beta. Scholes and Williams (1977) present a consistent estimator of 
daily data involves some complications, however, of the sort discussed in beta in the presence of nontrading based on the assumption that the true 
Chapter 3, and so the net benefit of very short intervals is unclear. Barclay return process is uncorrelated through time. They also present some em- 
and Litzenberger (1988) discuss the use of intradaily data in event studies. pirical evidence showing the nontrading-adjusted beta estimates of thinly 

traded securities to be approximately 10 to 20% larger than the unadjusted 
estimates. However, for actively traded securities, the adjustments are gen- 
erally small and unimportant. 

4.9.2 Inferences with Event-Date Uncertainty Jain (1986) considers the influence of thin trading on the distribution 
Thus far we have assumed that the event date can be identified with certainty. of the abnormal returns from the market model with the beta estimated 
However, in some studies it may be difficult to identify the exact date. A using the Scholes-Williams approach. He compares the distribution of these 
common example is when collecting event dates from financial publications abnormal returns to the distribution of the abnormal returns using the usual 
such as the Wall Street Journal. When the event announcement appears in OLS betas and finds that the differences are minimal. This suggests that in 
the newspaper one can not be certain if the market was informed before general the adjustment for thin trading is not important. 
the close of the market the prior trading day. If this is the case then the The statistical analysis of Sections 4.3, 4.4, and 4.5 is based on the as- 
prior day is the event day; if not, then the current day is the event day. The sumption that returns are jointly normal and temporally IID. Departures 
usual method of handling this problem is to expand the event window to from this assumption can lead to biases. The normality assumption is im- 

two days--day 0 and day +l. While there is a cost to expanding the event portant for the exact finite-sample results. Without assuming normality, all 

window, the results in Section 4.6 indicate that the power properties of two- results would be asymptotic. However, this is generally not a problem for 

day event windows are still good, suggesting that it is worth bearing the cost event studies since the test statistics converge to their asymptotic distribu- 
to avoid the risk of missing the event. tions rather quickly. Brown and Warner (1985) discuss this issue. 



4. Event-Study Analysis 4.10. Conclusion 179 

There can also be an upward bias in cumulative abnormal returns when return for target shareholders exceeds 20% for a sample of 663 successful 
these are calculated in the usual way. The bias arises from the observation- takeovers from 1960 to 1985. In contrast the abnormal return for acquirers 
by-observation rebalancing to equal weights implicit in the calculation of is close to zero at 1.14%, and even negative at -1 .10% in the 1980's. 
the aggregate cumulative abnormal return combined with the use of trans- Eckbo (1983) explicitly addresses the role of increased market power 
action prices which can represent both the bid and the ask side of the in explaining merger-related abnormal returns. He separates mergers of 
market. Blume and Stambaugh (1983) analyze this bias and show that it competing firms from other mergers and finds no evidence that the wealth 
can be important for studies using low-marketcapitalization firms which effects for competing firms are different. Further, he finds no evidence that 
have, in percentage terms, wide bid-ask spreads. In these cases the bias can rivals of firms merging horizontally experience negative abnormal returns. 
be eliminated by considering cumulative abnormal returns that represent From this he concludes that reduced competition in the product market 
buy-and-hold strategies. is not an important explanation for merger gains. This leaves competition 

for corporate control a more likely explanation. Much additional empirical 
work in the area of mergers and acquisitions has been conducted. Jensen 
and Ruback (1983) and Jarrell, Brickley, and Netter (1988) provide detailed 

4.10 Conclusion surveys of this work. 
A number of robust results have been developed from event studies 

In closing, we briefly discuss examples of event-study successes and limita- of financing decisions by corporations. When a corporation announces 
tions. Perhaps the most successful applications have been in the area of that it will raise capital in external markets there is on average a negative 
corporate finance. Event studies dominate the empirical research in this abnormal return. The magnitude of the abnormal return depends on the 
area. Important examples include the wealth effects of mergers and acqui- source of external financing. Asquith and Mullins (1986) study a sample of 
sitions and the price effects of financing decisions by firms. Studies of these 266 firms announcing an equity issue in the period 1963 to 1981 and find 
events typically focus on the abnormal return around the date of the first that the two-day average abnormal return is -2.7%, while on a sample of 
announcement. 80 firms for the period 1972 to 1982 Mikkelson and Partch (1986) find that 

In the 1960s there was a paucity of empirical evidence on the wealth the two-day average abnormal return is -3.56%. In contrast, when firms 
effects of mergers and acquisitions. For example, Manne (1965) discusses decide to use straight debt financing, the average abnormal return is closer 
the various arguments for and against mergers. At that time the debate cen- to zero. Mikkelson and Partch (1986) find the average abnormal return 
tered on the extent to which mergers should be regulated in order to foster for debt issues to be -0.23% for a sample of 171 issues. Findings such as 
competition in the product markets. Manne argues that mergers represent these provide the fuel for the development of new theories. For example, 
a natural outcome in an efficiently operating market for corporate control these external financing results motivate the pecking order theory of capital 
and consequently provide protection for shareholders. He downplays the structure developed by Myers and Majluf (1984). 
importance of the argument that mergers reduce competition. At the con- A major success related to those in the corporate finance area is the 
clusion of his article Manne suggests that the two competing hypotheses implicit acceptance of event-study methodology by the U.S. Supreme Court 
for mergers could be separated by studying the price effects of the involved for determining materiality in insider trading cases and for determining 
corporations. He hypothesizes that if mergers created market power one appropriate disgorgement amounts in cases of fraud. This implicit accep- 
would observe price increases for both the target and acquirer. In contrast tance in the 1988 Basic, Incorporated v. Levinson case and its importance 
if the merger represented the acquiring corporation paying for control of for securities law is discussed in Mitchell and Netter (1994). 
the target, one would observe a price increase for the target only and not There have also been less successful applications of event-study method- 
for the acquirer. However, at that time Manne concludes in reference to ology. An important characteristic of a successful event study is the ability 
the price effects of mergers that ". . . no data are presently available on this to identify precisely the date of the event. In cases where the date is difficult 
subject." to identify or the event is partially anticipated, event studies have been less 

Since that time an enormous body of empirical evidence on mergers and useful. For example, the wealth effects of regulatory changes for affected en- 
acquisitions has developed which is dominated by the use of event studies. tities can be difficultt o detect using event-study methodology. The problem 
The general result is that, given a successful takeover, the abnormal returns is that regulatory changes are often debated in the political arena over time 
of the targets are large and positive and the abnormal returns of the acquirer and any accompanying wealth effects will be incorporated gradually into 
are close to zero. Jarrell and Poulsen (1989) find that the average abnormal 



180 4. Event-Study Analysis 

the value of a corporation as the probability of the change being adopted 
increases. 

Dann and James (1982) discuss this issue in their study of the impact 
of deposit interest rate ceilings on thrift institutions. They look at changes The Capital Asset Pricing Model 
in rate ceilings, but decide not to consider a change in 1973 because it was 
due to legislative action and hence was likely to have been anticipated by the 
market. Schipper and Thompson (1983,1985) also encounter this problem 
in a study of merger-related regulations. They attempt to circumvent the 
problem of anticipated regulatory changes by identifying dates when the 
probability of a regulatory change increases or decreases. However, they 
find largely insignificant results, leaving open the possibility that the absence 
of distinct event dates accounts for the lack of wealth effects. 

Much has been learned from the body of research that uses event-study 
methodology. Most generally, event studies have shown that, as we would ONE OF THE IMPORTANT PROBLEMS of modern financial economics is the 
expect in a rational marketplace, prices do respond to new information. We quantification of the tradeoff between risk and expected return. Although 
expect that event studies will continue to be a valuable and widely used tool common sense suggests that risky investments such as the stock market will 
in economics and finance. generally yield higher returns than investments free of risk, it was only with 

the development of the Capital Asset Pricing Model (CAPM) that economists 
were able to quantify risk and the reward for bearing it. The CAPM implies 
that the expected return of an asset must be linearly related to the covariance 
of its return with the return of the market portfolio. In this chapter we 

4.1 Show that when using the market model to measure abnormal returns, discuss the econometric analysis of this model. 
the sample abnormal returns from equation (4.4.7) are asymptotically inde- The chapter is organized as follows. In Section 5.1 we briefly review 
pendent as the length of the estimation window (4)in creases to infinity. the CAPM. Section 5.2 presents some results from efficient-set mathemat- 

ics, including those that are important for understanding the intuition of 
4.2 You are given the following information for an event. Abnormal re- 

econometric tests of the CAPM. The methodology for estimation and testing 
turns are sampled at an interval of one day. The event-window length is 

is presented in Section 5.3. Some tests are based on large-sample statistical 
three days. The mean abnormal return over the event window is 0.3% per 

theory making the size of the test an issue, as we discuss in Section 5.4. Sec- 
day. You have a sample of 50 event observations. The abnormal returns are 

tion 5.5 considers the power of the tests, and Section 5.6 considers testing 
independent across the event observations as well as across event days for a 

with weaker distributional assumptions. Implementation issues are covered 
given event observation. For 25 of the event observations the daily standard 

in Section 5.7, and Section 5.8 considers alternative approaches to testing 
deviation of the abnormal return is 3% and for the remaining 25 observa- 

based on cross-sectional regressions. 
tions the daily standard deviation is 6%. Given this information, what would 
be the power of the test for an event study using the cumulative abnormal 
return test statistic in equation (4.4.22)? What would be the power using the 
standardized cumulative abnormal return test statistic in equation (4.4.24)? 5.1 Review of the CAPM 
For the power calculations, assume the standard deviation of the abnormal 

Markowitz (1959) laid the groundwork for the CAPM. In this seminal re- 
returns is known. 

search, he cast the investor's portfolio selection problem in terms of ex- 
4.3 What would be the answers to question 4.2 if the mean abnormal return pected return and variance of return. He argued that investors would opti- 
is 0.6% per day for the 25 firms with the larger standard deviation? mally hold a mean-variance efficient portfolio, that is, a portfolio with the 

highest expected return for a given level of variance. Sharpe (1964) and 
Lintner (196513) built on Markowitz's work to develop economy-wide im- 
plications. They showed that if investors have homogeneous expectations 



182 5. The Capital Asset Pricing Model 5.1. Review of the CAPM 183 

and optimally hold mean-variance efficient portfolios then, in the absence other uncorrelated portfolio would have the same expected return, but a 
of market frictions, the portfolio of all invested wealth, or the market port- higher variance.) Since it is wealth in real terms that is relevant, for the 
folio, will itself be a mean-variance efficient portfolio. The usual CAPM Black model, returns are generally stated on an inflation-adjusted basis and 
equation is a direct implication of the mean-variance efficiency of the mar- Bim is defined in terms of real returns, 
ket portfolio. 

The Sharpe and Lintner derivations of the CAPM assume the existence 
of lending and borrowing at a riskfree rate of interest. For this version of 
the CAPM we have for the expected return of asset i, Econometric analysis of the Black version of the CAPM treats the zero-beta 

portfolio return as an unobserved quantity, making the analysis more com- 
plicated than that of the Sharpe-Lintner version. The Black version can be 
tested as a restriction on the real-return market model. For the real-return 
market model we have 

where R,,, is the return on the market portfolio, and Rf is the return on the E[R,I = a i m  + BimE[Rml> (5.1.7) 
riskfree asset. The Sharpe-Lintner version can be most compactly expressed 
in terms of returns in excess of this riskfree rate or in terms of excess returns. and the implication of the Black version is 
Let Zi represent the return on the ith asset in excess of the riskfree rate, 
Z, = R, - Rf.T hen for the Sharpe-Lintner CAPM we have 

In words, the Black model restricts the asset-specific intercept of the real- 
return market model to be equal to the expected zero-beta portfolio return 

Cov[Zi, Zml 
B i m  = times one minus the asset's beta. 

Var[ZI ' The CAPM is a single-period model; hence (5.1.3) and (5.1.5) do not 
have a time dimension. For econometric analysis of the model, it is necessary 

where Zm is the excess return on the market portfolio of assets. Because the 
to add an assumption concerning the time-series behavior of returns and es- 

riskfree rate is treated as being nonstochastic, equations (5.1.2) and (5.1.4) 
timate the model over time. We assume that returns are independently and 

are equivalent. In empirical implementations, proxies for the riskfree rate 
identically distributed (IID) through time and jointly multivariate normal. 

are stochastic and thus the betas can differ. Most empirical work relating to 
This assumption applies to excess returns for the Sharpe-Lintner version 

the Sharpe-Lintner version employs excess returns and thus uses (5.1.4). 
and to real returns for the Black version. While the assumption is strong, 

Empirical tests of the Sharpe-Lintner CAPM have focused on three im- 
it has the benefit of being theoretically consistent with the CAPM holding 

plications of (5.1.3): (1) The intercept is zero; (2) Beta completely captures 
period by period; it is also a good empirical approximation for a monthly 

the cross-sectional variation of expected excess returns; and (3) The market 
observation interval. We will discuss relaxing this assumption in Section 5.6. 

risk premium, E[Z,] is positive. In much of this chapter we will focus on 
The CAPM can be useful for applications requiring a measure of ex- 

the first implication; the last two implications will be considered later, in pected stock returns. Some applications include cost of capital estimation, 
Section 5.8. 

portfolio performance evaluation, and event-study analysis. As an example, 
In the absence of a riskfree asset, Black (1972) derived a more general we briefly discuss its use for estimating the cost of capital. The cost of equity 

version of the CAPM. In this version, known as the Black version, the ex- capital is required for use in corporate capital budgeting decisions and in 
pected return of asset i in excess of the zero-beta return is linearly related the determination of a fair rate of return for regulated utilities. Implemen- 
to its beta. Specifically, for the expected return of asset i, E[&], we have tation of the model requires three inputs: the stock's beta, the market risk 

premium, and the riskfree return. The usual estimator of beta of the equity 
is the OLS estimator of the slope coefficient in the excess-return market 

R,,,is  the return on the market portfolio, and 8, is the return on the zero- model, that is, the beta in the regression equation 
beta portfolio associated with m. This portfolio is defined to be the portfolio 
that has the minimum variance of all portfolios uncorrelated with m. (Any 



184 5. The Capital Asset Pricing Model 5.2. Results from Ef$cient-Set Mathematics 185 

where i denotes the asset and t denotes the time period, t = 1 ,  . . . , T .  Z,, where L is a conforming vector of ones and 61 and 6* are Lagrange multipli- 
and Z,, are the realized excess returns in time period t for asset i and the ers. Differentiating L with respect to w and setting the result equal to zero, 
market portfolio, respectively. Typically the Standard and Poor's 500 Index we have 
serves as a proxy for the market portfolio, and the US Treasury bill rate 2 f l w  -alp - S2.4 = 0 .  (5.2.5) 
proxies for the riskfree return. The equation is most commonly estimated Combining (5.2.5)w ith (5.2.2)a nd (5.2.3)w e find the solution 
using 5 years of monthly data ( T  = 60) .  Given an estimate of the beta, the 
cost of capital is calculated using a historical average for the excess return 
on the S&P 500 over Treasury bills. This sort of application is only justified 
if the CAPM provides a good description of the data. where g and h are ( N x 1 )  vectors, 

5.2 Results from Efficient-Set Mathematics 

In this section we review the mathematics of mean-variance efficient sets. 
The interested reader is referred to Merton (1972) and Roll (1977) for 
detailed treatments. An understanding of this topic is useful for interpret- and A = L ' W ' ~B, =  p ' W 1 p ,C  = L ' W I L ,a nd D = BC - A*. 
ing much of the empirical research relating to the CAPM, because the key Next we summarize a number of results from efficient-set mathematics 
testable implication of the CAPM is that the market portfolio of risky assets for minimum-variance portfolios. These results follow from the form of the 
is a mean-variance efficient portfolio. Efficient-set mathematics also plays a solution for the minimum-variance portfolio weights in (5 .2 .6) .  
role in the analysis of multifactor pricing models in Chapter 6 .  

We start with some notation. Let there be N risky assets with mean Result 1: The minimum-variance frontier can be generated from any two 

vector p and covariance matrix f l .  Assume that the expected returns of at distinct minimum-variance portfolios. 

least two assets differ and that the covariance matrix is of full rank. Define Result 1': Any portfolio of minimum-variance portfolios is also a minimum- 

waa s the ( N  x 1 )  vector of portfolio weights for an arbitrary portfolio a with variance portfolio. 

weights summing to unity. Portfolio a has mean return pa = w a r pa nd Result 2: Let p and r be any two minimum-variance portfolios. The covari- 

variance a: = wafflw,. The covariance between any two portfolios a and ance of the return of p with the return of r is 

b is walf lwb.G iven the population of assets we next consider minimum- 
variance portfolios in the absence of a riskfree asset. 

De$nitim Portfolio p is the minimum-variance portfolio of all portfolios with mean 
Result 3: Define portfolio g as the global minimum-variance portfolio. For 

return pp if its portfolio weight vector is the solution to the following constrained 
portfolio g ,  we have 

optimization: 
min w'f lw 

W 

subject to 

To solve this problem, we form the Lagrangian function L, differentiate with 
Result 4: For each minimum-variance portfolio p, except the global mini- 

respect to w,  set the resulting equations to zero, and then solve for W .  For 
mum-variance portfolio g, there exists a unique minimum-variance port- 

the Lagrangian function we have 
folio that has zero covariance with p. This portfolio is called the zero- 
beta portfolio with respect to p. 



186 5. The Capital Asset Pricing Model 5.2. Results @om Efficient-Set Mathematics 187 

Result 4': The covariance of the return of the global minimum-variance 
portfolio g with any asset or portfolio of assets a is 

Figure 5.1 illustrates the set of minimum-variance portfolios in the 
absence of a riskfree asset in mean-standard deviation space. Minimum- 
variance portfolios with an expected return greater than or equal to the 
expected return of the global minimum-variance portfolio are efficient port- 
folios. These portfolios have the highest expected return of all portfolios 
with an equal or lower variance of return. In Figure 5.1 the minimum- 
variance portfolio is g .  Portfolio p is an efficient portfolio. Portfolio is 
the zerb-beta portfolio with respect to p. It can be shown that it plots in 
the location shown in Figure 5.1, that is, the expected return on the zero- 
beta portfolio is the expected return on portfolio p, less the slope of the 
minimum-variance frontier at p times the standard deviation of portfolio p. 

Result 5: Consider a multiple regression of the return on any asset or port- 
folio R, on the return of any minimum-variance portfolio 4 (except for 
the global minimum-variance portfolio) and the return of its associated 
zero-beta portfolio 4 .  

R, = B o + B i % p + B ~ f $ + ~ p  Figure 5.1. Minimum-Van'ance Portfolios Without Riskfree Asset 

EkP I 4 ,4 1 = 0. 

For the regression coefficients we have Given a riskfree asset with return Rf the minimum-variance portfolio with 
expected return pp will be the solution to the constrained optimization 

Cov[%, Rpl 
8 2  = = Pap min w'nw 

W 
up' 

subject to 
Cov[%, %pl 

B1 = --  1 - w ' p  + (1 - w ' L ) R ~=  pp. 
Pap (5.2.21) 

0; As in the prior problem, we form the Lagrangian function L, differentiate 

Bo = 0 it with respect to w ,  set the resulting equations to zero, and then solve for 
w .  For the Lagrangian function we have 

where Papi s the beta of asset a with respect to portfolio p. L = w'nw + 6 (wp - w'p  - (1 - w ' L ) R ~ ) .  (5.2.22) 
Result 5': For the expected return of a we have 

Differentiating L with respect to w and setting the result equal to zero, we 
have 

PR = (1 - Pap)pC+Lq BPa p~p. 
2 a w  - S ( p -  R p )  = 0. (5.2.23) 

We next introduce a riskfree asset into the analysis and consider portfb- Combining (5.2.23) with (5.2.21) we have 
lios composed of a combination of the N risky assets and the riskfree asset. 
With a riskfree asset the portfolio weights of the risky assets are not con- 
strained to sum to 1, since (1 - W'L) can be invested in the riskfree asset. 



188 5. The Capital Asset Pricing Model 5.3. Statistical Framework for Estimation and Testing 

Note that we can express wp as a scalar which depends on the mean of p 
times a portfolio weight vector which does not depend on p, 

where 
( I p-  Rf) (5.2.26) 

cp = 
(p-  R ~ L ) ~ S ~-- R' (p~)  

Thus with a riskfree asset all minimum-variance portfolios are a combination 
of a given risky asset portfolio with weights proportional to i3 and the riskfree 
asset. This portfolio of risky assets is called the tangency portfolio and has 
weight vector 

We use the subscript q to identify the tangency portfolio. Equation (5.2.28) 
divides the elements of 5 by their sum to get a vector whose elements sum 
to one, that is, a portfolio weight vector. Figure 5.2 illustrates the set of 
minimum-variance portfolios in the presence of a riskfree asset. With a 
riskfree asset all efficient portfolios lie along the line from the riskfree asset 

Figure 5.2. Minimum-Variance Portfolios With Riskfree Asset 
through portfolio q. 

The expected excess return per unit risk is useful to provide a basis for 
economic interpretation of tests of the CAPM. The Sharpe ratio measures 

5.3.1 Sharpe-Lintner Version 
this quantity. For any asset or portfolio a, the Sharpe ratio is defined as the 
mean excess return divided by the standard deviation of return, Define Zt as an (Nx 1) vector of excess returns for N assets (or portfolios 

of assets). For these N assets, the excess returns can be described using the 
excess-return market model: 

In Figure 5.2 the Sharpe ratio is the slope of the line from the riskfree return 
(Rf, 0) to the portfolio (I,, a,). The tangency portfolio q can be character- 
ized as the portfolio with the maximum Sharpe ratio of all portfolios of risky 
assets. Testing the mean-variance efficiency of a given portfolio is equivalent 
to testing whether the Sharpe ratio of that portfolio is the maximum of the 
set of Sharpe ratios of all possible portfolios. 

P is the ( N x l )  vector of betas, Zmt is the time period t market portfolio 
5.3 Statistical Framework for Estimation and Testing excess return, and a and et  are (Nx 1) vectors of asset return intercepts and 

disturbances, respectively. As will be the case throughout this chapter we 
have suppressed the dependence of a, P, and E ,  on the market portfolio or 

Initially we use the assumption that investors can borrow and lend at a 
its proxy. For convenience, with the Sharpe-Lintner version, we redefine p 

riskfree rate of return, and we consider the Sharpe-Lintner version of the 
to refer to the expected excess return. 

CAPM. Then, we eliminate this assumption and analyze the Black version. 



190 5. The Capital Asset Pricing Model 5.3. Statistical Framework f w  Estimation and Testing 191 

The implication of the Sharpe-Lintner version of the CAPM for (5.3.1) The maximum likelihood estimators are the values of the parameters which 
is that all of the elements of the vector a are zero. This implication follows maximize C. To find these estimators, we differentiate C with respect to a, 
from comparing the unconditional expectation of (5.3.1) to (5.1.3) and P, and C, and set the resulting equations to zero. The partial derivatives 
forms the principal hypothesis for tests of the model. If all elements of a are 
are zero then m is the tangency portfolio. 

We use the maximum likelihood approach to develop estimators of 
the unconstrained model. Ordinary least squares (OLS) regressions asset 
by asset lead to the same estimators for a and P. To start, we consider 
the probability density function (pdf) of excess returns conditional on the 
excess return of the market. Given the assumed joint normality of excess 
returns for the pdf of Zt,w e have 

f(Z, I z,,) = (2n ) -+ /x l -+  

x exp [-$(z, - ~ - ~ Z ~ ~ ) ' C - ' ( Z , - ~ -, ~ Z(5.~3.,6)) ]  

and since excess returns are temporally IID, given T observations, the joint 
probability density function is 

Setting (5.3.10), (5.3.1I ) ,  and (5.3.12) to zero, we can solve for the maxi- 
mum likelihood estimators. These are 

x exp [-$(z, - a - ~ Z ~ ~ ) ' C - '-(  Za, -  ,Bz,~)]. (5.3.8) 

Given (5.3.8) and the excess-return observations, the parameters of where 
the excess-return market model can be estimated using maximum likelihood. 
This approach is desirable because, given certain regularity conditions, 
maximum likelihood estimators are consistent, asymptotically efficient, and 
asymptotically normal. To define the maximum likelihood estimator, we 
form the log-likelihood function, that is, the logarithm of the joint probability As already noted, these are just the formulas for OLS estimators of the 
density function viewed as a function of the unknown parameters, a, 0,a nd parameters. 
C. Denoting C as the log-likelihood function we have: The distributions of the maximum likelihood estimators conditional on 

the excess return of the market, L1G,2 ,. . . , LTf,oll ow from the assumed 
N T  T joint normality of excess returns and the IID assumption. The variances and 

( a , )  = -- log(2n) - - log 1x1 
2 2 covariances of the estimators can be derived using the inverse of the Fisher 

information matrix. As discussed in the Appendix, the Fisher information 
matrix is minus the expectation of the second order derivative of the log- 
likelihood function with respect to the vector of the parameters. 



192 5. The Capital Asset Pricing Model 5.3. Statistical Framework for Estimation and Testing 193 

The conditional distributions are However, in this case we need not resort to large-sample distribution the- 
ory to draw inferences using a Wald-type test. The finitesample distribution, 
which is developed in MacKinlay (1987)  and Gibbons, Ross, and Shanken 
(1989) ,c an be determined by applying the following theorem presented in 
Muirhead (1983): 

Theorem Let the m-vector x be distributed N ( 0 ,  O ) ,  bt  the ( m xm ) matrix A be 
distributed W m ( n ,O ) w ith ( n  2 m), and let x a n d A  be independent. Then: 

where fimi s as previously defined and 

To apply this theorem we set x = [ 1 +  fii/&:l-1/2&, A = ~ km ,=  N ,  
and n = ( T-  2) .  Then defining J1 as the test statistic we have: 

The notation W N ( T-  2, X) indicates that the ( N x N )  matrix ~2 has a 
Wishart distribution with ( T  - 2)  degrees of freedom and covariance ma- 
trix C. This distribution is a multivariate generalization of the chi-square 
distribution. Anderson (1984) and Muirhead (1983) provide discussions of Under the null hypothesis, J is unconditionally distributed central F with 
its properties. N degrees of freedom in the numerator and ( T-  N - 1) degrees of freedom 

The covariance of & and p is in the denominator. 
We can construct the Wald test JQ and the finite-sample F-test J1 using 

only the estimators from the unconkiiained model, that is, the excess-return 
market model. To consider a third test, the likelihood ratio test, we need 

2 is independent of both & and p. the estimators of the constrained model. For the constrained model, the 
Using the unconstrained estimators, we can form a Wald test statistic of Sharpe-Lintner CAPM, the estimators follow from solving for P and E from 

the null hypothesis, (5.3.11) and (5.3.12)w ith a constrained to be zero. The constrained esti- 
Ho: a = 0 (5 .3 .20)  mators are 

against the alternative hypothesis, 

HA: (Y # 0 .  (5 .3 .21 )  

The Wald test statistic is 

Jo = &'[~ar[h]]- l& 
The distributions of the constrained estimators under the null hypothesis 

( 5 . 3 . 2 2 )  are 

where we have substituted from (5.3.16)f orVar[&]. Under the null hypoth- 
esis Jo will have a chi-square distribution with N degrees of freedom. Since 
E is unknown, to use Jo for testing Ho,  we substitute a consistent estimator T%* - W N ( T-  1, C). (5.3.27) 
for C in (5.3.22) and then asymptotically the null distribution will be chi-  

Given both the unconstrained and constrained maximum likelihood esti- 
square with N degrees of freedom. The maximum likelihood estimator of 
E mators, we can test the restrictions implied by the Sharpe-Lintner version 

can serve as a consistent estimator. 



5. The Capital Asset Pricing Model 5.3. Statistical Frameworkfm Estimation and Testing IS! 

using the likelihood ratio test. This test is based on the logarithm of the like- estimators can be expressed in terms of the unconstrained estimators. Fol 
lihood ratio, which is the value of the constrained log-likelihood function B* we have 
minus the unconstrained log-likelihood function evaluated at the maximum 
likelihood estimators. Denoting LR as the log-likelihood ratio, we have 

and for e*w e have 

where L* represents the constrained log-likelihood function. To derive 
(5.3.28) we have used the fact that summation in the last term in both 
the unconstrained and constrained likelihood function evaluated at the 
maximum likelihood estimators simplifies to N T .  We now show this for 
the unconstrained function. For the summation of the last term in (5.3.9), 
evaluated at the maximum likelihood estimators, we have 

Noting that 

we have 

Taking the determinant of both sides we have 

The step from (5.3.29) to (5.3.30) uses the result that trace AB = trace BA, 
and the step to (5.3.31) uses the result that the trace of a sum is equal to the 
sum of a trace. In (5.3.32) we use the result that the trace of the identity where to go from (5.3.37) to (5.3.38) we factorize 2 and use the result that 
matrix is equal to its dimension. 1 + xx'l = (I + X'X) for the identity matrix I and a vector x. Substituting 

The test is based on the asymptotic result that, under the null hypothesis, (5.3.38) into (5.3.28) gives 
-2 times the logarithm of the likelihood ratio is distributed chi-square with 
degrees of freedom equal to the number of restrictions under Ho. That is, 
we can test Ho using 

and for J we have 

J = ( T - N -  1) 
N (exp [$- I1) 

Interestingly, here we need not resort to large-sample theory to con- which is a monotonic transformation of J2,  This shows that J can be inter- 
uct a likelihood ratio test. J1 in (5.3.23) is itself a likelihood ratio test preted as a likelihood ratio test. 
atistic. This result, which we next develop, follows from the fact that Jl is Since the finite-sample distribution of J is known, equation (K  2 An\ 

monotonic transformation of J2. The constrained maximum likelihood can also be used to deriv~th e C-L 



196 5. The Capital Asset Pricing Model 5.3. Statistical Framework for Estimation and Testing 197 

see, under the null hypothesis the finite-sample distribution of J.L can differ model. Define RIa s an ( N x1 ) vector of real returns for N assets (or port- 
from its large-sample distribution. Jobson and Korkie (1982) suggest an folios of assets). For these N assets, the real-return market model is 
adjustment to J2 which has better finitesample properties. Defining J3 as 
the modified statistic, we have 

We will visit the issue of the finite-sample properties of k and J3 in Sec- 
tion 5.4. 

A useful economic interpretation can be made of the test statistic J ,kJ is the ( N x1 ) vector of asset betas, ati s the time period t market port- 
using results from efficient-set mathematics. Gibbons, Ross, and Shanken folio return, and a and are ( N x1 )  vectors of asset return intercepts and 
(1989)s how that disturbances, respectively. 

The testable implication of the Black version is apparent from compar- 
ing the unconditional expectation of (5.3.44) with (5.3.43).T he implication 
is 

a = ( L - P ) y .  (5.3.49) 

This implication is more complicated to test than the zero-intercept restric- 
where the portfolio denoted by q represents the ex post tangency portfolio tion of the Sharpe-Lintner version because the parameters P and y enter 
constructed as in (5.2.28) from the N included assets plus the market port- in a nonlinear fashion. 
folio. Recall from Section 5.2 that the portfolio with the maximum squared Given the IID assumption and the joint normality of returns, the Black 
Sharpe ratio of all portfolios is the tangency portfolio. Thus when ex Post version of the CAPM can be estimated and tested using the maximum like- 
the market portfolio is the tangency portfolio J1 will be equal to zero, and lihood approach. The maximum likelihood estimators of the unrestricted 
as the squared Sharpe ratio of the market decreases, J1 will increase, indi- model, that is, the real-return market model in (5.3.44),a re identical to the 
cating stronger evidence against the efficiency of the market portfolio. In estimators of the excess-return market model except that real returns are 
Section 5.7.2 we present an empirical example using J after considering substituted for excess returns. Thus 6, for example, is now the vector of 
the Black version of the CAPM in the next section. sample mean real returns. For the maximum likelihood estimators of the 

parameters we have 
5.3.2 Black Version 

In the absence of a riskfree asset we consider the Black version of the CAPM 
in (5.1.5). The expected return on the zero-beta portfolio E[%,] is treated 
as an unobservable and hence becomes an unknown model parameter. 
Defining the zero-beta portfolio expected return as y ,  the Black version 
is 

where 

With the Black model, the unconstrained model is the real-return market 



198 5. The Capital Asset Pricing Model 5.3. Statistical Framework for Estimation and Testing 199 

Conditional on the real return of the market, K 1h,  2. ., .  , K T ,th e distri- Setting (5.3.59),( 5.3.59),a nd (5.3.60) to zero, we can solve for the maxi- 

butions are mum likelihood estimators. These are: 

where 
Equations (5.3.61),( 5.3.62),a nd (5.3.63)d o not allow us to solve explicitly 
for the maximum likelihood estimators. The maximum likelihood estima- 
tors can be obtained, given initial consistent estimators of 0 and E, by 

The covariance of ti and f i is iterating over (5.3.61),( 5.3.62),a nd (5.3.63) until convergence. The un- 
A 

constrained estimators p and X can serve as the initial consistent estimators 
of p and C, respectively. 

Given both the constrained and unconstrained maximum likelihood 
estimators, we can construct an asymptotic likelihood ratio test of the null 

For the constrained model, that is, the Black version of the CAPM. the hypothesis.' The null and alternative hypotheses are 
log-likelihood function is 

N T  T 
C ( y ,0 ,X ) = -- log(2n) - - log 1x1 

2 2 
A likelihood ratio test can be constructed in a manner analogous to the test 
constructed for the Sharpe-Lintner version in (5.3.33).  Defining J4 as the 
test statistic, we have 

Differentiating with respect to y ,  P, and X, we have 
Notice that the degrees of freedom of the null distribution is N - 1 .  Relative 
to the Sharpe-Lintner version of the model, the Black version loses one de- 
gree of freedom because the zero-beta expected return is a free parameter. 
In addition to the N ( N-  1 ) / 2p arameters in the residual covariance matrix, 
the unconstrained model has 2 N parameters, N parameters comprising the 
vector a and N comprising the vector 0. The constrained model has, in 
addition to the same number of covariance matrix parameters, N parame- 
ters comprising the vector and the parameter for the expected zero-beta 
portfolio return y .  Thus the unconstrained model has ( N  - 1 )  more free 
parameters than the constrained model. 

'1n the context of the Black version of the CAPM, Gibbons (1982) first developed this test. 
Shanken (198513) provides detailed analysis. 



200 5. The Capital Asset Pricing Model 5.3. Statistical Framework for Estimation and Testing 

We can also adjust Jq to improve the finite-sample properties. Defining Constraining (Y to be zero, the constrained estimators are 
J5 as the adjusted test statistic we have 

In finite samples, the null distribution of Js will more closely match the chi- 
square distribution. (See Section 5.4 for a comparison in the context of the 
Sharpe-Lintner version.) 

There are two drawbacks to the methods we have just discussed. First, 
the estimation is somewhat tedious since one must iterate over the first-order and the value of the constrained likelihood function is 
conditions. Second, the test is based on largesample theory and can have N T  T ): N T  
very poor finite-sample properties. We can use the results of Kandel (1984) L*(Y) = -- log(%) - - log II: (y)J-  -. 

2 2 2 
and Shanken (1986) to overcome these drawbacks. These authors show how 
to calculate exact maximum likelihood estimators and how to implement Note that the constrained function does depend on y. Forming the loga- 
an approximate test with good finite-sample performance. rithm of the likelihood ratio we have 

For the unconstrained model, consider the market model expressed in 
terms of returns in excess of the expected zero-beta return y: 

The value of y that minimizes the value of the logarithm of the likeli- 
Assume y is known. Then the maximum likelihood estimators for the un- hood ratio will be the value which maximizes the constrained log-likelihood 
constrained model are function and thus is the maximum likelihood estimator of y. 

Using the same development as for the Sharpe-Lintner version, the 
log-likelihood ratio can be simplified to 

&(Y) = fi - y~ - b(b,,,-  y), (5.3.69) 

and 

Minimizing L R  with respect to y is equivalent to maximizing G  where 

The unconstrained estimators of P and I: do not depend on the value of y 
but, as indicated, the estimator of a does. The value of the unconstrained 
log-likelihood function evaluated at the maximum likelihood estimators is (5.3.78) 

Thus the value of y which maximizes G  will be the maximum likelihood 
estimator. There are two solutions of a G / ay  = 0, and these are the real 
roots of the quadratic equation 

which does not depend on y.  



202 5. The Capital Asset Pricing Model 5 .4 .  Size of Tests 203 

where This estimator can be evaluated at the maximum likelihood estimates, and 
then inferences concerning the value of y are possible given the asymptotic 
normality of f * . 

5.4 Size of Tests 

In some econometric models there are no analytical results on the finite- 
sample properties of estimators. In such cases, it is common to rely on large- 
sample statistics to draw inferences. This reliance opens up the possibility 
that the size of the test will be incorrect if the sample size is not large enough 
for the asymptotic results to provide a good approximation. Because there 
is no standard sample size for which large-sample theory can be applied, it 

If A is greater than zero, the maximum likelihood estimator ?* is the small- 
est root, and if A is less than zero, then ?* is good practice to investigate the appropriateness of the theory. 

is the largest root. A will be The multivariate F-test we have developed provides an ideal framework 
greater than zero if f i ,  is greater than the mean return on the sample for illustrating the problems that can arise if one relies on asymptotic distri- 
global minimum-variance portfolio; that is, the market portfolio is on the bution theory for inference. Using the known finite-sample distribution of 
efficient part of the constrained mean-variance frontier. We can substitute the F-test statistic J,w e can calculate the finite-sample size for the various 
f *  into (5.3.62) and (5.3.63) to obtain fi* and k*w ithout resorting to an asymptotic tests. Such calculations are possible because the asymptotic test 
iterative procedure. statistics are monotonic transformations of J1. 

We can construct an approximate test of the Black version using returns We draw on the relations of J to the large-sample test statistics. Com- 
in excess of y as in (5.3.68).I f y is known then the same methodology used paring equations (5.3.22)a nd (5.3.23)f or Jo we have 
to construct the Sharpe-Lintner version F-test in (5.3.23)a pplies to testing 
the null hypothesis that the zero-beta excess-return market-model intercept 
is zero. The test statistic is 

Recall in (5.3.40)f or J2 we have 

( T - N - 1 )  
J =  

N (exp [$-I I )  9 

Because y is unknown, the test in (5.3.80)c annot be directly implemented. 
But an approximate test can be implemented with Js(?*) .  Because y = f *  and for J3 from (5.4.2)a nd (5.3.41),  
minimizes the log-likelihood ratio, it minimizes J s ( y ) .  Hence Js(?*)  5 ( T - N - I )  
J6(yo),w here yo is the unknown true value of y.  Therefore a test using J = 

N ('"P [&I - 1 )  . 
J6(?*) will accept too often. If the null hypothesis is rejected using ?*  it will 
be rejected for any value of yo. This testing approach can provide a useful Under the null hypothesis, Jo, J 2 ,  and J3 are all asymptotically distributed 
check because the usual asymptotic likelihood ratio test in (5.3.77)h as been chi-square with N degrees of freedom. The exact null distribution of J 
found to reject too often. is central F with N degrees of freedom in the numerator and T - N - 1 

Finally, we consider inferences for the expected zero-beta portfolio re- degrees of freedom in the denominator. 
turn. Given the maximum likelihood estimator of y ,  we require its asymp We calculate the exact size of a test based on a given largesample statistic 
totic variance to make inferences. Using the Fisher information matrix, the and its asymptotic 5% critical value. For example, consider a test using Jo 
asymptotic variance of the maximum likelihood of y is with 10 portfolios and 60 months of data. In this case, under the null 

hypothesis Jo is asymptotically distributed as a chi-square random variate 
with 10 degrees of freedom. Given this distribution, the critical value for a 
test with an asymptotic size of 5% is 18.31. From (5.4.1) this value of 18.31 



204 5. The Capital Asset Pricing Model 5.5. Power of Tests 205 

for Jo corresponds to a critical value of 1.495 for J. Given that the exact Table 5.1. Finite-sample size of tests of the Sharpe-Lintner CAPM using large-sample test 
null distribution of J1 is F with 10 degrees of freedom in the numerator and statistics. 
49 degrees of freedom in the denominator, a test using this critical value for 
J has a size of 17.0%. Thus, the asymptotic 5% test has a size of 17.0% in 
a sample of 60 months; it rejects the null hypothesis more than three times 
too often. 

Table 5.1 presents this calculation for Jo,J 2 ,  and J3 using 10,20, and 40 
for values of N and using 60, 120, 180, 240, and 360 for values of T. It is 
apparent that the finite-sample size of the tests is larger than the asymptotic 
size of 5%. Thus the large-sample tests will reject the null hypothesis too 
often. This problem is severe for the asymptotic tests based on Jo and J2. 
When N = 10 the problem is mostly important for the low values of T. 
For example, the finite-sample size of a test with an asymptotic size of 5% 
is 17.0% and 9.6% for Jo and J 2 ,  respectively. As N increases the severity of 
the problem increases. When N = 40 and T = 60 the finitesample size of 
an asymptotic 5% test is 98.5% for Jo and 80.5% for J2.  In these cases, the 
null hypothesis will be rejected most of the time even when it is true. With 
N = 40, the size of a 5% asymptotic test is still overstated considerably even 
when N = 360. 

The asymptotic test with a finitesample adjustment based on J3 per- 
forms much better in finite samples than does its unadjusted counterpart. 
Only in the case of N = 40 and T = 60 is the exact size significantly over- 
stated. This shows that finite-sample adjustments of asymptotic test statistics 
can play an important role. 

The exact finite-sample size is presented for tests with a size of 5% asymptotically. The finite- 
sample size uses the distribution of Jl and the relation between J1 and the largesample test 
statistics, Jo, h,a nd Js. N is the number of dependent portfolios, and T is the number of 

5.5 Power of Tests time-series observations. 

When drawing inferences using a given test statistic it is important to con- 
sider its power. The power is the probability that the null hypothesis will should be representative, and it is convenient to document since the exact 
be rejected given that an alternative hypothesis is true. Low power against finite-sample distribution of J is known under both the null and alternative 
an interesting alternative suggests that the test is not useful to discriminate hypotheses. Conditional on the excess return of the market portfolio, for 
between the alternative and the null hypothesis. On the other hand, if the the distribution of J as defined in (5.3.23), we have 
power is high, then the test can be very informative but it may also reject 
the null hypothesis against alternatives that are close to the null in eco- 
nomic terms. In this case a rejection may be due to small, economically 
unimportant deviations from the null. where 6 is the noncentrality parameter of the F distribution and 

To document the power of a test, it is necessary to specify the alternative 
data-generating process and the size of the test. The power for a given size 
of test is the probability that the test statistic is greater than the critical value 
under the null hypothesis, given that the alternative hypothesis is true. 

TO specify the distribution of J1 under both the null and the alternative 
To illustrate the power of tests of the CAPM, we will focus on the test of 

the Sharpe-Lintner version using J1 hypotheses, we need to specify 6, N, and T. 
from (5.3.23). The power of this test 



206 5. The Capital Asset Pricing Model 5.5. Power of Tests 

Under the null hypothesis a is zero, so in this case 6 is zero and we have Table 5.2. Power of 1;-test of Shaqe-Lintner CAPM using statistic J . 
the previous result that the distribution is central F with N and T - N - 1 
degrees of freedom in the numerator and denominator, respectively. Under N = l  N = 5  N = 1 0  N = 2 0  N = 4 0  
the alternative hypothesis, to specify 6 we need to condition on a value of 
fik/8; and speciQ the value of a'CplaF.o r the value of fi;/8;, given a Alternative 1: wq = 8.5% aq=  16% 
monthly observation interval, we choose 0.013 which corresponds to an ex T = 60 0.117 0.075 0.065 0.059 0.053 
post annualized mean excess return of 8% and a sample annualized standard T = 120 0.191 0.106 0.086 0.072 0.062 
deviation of 20%. T = 240 0.341 0.178 0.134 0.103 0.082 

For the quadratic term a'C-'a,r ather than separately specifying a and T = 360 0.480 0.259 0.190 0.139 0.105 
C,w e can use the following result of Gibbons, Ross, and Shanken (1989).* Alternative 2: pq=  10.2% aq=  16% 
Recalling that q is the tangency portfolio and that m is the market portfolio, 
we have 

Alternative 3: pq=  11.6% aq=  16% 
Using this relation, we need only specify the difference in the squared 
Sharpe ratio for the tangency portfolio and the market portfolio. The tan- 
gency portfolio is for the universe composed of the N included portfolios 
and the market portfolio. We consider four sets of values for the tangency 
portfolio parameters. For all cases the annualized standard deviation of the Alternative 4: pq = 13.0% aq = 16% 
tangency portfolio is set to 16%. The annualized expected excess return T = 60 0.334 0.167 0.121 0.089 0.065 
then takes on four values, 8.5%, 10.2%, 11.676, and 13.0%. Using an an- T = 120 0.593 0.332 0.237 0.163 0.110 
nualized expected excess return of 8% for the market and an annualized T = 240 0.873 0.647 0.502 0.356 0.234 
standard deviation of 20% for the market's excess return, these four values T = 360 0.965 0.845 0.726 0.563 0.389 
correspond to values of 0.01,0.02,0.03, and 0.04 for 6 /  T. 

We consider five values for N: 1, 5, 10, 20, and 40. For T we consider The alternative hypothesis is characterized by the value of the expected excess return and 
four values-60, 120, 240, and 360-which are chosen to correspond to 5, the value of the standard deviation of the tangency portfolio. The tangency portfolio is with 
10,20,a nd 30 years of monthly data. The power is tabulated for a test with respect to the N included portfolios and the market portfolio. pq is the expected excess return 

of the tangency portfolio, and aqi s the annualized standard deviation of the excess return of 
a size of 5%. The results are presented in Table 5.2. the tangency portfolio. The market portfolio is assumed to have an expected excess return of 

Substantial variation in the power of the test for different experimental 8.0%a nd a standard deviation of 20%. Under the null hypothesis the market portfolio is the 
designs and alternatives is apparent in Table 5.2. For a fixed value of N, tangency portfolio. N is the number of portfolios included in the test and 7' is the number of 

considerable increases in power are possible with larger values of T. For months of data included. 

example, under alternative 2 for N equal to 10, the power increases from 
0.082 to 0.380 as T increases from 60 to 360. 

The power gain is substantial when N is reduced for a fixed alternative. 
For example, under alternative 3, for T equal to 120, the power increases 
from 0.093 to 0.475 as N decreases from 40 to 1. However, such gains would rate at which the Sharpe ratio of the tangency portfolio declines as assets 
not be feasible in practice. As N is reduced, the Sharpe ratio of the tangency are grouped together. 
portfolio (and the noncentrality parameter of the F distribution) will decline While we do not have general results about the optimal design of a mul- 
unless the portfolios are combined in proportion to their weightings in that tivariate test, we can draw some insights from this power analysis. Increasing 
portfolio. The choice of N which maximizes the power will depend on the the length of the time series can lead to a significant payoff in terms of power. 

Further, the power is very sensitive to the value of N. The analysis suggests 
'We discuss this result further in Chapter 6. that the value of N should be kept small, perhaps no larger than about ten. 



5. The Capital Asset Pricing Model 5.6. Nonnormal and Non-ZZD Returns 

5.6 Nonnormal and Non-IID Returns we have 

In this section we are concerned with inferences when there are deviations 
from the assumption that returns are jointly normal and IID through time. 
We consider tests which accommodate non-normality, heteroskedasticity, The GMM estimator e is chosen to minimize the quadratic form 
and temporal dependence of returns. Such tests are of interest for two rea- 
sons. First, while the normality assumption is sufficient, it is not necessary to 
derive the CAPM as a theoretical model. Rather, the normality assumption where W is a positive definite (2Nx 2N) weighting matrix. Since in this case 
is adopted for statistical purposes. Without this assumption, finite-sample we have 2N moment condition equations and 2N unknown parameters, the 
properties of asset pricing model tests are difficult to derive. Second, depar- system is exactly identified and e can be chosen to set the average of the 
tures of monthly security returns from normality have been d~cumented .~  sample moments gT(8)  equal to zero. The GMM estimator will not depend 
There is also abundant evidence of heteroskedasticity and temporal depen- A 

dence in stock  return^.^ Even though temporal dependence makes the on W since QT(8) will attain its minimum of zero for any weighting matrix. 

CAPM unlikely to hold as an exact theoretical model, it is still of interest to The estimators from this GMM procedure are equivalent to the maximum 

examine the empirical performance of the model. It is therefore desirable likelihood estimators in (5.3.13) and (5.3.14). The estimators are 

to consider the effects of relaxing these statistical assumptions. 
Robust tests of the CAPM can be constructed using a Generalized 

Method of Moments (GMM) framework. We focus on tests of the Sharpe- 
Lintner version; however, robust tests of the Blackversion can be constructed 
in the same manner. Within the GMM framework, the distribution of returns 
conditional on the market return can be both serially dependent and con- The importance of the GMM approach for this application is that a 
ditionally heteroskedastic. We need only assume that excess asset returns robust covariance matrix of the estimators can be formed. The variances 
are stationary and ergodic with finite fourth moments. The subsequent of & and p will differ from the variances in the maximum likelihood ap- 
analysis draws on Section A.2 of the Appendix which contains a general proach. The covariance matrix of the GMM estimator 0 follows from equa- 
development of the GMM methodology. We continue with a sample of T tion (A.2.8) in the Appendix. It is 
time-series observations and N assets. Following the Appendix, we need to 
set up the vector of moment conditions with zero expectation. The required 
moment conditions follow from the excess-return market model. The resid- where 
ual vector provides N moment conditions, and the product of the excess 
return of the market and the residual vector provides another N moment 
conditions. Using the notation of the Appendix, for f t ( 8 )  we have 

and 

where hi = [ l  Zmt],e t = Zt - a - ,f3 Zmt,a nd 8' = [a'@ 'I. 
The specification of the excess-return market model implies the mo- The asymptotic distribution of 6 is normal. Thus we have 

ment condition E[ft(OO)]=  0, where O0 is the true parameter vector. This 
moment condition forms the basis for estimation and testing using a GMM 
approach. GMM chooses the estimator so that linear combinations of the 
sample average of this moment condition are zero. For the sample average, The application of the distributional result in (5.6.9) requires consistent 

estimators of Do and So since they are unknown. In this case, for Do we have 
%ee Fama (1965, l976),  Blattberg and Gonedes ( l974),  Affleck-Graves and McDonald 

(1989), and Table 1 . l  in Chapter 1. 
4 ~ eCeh apters 2 and 12, and the references given in those chapters. 



210 5. The Capital Asset Pricing Model 

Aconsistent estimator DT can easily be constructed using the maximum like- 
lihood estimators of p, and a:. To compute a consistent estimator of So,a n 
assumption is necessary to reduce the summation in (5.6.8)t o a finite num- In this section we consider issues relating to empirical implementation of 
ber of terms. Section A.3 in the Appendix discusses possible assumptions. the test methodology. A summary of empirical results, an illustrative imple- 
Defining ST as a consistent estimator of So, ( 1 /  T ) [ D $ T ' D ~ ] - 'i s a consis- mentation, and discussion of the obsemability of the market portfolio are 

tent estimator of the covariance matrix of e. Noting that h = R e  where included. 

R = (1 0 )  €3 I N ,  a robust estimator of Var(h) is (11T )R[D;S, 'D~]- 'R' .  
Using this we can construct a chi-square test of the Sharpe-Lintner model 
as in (5.3.22).T he test statistic is 5.7.1 Summaq ofEmpirica1 Evidence 

An enormous amount of literature presenting empirical evidence on the 
CAPM has evolved since the development of the model in the 1960s. The 
early evidence was largely positive, with Black,Jensen, and Scholes (1972),  

Under the null hypothesis a = 0 ,  Fama and MacBeth (1973),a nd Blume and Friend (1973) all reporting evi- 
' 

dence consistent with the mean-variance efficiency of the market portfolio. 
There was some evidence against the Sharpe-Lintner version of the CAPM 
as the estimated mean return on the zero-beta portfolio was higher than the 

I 
MacKinlay and Richardson (1991) illustrate the bias in standard CAPM riskfree return, but this could be accounted for by the Black version of the 

test statistics that can result from violations of the standard distributional model. 
assumptions. Specifically, they consider the case of contemporaneous condi- In the late 1970s less favorable evidence for the CAF'M began to appear 
tional heteroskedasticity. With contemporaneous conditional heteroskedas- in the so-called anomalies literature. In the context of the tests discussed in 
ticity, the variance of the market-model residuals of equation (5.3.3) de- this chapter, the anomalies can be thought of as firm characteristics which 
pends on the contemporaneous market return. In their example, the as- can be used to group assets together so that the tangency portfolio of the 
sumption that excess returns are IID andjointly multivariate Student t leads included portfolios has a high expost Sharpe ratio relative to the Sharpe ratio 

1 
to conditional heteroskedasticity. The multivariate Student t assumption of the market proxy. Alternatively, contrary to the prediction of the CAE'M, 
for excess returns can be motivated both empirically and theoretically. One the firm characteristics provide explanatory power for the cross section of 
empirical stylized fact from the distribution of returns literature is that re- sample mean returns beyond the beta of the CAPM. 
turns have fatter tails and are more peaked than one would expect from a Early anomalies included the priceearnings-ratio effect and the size 
normal distribution. This is consistent with returns coming from a multi- effect. Basu (1977) first reported the priceearnings-ratio effect. Basu's 
variate Student t. Further, the multivariate Student t is a return distribution finding is that the market portfolio appears not to be mean-variance efficient 

for which mean-variance analysis is consistent with expected utility maxi- relative to portfolios formed on the basis of the priceearnings ratios of 
mization, making the choice theoretically appealing.5 firms. Firms with low priceearnings ratios have higher sample returns, and 

The bias in the size of the standard CAPM test for the Student t case firms with high priceearnings ratios have lower mean returns than would 

depends on the Sharpe ratio of the market portfolio and the degrees of be the case if the market portfolio was mean-variance efficient. The size 

freedom of the Student t. MacKinlay and Richardson (1991) present some effect, which was first documented by Banz (1981),  is the result that low 

estimates of the potential bias for various Sharpe ratios and for Student t market capitalization firms have higher sample mean returns than would 
be expected if the market portfolio was mean-variance efficient. These two 

degrees of freedom equal to 5 and 10. They find that in general the bias is 
anomalies are at least partially related, as the low priceearnings-ratio firms 

small, but if the Sharpe ratio is high and the degrees of freedom small, the 
tend to be small. 

bias can be substantial and lead to incorrect inferences. Calculation of the 
A number of other anomalies have been discovered more recently. 

test statistic J7 based on the GMM framework provides a simple check for the Fama and French (1992, 1993) find that beta cannot explain the differ- 
possibility that the rejection of the model is the result of heteroskedasticity ence in return between portfolios formed on the basis of the ratio of book 
in the data. value of equity to market value of equity. Firms with high book-market ra- 

tios have higher average returns than is predicted by the CAPM. Similarly, 



212 5. The Capital Asset Pricing Mohl  5.7. Implementation of Tests 213 

DeBondt and Thaler (1985) and Jegadeesh and Titman (1993) find that from (5.3.41),a nd J7 from (5.6.11 ). The tests are conducted using a thirty- 
a portfolio formed by buying stocks whose value has declined in the past year sample of monthly returns on ten portfolios. Stocks listed on the New 
(losers) and selling stocks whose value has risen in the past (winners) has a York Stock Exchange and on the American Stock Exchange are allocated to 
higher average return than the CAPM predicts. Fama (1991) provides a the portfolios based on the market value of equity and are value-weighted 
good discussion of these and other anomalies. within the portfolios. The CRSP value-weighted index is used as a proxy 

Although the results in the anomalies literature may signal economically for the market portfolio, and the one-month US Treasury bill return is used 
important deviations from the CAPM, there is little theoretical motivation for the riskfree return. The sample extends from January 1965 through 
for the firm characteristics studied in this literature. This opens up the December 1994. 
possibility that the evidence against the CAPM is overstated because of data- Tests are conducted for the overall period, three ten-year subperiods, 
snooping and sample selection biases. We briefly discuss these possibilities. and six five-year subperiods. The subperiods are also used to form overall 

Datasnooping biases refer to the biases in statistical inference that result aggregate test statistics by assuming that the subperiod statistics are indepen- 
from using information from data to guide subsequent research with the dent. The aggregate statistics for J2 ,  k,an d J7 are the sum of the individual 
same or related data. These biases are almost impossible to avoid due to statistics. The distribution of the sum under the null hypothesis will be chi- 
the nonexperimental nature of economics. We do not have the luxury of square with degrees of freedom equal to the number of subperiods times 
running another experiment to create a new data set. Lo and MacKinlay the degrees of freedom for each subperiod. The aggregate statistic for J1 
(1990b) illustrate the potential magnitude of data-snooping biases in a test of is calculated by scaling and summing the F statistics. The scale factor is 
the Sharpe-Lintner version of the CAPM. They consider the case where the calculated by approximating the F distribution with a scaled chi-square dis- 
characteristic used to group stocks into portfolios (e.g. size or priceearnings tribution. The approximation matches the first two moments. The degrees 
ratio) is selected not from theory but from previous observations of mean of freedom of the null distribution of the scaled sum of the subperiod J's 
stock returns using related data. Comparisons of the null distribution of the is the number of subperiods times the degrees of freedom of the chi-square 
test statistic with and without data-snooping suggests that the magnitude of approximation. 
the biases can be immense. However, in practice, it is difficult to specify the The empirical results are reported in Table 5.3. The results present 
adjustment that should be made for data-snooping. Thus, the main message evidence against the Sharpe-Lintner CAPM. Using J1,t he pvalue for the 
is a warning that the biases should at least be considered as a potential overall thirty-year period is 0.020, indicating that the null hypothesis is re- 
explanation for model deviations. jected at the 5% significance level. The five- and ten-year subperiod results 

Sample selection biases can arise when data availability leads to certain suggest that the strongest evidence against the restrictions imposed by the 
subsets of stocks being excluded from the analysis. For example, Kothari, model is in the first ten years of the sample from January 1965 to December 
Shanken, and Sloan (1995) argue that data requirements for studies looking 1974. 
at book-market ratios lead to failing stocks being excluded and a resulting Comparisons of the results across test statistics reveal that in finite sam- 
survivorship bias. Since the failing stocks would be expected to have low ples inferences can differ. A comparison of the results for J1 versus J2 
returns and high book-market ratios, the average return of the included high illustrates the previously discussed fact that the asymptotic likelihood ratio 
book-market-ratio stocks would have an upward bias. Kothari, Shanken, and test tends to reject too often. The finite-sample adjustment to J2 works well 
Sloan (1995) argue that this bias is largely responsible for the previously cited as inferences with J3 are almost identical to those with Jl. 

result of Fama and French (1992, 1993). However, the importance of this 
particular survivorship bias is not fully resolved as Fama and French (1996b) 
dispute the conclusions of Kothari, Shanken, and Sloan. In any event, i t  is 5.7.3 Unobsmability of the Market Portfolio 
clear that researchers should be aware of the potential problems that can In the precedinganalysis, we have not addressed the problem that the return 
arise from sample selection biases. on the market portfolio is unobserved and a proxy is used in the tests. Most 

tests use a value- or equal-weighted basket of NYSE and AMEX stocks as the 
market proxy, whereas theoretically the market portfolio contains all assets. 

5.7.2 Illustrative Implementation Roll (1977) emphasizes that tests of the CAPM really only reject the mean- 

We present tests of the Sharpe-Lintner model to illustrate the testing method- variance efficiency of the proxy and that the model might not be rejected if 
-'--J WPconsider four test statistics: J from (5.3.23), & from (5.3.33),&  the return on the true market portfolio were used. Several approaches have 

 



5. The Capital Asset Pricing Model 5.8. Cross-Sectional Regressions 

A second approach to the problem is presented by Kandel and St 
Table 5.3. Empirical resultsfor tests ofthe Sharpe-Lintner venion ofthe CAPiCI. baugh (1987) and Shanken (1987a). Their papers estimate an upper hot 

Time , pvalue ,k pvalue J pvalue J7 pvalue on the correlation between the market proxy return and the true mar 
return necessary to overturn the rejection of the CAPM. The basic findin; 

Fiveyear subperiods that if the correlation between the proxy and the true market exceeds abc 
1/65-12/69 2.038 0.049 20.867 0.022 18.432 0.048 22.105 0.015 0.70, then the rejection of the CAPM with a market proxy would also iml 

1/70-12/74 2.136 0.039 21.712 0.017 19.179 0.038 21.397 0.018 the rejection of the CAPM with the true market portfolio. Thus, as long 
we believe there is a high correlation between the true market return at 

1/75-12/79 1.914 0.066 19.784 0.031 17.476 0.064 27.922 0.002 
the proxies used, the rejections remain intact. 

1/80-12/84 1.224 0.300 13.378 0.203 11.818 0.297 13.066 0.220 
1/85-12/89 1.732 0.100 18.164 0.052 16.045 0.098 16.915 0.076 
1/90-12/94 1.153 0.344 12.680 0.242 11.200 0.342 12.379 0.260 5.8 Cross-Sectional Regressions 

Overall 77.224 0.004 106.586 ** 94.151 0.003 113.785 ** 
So far in this chapter we have focused on the mean-variance efficiency o 

Tenyear subperiods the market portfolio. Another view of the CAPM is that it implies a lin 
1/65-12/74 2.400 0.013 23.883 0.008 22.490 0.013 24.649 0.006 ear relation between expected returns and market betas which completel~ 
1/75-12/84 2.248 0.020 22.503 0.013 21.190 0.020 27.192 0.002 explain the cross section of expected returns. These implications can be 
1/85-12/94 1.900 0.053 19.281 0.037 18.157 0.052 16.373 0.089 tested using a cross-sectional regression methodology. 

Overall 57.690 0.001 65.667 ** 61.837 0.001 68.215 ** Fama and MacBeth (1973) first developed the cross-sectional regression 
approach. The basic idea is, for each cross section, to project the returns on 

Thirtyyear period the betas and then aggregate the estimates in the time dimension. Assuming 
1/65-12/94 2.159 0.020 21.612 0.017 21.192 0.020 22.176 0.014 that the betas are known, the regression model for the t t h  cross section of 

N assets is 
**Less than 0.0005. Z = ~ 0 t L ~f  l t P n rfl l ,  (5.8.1) 

where Zt is the ( N x1 ) vector of excess asset returns for time period t ,  L is an 
Results are for ten value-weighted portfolios ( N  = 10) with stocks assigned to the portfolios 
based on market value of equity. The CRSP value-weighted index is used as a measure of the ( N x1 ) vector of ones, and p, is the ( N x I )  vector of CAPM betas. 
market portfolio and a one-month Treasury bill is used as a measure of the riskfree rate. The Implementation of the Fama-MacBeth approach involves two steps. 
tests are based on monthly data from January 1965 to December 1994. First, given T periods of data, (5.8.1) is estimated using OLS for each t ,  

t = 1, . . . , T, giving the T estimates of yo, and yll. Then in the second 
step, the time series of Pot's and lilt's are analyzed. Defining yo = E[yot] 
and yl = E [ y l l ] ,t he implications of the Sharpe-Lintner CAPM are yo = 0 

been suggested to consider if inferences are sensitive to the use of a proxy (zero intercept) and yl > 0 (positive market risk premium). Because the 
in place of the market portfolio. returns are normally distributed and temporally IID, the gammas will also 

One approach is advanced in Stambaugh (1982). He examines the be normally distributed and IID. Hence, given time series of yol and ylt, 
sensitivity of tests to the exclusion of assets by considering a number of t = 1, . . . , T, we can test these implications using the usual t-test. Defining 
broader proxies for the market p o r t f o l i o . ~ esh ows that inferences are w(?,) as the t-statistic, we have 
similar whether one uses a stock-based proxy, a stock- and bond-based proxy, 
or a stock-, bond-, and realestate-based proxy. This suggests that inferences 
are not sensitive to the error in the proxy when viewed as a measure of the 
market portfolio and thus Roll's concern is not an empirical problem. 

where 

"elated work considers the possibility of accounting for the return on human capital. See 
Mayers (1972),C ampbell (1996a),a nd Jagannathan and Wang (1996). 



216 5. The Capital Asset Pricing Model 5.9. Conclusion 217 

and - result of the fact that empirical work is forced to work with proxies for the 
market portfolio. Kandel and Stambaugh (1995) show that this extreme 
sensitivity can potentially be mitigated by using a generalized-least-squares 
(GLS) estimation approach in place of ordinary least squares. However their 

The distribution of w ( f i )  is Student t with (T-1) degrees of freedom and result depends on knowing the true covariance matrix of returns. The gains 
asymptotically is standard normal. Given the test statistics, inferences can from using GLS with an estimated covariance matrix are as yet uncertain. 
be made in the usual fashion. 

The Fama-MacBeth approach is particularly useful because it can easily 
be modified to accommodate additional risk measures beyond the CAPM 5.9 Conclusion 
beta. By adding additional risk measures, we can examine the hypothe- 
sis that beta completely describes the cross-sectional variation in expected In this chapter we have concentrated on the classical approach to testing 
returns. For example, we can consider if firm size has explanatory power the unconditional CAPM. Other lines of research are also of interest. One 
for the cross-section of expected returns where firm size is defined as the important topic is the extension of the framework to test conditional versions 
logarithm of the market value of equity. Defining ct as the ( N x 1 ) vector of the CAPM, in which the model holds conditional on state variables that 
with elements corresponding to firm size at the beginning of period t, we describe the state of the economy. This is useful because the CAPM can hold 
can augment (5.8.1) to investigate if firm size has explanatory power not conditionally, period by period, and yet not hold unconditionally. Chapter 8 
captured by the market beta: discusses the circumstances under which the conditional CAPM might hold 

in a dynamic equilibrium setting, and Chapter 12 discusses econometric 
methods for testing the conditional CAPM. 

Using the f 2 t ' ~f rom (5.8.5), we can test the hypothesis that size does not Another important subject is Bayesian analysis of mean-variance effi- 
have any explanatory power beyond beta, that is, ~2 = 0, by setting j = 2 in ciency and the CAPM. Bayesian analysis allows the introduction of prior 
(5.8.2)-(5.8.4). information and addresses some of the shortcomings of the classical ap- 

The Fama-MacBeth methodology, while useful, does have several prob- proach such as the stark dichotomy between acceptance and rejection of 
lems. First, it cannot be directly applied because the market betas are not the model. Harvey and Zhou (1990), Kandel, McCulloch, and Stambaugh 
known. Thus the regressions are conducted using betas estimated from the (1995), and Shanken (1987~a)r e examples of work with this perspective. 
data, which introduces an errors-in-variables complication. The errors-in- We have shown that there is some statistical evidence against the CAPM 
variables problem can be addressed in two ways. One approach, adopted in the past 30 years of US stock-market data. Despite this evidence, the 
by Fama and MacBeth, is to minimize the errors-in-variables problem by CAPM remains a widely used tool in finance. There is controversy about 
grouping the stocks into portfolios and increasing the precision of the how the evidence against the model should be interpreted. Some authors 
beta estimates. A second approach, developed by Litzenberger and Ra- argue that the CAPM should be replaced by multifactor models with several 
maswamy (1979) and refined by Shanken (1992b), is to explicitly adjust sources of risk; others argue that the evidence against the CAPM is overstated 
the standard errors to correct for the biases introduced by the errors-in- because of mismeasurement of the market portfolio, improper neglect of 
variables. Shanken suggests multiplying 3; in (5.8.4) by an adjustment conditioning information, data-snooping, or sample-selection bias; and yet 

factor (1 + ( f i ,  others claim that no risk-based model can explain the anomalies of stock- 
- fo)2/3:). While this approach eliminates the errors-in- 

market behavior. In the next chapter we explore multifactor asset pricing 
variables bias in the t-statistic in (5.8.2), it does not eliminate the possibility 

models and then return to this debate in Section 6.6. 
that other variables might enter spuriously in (5.8.5) as a result of the un- 
observability of the true betas. 

The unobservability of the market portfolio is also a potential problem 
for the cross-sectional regression approach. Roll and Ross (1994) show that 
if the true market portfolio is efficient, the cross-sectional relation between 
expected returns and betas can be very sensitive to even small deviations of 5.1 Result 5 states that for a multiple regression of the return on any asset 
the market portfolio proxy from the true market portfolio. Thus evidence or portfolio %, on the return of any minimum-variance portfolio Rp (except 
of the lack of a relation between expected return and beta could be the for the global minimum-variance portfolio) and the return of its associated 



218 5. The Capital Asset Pricing Model 

zero-beta portfolio 4,& =  B o + B ~ & ~ + B ~ & + E ~ t,h e regression coefficients 
are B2 = pap, = 1 - pap ,  and Bo = 0. Show this. 

5.2 Show that the intercept of the excess-return market model, a, is zero 
if the market portfolio is the tangency portfolio. Multifactor Pricing Models 
5.3 Using monthly returns from the 10-year period January 1985 to De- 
cember 1994 for three individual stocks of your choice, a value-weighted 
market index, and a Treasury bill with one month to maturity, perform the 
following tests of the Sharpe-Lintner Capital Asset Pricing Model. 

5.3.1 Using the entire 10-year sample, regress excess returns of each 
stock on the excess (value-weighted) market return, and perform tests 
with a size of 5% that the intercept is zero. Report the point estimates, 
t-statistics, and whether or not you reject the CAPM. Perform regression 
diagnostics to check your specification. AT THE END OF CHAPTER 5 we summarized empirical evidence indicating 

that the CAPM beta does not completely explain the cross section of ex- 
5.3.2 For each stock, perform the same test over each of the two equi- pected asset returns. This evidence suggests that one or more additional 
partitioned subsamples and report the point estimates, t-statistics, and factors may be required to characterize the behavior of expected returns and 
whether or not you reject the CAPM in each subperiod. Also include the naturally leads to consideration of multifactor pricing models. Theoretical 
same diagnostics as above. arguments also suggest that more than one factor is required, since only 

under strong assumptions will the CAPM apply period by period. Two main 
5.3.3 Combine all three stocks into a single equal-weighted portfolio 

theoretical approaches exist. The Arbitrage Pricing Theory (APT) devel- 
and re-do the tests for the entire sample and for each of the two subsam- oped by Ross (1976) is based on arbitrage arguments and the Intertemporal 
ples, and report the point estimates, t-statistics, and whether or not you Capital Asset Pricing Model (ICAPM) developed by Merton (1973a) is based 
reject the CAPM for the whole sample and in each subsample. Include on equilibrium arguments. In this chapter we will consider the econometric 
diagnostics. analysis of multifactor models. 
5.3.4 Jointly test that the intercepts for all three stocks are zero using the The chapter proceeds as follows. Section 6.1 briefly discusses the the- 
F-test statistic J1 in (5.3.23) for the whole sample and for each subsample. oretical background of the multifactor approaches. In Section 6.2 we con- 

sider estimation and testing of the models with known factors, while in 
5.4 Derive the Gibbons, Ross, and Shanken result in equation (5.5.3). Section 6.3 we develop estimators for risk premia and expected returns. 

Since the factors are not always provided by theory, we discuss ways to con- 
struct them in Section 6.4. Section 6.5 presents empirical results. Because 
of the lack of specificity of the models, deviations can always be explained 
by additional factors. This raises an issue of interpreting model violations 
which we discuss in Section 6.6. 

6.1 Theoretical Background 

The Arbitrage Pricing Theory (APT) was introduced by Ross (1976) as an 
alternative to the Capital Asset Pricing Model. The APT can be more gen- 
eral than the CAPM in that it allows for multiple risk factors. Also, unlike 
the CAPM, the APT does not require the identification of the market port- 
folio. However, this generality is not without costs. In its most general form 



6. Multifactor Pricing Models 6.1. Theoretical Background 22 1 

let L represent a conforming vector of ones. The relation in (6.1.7) is a p  
the APT provides an approximate relation for expected asset returns with 

proximate as a finite number of assets can be arbitrarily mispriced. Because 
an unknown number of unidentified factors. At this level rejection of the (6.1.7) is only an approximation, it does not produce directly testable restric- 
theory is impossible (unless arbitrage opportunities exist) and as a conse- 

tions for asset returns. TO obtain restrictions we need to impose additional 
quence testability of the model depends on the introduction of additional 

structure so that the approximation becomes exact. 
assumptions.' 

Connor (1984) presents a competitive equilibrium version of the APT 
The Arbitrage Pricing Theory assumes that markets are competitive and 

which has exact factor pricing as a feature. In Connor's model the additional 
frictionless and that the return generating process for asset returns being 

requirements are that the market portfolio be welldiversified and that the 
considered is 

factors be pervasive. The market portfolio will be welldiversified if no single 
asset in the economy accounts for a significant proportion of aggregate 
wealth. The requirement that the factors be pervasive permits investors to 
diversify away idiosyncratic risk without restricting their choice of factor risk 
exposure. 

Dybvig (1985) and Grinblatt and Titman (1985) take a different a p  
where R, is the return for asset i ,  ai is the intercept of the factor model, proach. They investigate the potential magnitudes of the deviations from 
bi is a (Kx 1) vector of factor sensitivities for asset i, f is a (Kxl)  vector of exact factor pricing given structure on the preferences of a representative 
common factor realizations, and ei is the disturbance term. For the system agent. Both papers conclude that given a reasonable specification of the 
of N assets, parameters of the economy, theoretical deviations from exact factor pricing 

are likely to be negligible. As a consequence empirical work based on the 
exact pricing relation is justified. 

Exact factor pricing can also be derived in an intertemporal asset pricing 
framework. The Intertemporal Capital Asset Pricing Model developed in 
Merton (1973a) combined with assumptions on the conditional distribution 
of returns delivers a multifactor model. In this model, the market portfolio 

In the system equation, R is an (Nx 1) vector with R = [R1R 2 . . . RNI1a,  is serves as one factor and state variables serve as additional factors. The 
an (Nx 1) vector with a = [a l  % . . . aNI1,B  is an (Nx K) matrix with B = additional factors arise from investors' demand to hedge uncertainty about 
[blb 2 . . . bNI1a,n d E is an (Nx 1) vector with E = [e l  € 2  . . . e N I 1W e further future investment opportunities. Breeden (1979),C ampbell (1993a, 1996), 
assume that the factors account for the common variation in asset returns and Fama (1993) explore this model, and we discuss it in Chapter 8. 
so that the disturbance term for large welldiversified portfolios van is he^.^ In this chapter, we will generally not differentiate the APT from the 
This requires that the disturbance terms be sufficiently uncorrelated across ICAPM. We will analyze models where we have exact factor pricing, that is, 
assets. 

Given this structure, Ross (1976) shows that the absence of arbitrage in 
large economies implies that 

There is some flexibility in the specification of the factors. Most empiri- 
cal implementations choose a proxy for the market portfolio as one factor. 

where p is the (Nx 1) expected return vector, A. is the model zero-beta pa- However, different techniques are available for handling the additional fac- 

rameter and is equal to the riskfree return if such an asset exists, and XK tors. We will consider several cases. In one case, the factors of the APT and 
the state variables of the ICAPM need not be traded portfolios. In other 

is a (K x 1) vector of factor risk premia. Here, and throughout the chapter, 
cases the factors are returns on portfolios. These factor portfolios are called 
mimicking portfolios because jointly they are maximally correlated with the 

 h here has been substantial debate on the testability of the APT. Shanken (1982) and factors. Exact factor pricing will hold with such portfolios. Huberman and 
Dybvig and Ross (1985) provide one interesting exchange. Dhrymes, Friend, Gultekin, and 
Gultekin (1984) also question the empirical relevance of the model. Kandel (1987) and Breeden (1979) discuss this issue in the context of the 

' ~ l a r  APT and ICAPM, respectively. 
of order mF

e welldiversified portfolio is a portfolio with alarge number of stockswith weightings 
.  



222 6. Multqactor Pricing Models 6.2. Estimation and Testing 225 

6.2 Estimation and Testing to the large sample distribution."he large sample distribution of J under 
the null hypothesis will be chi-square with the degrees of freedom equal to 

In this section we consider the estimation and testing of various forms of the the number of restrictions imposed by the null hypothesis. 
exact factor pricing relation. The starting point for the econometric analysis 
of the model is an assumption about the time-series behavior of returns. 

6.2.1 Portfolios as Factors with a Riskfree Asset 
We will assume that returns conditional on the factor realizations are IID 
through time and jointly multivariate normal. This is a strong assumption, We first consider the case where the factors are traded portfolios and there 
but it does allow for limited dependence in returns through the timeseries exists a riskfree asset. The unconstrained model will be a K-factor model 
behavior of the factors. Furthermore, this assumption can be relaxed by expressed in excess returns. Define Z ,  as an ( N x1 ) vector of excess returns 
casting the estimation and testing problem in a Generalized Method of for N  assets (or portfolios of assets). For excess returns, the K-factor linear 
Moments framework as outlined in the Appendix. The GMM approach for model is: 
multifactor models is just a generalization of the GMM approach to testing Zt = a +  B Z K ~ + E ~  (6.2.2) 
the CAPM presented in Chapter 5. 

As previously mentioned, the multifactor models specify neither the 
number of factors nor the identification of the factors. Thus to estimate and 
test the model we need to determine the factors-an issue we will address in 
Section 6.4. In this section we will proceed by taking the number of factors 
and their identification as given. 

We consider four versions of the exact factor pricing model: ( I )  Fac- 
tors are portfolios of traded assets and a riskfree asset exists; (2) Factors are 
portfolios of traded assets and there is not a riskfree asset; (3) Factors are B is the ( N x  K) matrix of factor sensitivities, Z K 1i s the ( K x 1 ) vector of factor 
not portfolios of traded assets; and (4) Factors are portfolios of traded assets portfolio excess returns, and a and E~ are ( N x1 ) vectors of asset return in- 
and the factor portfolios span the mean-variance frontier of risky assets. We tercepts and disturbances, respectively. X is the variance-covariance matrix 
use maximum likelihood estimation to handle all four cases. See Shanken of the disturbances, and OKi s the variancecovariance matrix of the factor 
(1992b) for a treatment of the same four cases using a cross-sectional re- portfolio excess returns, while 0 is a (K x N )  matrix of zeroes. Exact factor 
gression approach. pricing implies that the elements of the vector a in (6.2.2) will be zero. 

Given the joint normality assumption for the returns conditional on the For the unconstrained model in (6.2.2) the maximum likelihood esti- 
factors, we can construct a test of any of the four cases using the likelihood mators are just the OLS estimators: 
ratio. Since derivation of the test statistic parallels the derivation of the 
likelihood ratio test of the CAPM presented in Chapter 5, we will not repeat 
it here. The likelihood ratio test statistic for all cases takes the same general 
form. Defining J as the test statistic we have 

- * 
where k and E are the maximum likelihood estimators of the residual where 
covariance matrix for the unconstrained model and constrained model, , l T  1 
respectively. 7' is the number of time-series observations, N is the number p = - X Z ,  and fi, = - - C Z ~ , .  

T 
of included portfolios, and K is the number of factors. As discussed in t= 1 T t= 1 

Chapter 5, the statistic has been scaled by (T-  $ ! - K  - 1 )  rather than the 
usual T to improve the convergence of the finite-sample null distribution %ee equation (5.3.41)a nd Jobson and Korkie (1982). 



224 6. Multifactor Pricing Models 6.2. Estimation and Testing 225 

For the constrained model, with a constrained to be zero, the maximum E[RK~I=  PK, E [ ( R ~-t  PK) ( R ~-t  PK)'] = a~ (6.2.17) 
likelihood estimators are 

Cov[RKt,E :] = 0. (6.2.18) 

B is the (NxK ) matrix of factor sensitivities, R K i~s  the (Kx 1) vector of 
factor portfolio real returns, and a and are (Nx1 ) vectors of asset return 
intercepts and disturbances, respectively. 0 is a (Kx N) matrix of zeroes. 

For the unconstrained model in (6.2.14) the maximum likelihood esti- 
mators are 

a = f i -Bf i  K (6.2.19) 
The null hypothesis a equals zero can be tested using the likelihood ratio 
statistic Jin (6.2.1). Under the null hypothesis the degrees of freedom of the 
null distribution will be N since the null hypothesis imposes N restrictions. 

In this case we can also construct an exact multivariate F-test of the null 
hypothesis. Defining J1 as the test statistic we have 

where 
where A K  is the maximum likelihood estimator of flK, fi 1 1

= ---R  
and fiK = - -R~. .  

t=1 T t=1 

In the constrained model real returns enter in excess of the expected 
Under the null hypothesis, Jj is unconditionally distributed central F with N zero-beta portfolio return yo. For the constrained model, we have 
degrees of freedom in the numerator and (T - N - K) degrees of freedom 
in the denominator. This test can be very useful since it can eliminate the 
problems that can accompany the use of asymptotic distribution theory. 
Jobson and Korkie (1985) provide a derivation of J1. 

The constrained model estimators are: 
6.2.2 Portfolios as Factm without a Riskfie Asset 

In the absence of a riskfree asset, there is a zero-beta model that is a multi- 
factor equivalent of the Black version of the CAPM. In a multifactor context, 
the zero-beta portfolio is a portfolio with no sensitivity to any of the factors, 
and expected returns in excess of the zerebeta return are linearly related 
to the columns of the matrix of factor sensitivities. The factors are assumed 
to be portfolio returns in excess of the zero-beta return. 

Define Rt as an (Nx  1) vector of real returns for N assets (or portfolios 
of assets). For the unconstrained model, we have a K-factor linear model: 



226 6. Multifactor Pricing Models 6.2. Estimation and Testing 227 

The maximum likelihood estimates can be obtained by iterating over (6.2.23) B is the (Nx K) matrix of factor sensitivities, fKtis  the (K x 1) vector of factor 
to (6.2.25). B from (6.2.20) and 2 from (6.2.21) can be used as starting realizations, and a and et are (Nx 1) vectors of asset return intercepts and 
values for B and C in (6.2.25). disturbances, respectively. 0 is a (Kx N) matrix of zeroes. 

Exact maximum likelihood estimators can also be calculated without For the unconstrained model in (6.2.14) the maximum likelihood esti- 
iteration for this case. The methodology is a generalization of the approach mators are 
outlined for the Black version of the CAPM in Chapter 5; it is presented 
by Shanken (1985a). The estimator of yo is the solution of a quadratic 
equation. Given yo, the constrained maximum likelihood estimators of B 
and C follow from (6.2.23) and (6.2.24). 

The restrictions of the constrained model in (6.2.22) on the uncon- 
strained model in (6.2.14) are 

These restrictions can be tested using the likelihood ratio statistic J in 
(6.2.1). Under the null hypothesis the degrees of freedom of the null dis- 
tribution will be N-1. There is a reduction of one degree of freedom in 
comparison to the case with a riskfree asset. A degree of freedom is used where 
up in estimating the zero-beta expected return. 

For use in Section 6.3, we note that the asymptotic variance of Po evalu- fi 1 1
= $R, and fifK  

= - Z f K t .  
ated at the maximum likelihood estimators is t=1  T 

t= 1 

The constrained model is most conveniently formulated by comparing 
the unconditional expectation of (6.2.28) with (6.1.8). The unconditional 
expectation of (6.2.28) is 

6.2.3 Macroeconomic Variables as Factors 
where pfK=  E [ ~ K ~E]q.u ating the right hand sides of (6.1.8) and (6.2.36) 

Factors need not be traded portfolios of assets; in some cases proposed fac- we have 
tors include macroeconomic variables such as innovations in GNP, changes a = + B(XK-  pfK). (6.2.37) 
in bond yields, or unanticipated inflation. We now consider estimating and 
testing exact factor pricing models with such factors. Defining yo as the zero-beta parameter ho and defining rl as (XK - pfK) 

Again define Rt as an ( N x l )  vector of real returns for N assets (or where XK is the (Kxl)  vector of factor risk premia, for the constrained 
portfolios of assets). For the unconstrained model we have a K-factor linear model, we have 
model: Rt = L ~ o +1 +BB~f ~ +t e t. (6.2.38) 

Rt = a + B f ~ ~ + € t  (6.2.28) 
The constrained model estimators are 



6.  Multifactor Pricing Models 6.2.  Estimation and Testing 229 

Thus this case retains the simplicity of the first case with the riskfree asset. In 
the context of the APT, spanning occurs when two welldiversified portfolios 
are on the minimum-variance boundary. Chamberlain (1983a) provides 
discussion of this case. 

The unconstrained model will be a K-factor model expressed in real 
returns. Define Rt as an ( N x l )  vector of real returns for N assets (or 
portfolios of assets). Then for real returns we have a K-factor linear model: 

where in (6.2.41) X - [ L B * ]  and y = [yo y;]'. 
The maximum likelihood estimates can be obtained by iterating over 

(6.2.39) to (6.2.41). B from (6.2.34) and f: from (6.2.35) can be used as 
starting values for B and C in (6.2.41). 

The restrictions of (6.2.38) on (6.2.28) are 

These restrictions can be tested using the likelihood ratio statistic J in 
(6.2.1). Under the null hypothesis the degrees of freedom of the null dis- 
tribution is N - K - 1. There are N restrictions but one degree of freedom 
is lost estimating yo, and K degrees of freedom are used estimating the K 
elements of X K .  B is the (NxK ) matrix of factor sensitivities, RKti s the (Kx 1) vector of factor 

The asymptotic variance of ;/ follows from the maximum likelihood 
portfolio real returns, and a and E ,  are (Nx1 ) vectors of asset return inter- 

approach. The variance evaluated at the maximum likelihood estimators is 
cepts and disturbances, respectively. 0 is a (KxN) matrix of zeroes. The 
restrictions on (6.2.46) imposed by the included factor portfolios spanning 
the mean-variance frontier are: 

Applying the partitioned inverse rule to (6.2.43), for the variances of the 
components of 4w e have estimators a = 0 and B L  = L. (6.2.51) 

To understand the intuition behind these restrictions, we can return to 
the Black version of the CAPM from Chapter 5 and can construct a span- 
ning example. The theory underlying the model differs but empirically the 
restrictions are the same as those on a two-factor APT model with spanning. 
The unconstrained Black model can be written as 

where Rmta nd Rota re the return on the market portfolio and the associated 
zero-beta portfolio, respectively. The restrictions on the Black model are 

We will use these variance results for inferences concerning the factor risk a = 0 and Po,+P, = L as shown in Chapter 5. These restrictions correspond 
premia in Section 6.3. 

to those in (6.2.51). 
For the unconstrained model in (6.2.46) the maximum likelihood esti- 

6.2.4F actor Portfolios Spanning the Mean-Variance Frontier mators are 

When factor portfolios span the mean-variance frontier, the intercept term 
of the exact pricing relation ko is zero without the need for a riskfree asset. 



6. Multqactor Pricing Mo&k 6.3. Estimation of Risk Premia and Expected Returns 23 1 

Defining J2 as the test statistic we have 

Under the null hypothesis, J.L is unconditionally distributed central F with 
2N degrees of freedom in the numerator and 2 ( T - N - K )  degrees of free- 
dom in the denominator. Huberman and Kandel (1987) present a deriva- 
tion of this test. 

6.3 Estimation of Risk Premia and Expected Returns 

All the exact factor pricing models allow one to estimate the expected return 
To estimate the constrained model, we consider the unconstrained on a given asset. Since the expected return relation is p = + BXK,o ne 

model in (6.2.46)w ith the matrix B partitioned into an ( N x  1)c olumn vector needs measures of the factor sensitivity matrix B, the riskfree rate or the 
bl and an ( N x ( K- 1 ) )  matrix B1 and the factor portfolio vector partitioned zero-beta expected return Lo, and the factor risk premia X K .  Obtaining 
into the first row R l t  and the last ( K - 1 )  rows RK*t With this partitioning measures of B and the riskfree rate or the expected zero-beta return is 
the constraint B L = L can be written bl + B1L  = L .  For the unconstrained straightforward. For the given case the constrained maximum likelihood 
model we have estimator B* can be used for B. The observed riskfree rate is appropriate 

Rt = a + bl Rlt  + BI R K *+~  e t .  (6.2.56) for the riskfree asset or, in the cases without a riskfree asset, the maximum 
Substituting a = 0 and bl = L - B ] Li nto (6.2.56)g ives the constrained likelihood estimator Po can be used for the expected zero-beta return. 

model, Further estimation is necessary to form estimates of the factor risk pre- 
Rt - L R ~= ~B 1 (RK*t-  L R ~ ~ ) + E ~ .  (6.2.57) mia. The appropriate procedure varies across the four cases of exact factor 

pricing. In the case where the factors are the excess returns on traded port- 
Using (6.2.57) the maximum likelihood estimators are folios, the risk pre~niac an be estimated directly from the sample means of 

the excess returns on the portfolios. For this case we have 

An estimator of the variance of X K  is 

In the case where portfolios are factors but there is no riskfree asset, 
the factor risk premia can be estimated using the difference between the 
sample mean of the factor portfolios and the estimated zero-beta return: 

The null hypothesis a equals zero can be tested using the likelihood ratio 
statistic J in (6.2.1).  Under the null hypothesis the degrees of freedom of iK=  f iK - L);0 
the null distribution will be 2N since a = 0 is N restrictions and B L = L is 
N In this case, an estimator of the variance of X K  is 

additional restrictions. 
We can also construct an exact test of the null hypothesis given the linear- 

ity of the restrictions in (6.2.51)a nd the multivariate normality assumption. 



232 6. MultiJuctor Pricing Models 6.4. S~hctiono f Factors 233 

where ~%r[);t,] is from (6.2.27). The fact that @ , and fo are independent I Shanken (1992b) shows that factor risk premia car1 also be estimated 
1 

has been utilized to set the covariance term in (6.3.4) to zero. I a using a two-pass cross-sectional regression approach. In the first pass the 
In the case where the factors are not traded portfolios, an estimator of factor sensitivities are estimated asset-by-asset using OLS. These estimators 

the vector of factor risk premia AK is the sum of the estimator of the mean represent a measure of the factor loading matrix B which we denote B. This 
of the factor realizations and the estimator of yl, estimator of B will be identical to the unconstrained maximum likelihood 

estimators previously presented for jointly normal and IID residuals. 
Using this estimator of B and the (N x 1) vector of asset returns for each 

time period, the expost factor risk premia can be estimated time- period-by- 
An estimator of the variance of i~is  

where cr[r[j . l]i s from (6.2.45). Because f i f K  and are independent the 
01s;  however, GLS can 

covariance term in (6.3.6) is zero. 
also be used. The output of the regression is a time series of ex post risk 

The fourth case, where the factor portfolios span the mean-variance 
frontier, is the same as the first case except that real returns are substituted premia, X K ~ t, =  1. . . . , T ,a nd an expost measure of the zero-beta portfolio 

for excess returns. Here XK is the vector of factor portfolio sample means return, hot,t  = 1 , .  . . , T. 
and ho is zero. Common practice is then to conduct inferences about the risk premia 

For any asset the expected return can be estimated by substituting the using the means and standard deviations of these ex post series. While this 

estimates of B, ho,a nd XK into (6.1.8). Since (6.1.8) is nonlinear in the pa- approach is a reasonable approximation, Shanken (1992b) shows that the 

rameters, calculating a standard error requires using a linear approximation calculated standard errors of the means will understate the true standard 

and estimates of the covariances of the parameter estimates. errors because they do not account for the estimation error in B. Shanken 

It is also of interest to ask if the factors are jointly priced. Given the derives an adjustment which gives consistent standard errors. No adjust- 

vector of risk premia estimates and its covariance matrix, tests of the null ment is needed when a maximum likelihood approach is used, because the 

hypothesis that the factors are jointly not priced can be conducted using maximum likelihood estimators already incorporate the adjustment. 

the following test statistic: 

6.4 Selection of Factors 

Asymptotically, under the null hypothesis that XK = 0, J3 has an F distribu- The estimation and testing results in Section 6.2 assume that the identity 

tion with K and 7'-K degrees of freedom. This distributional result is an of the factors is known. In this section we address the issue of specifying 

application of the Hotelling T~ statistic and will be exact in finite samples the factors. The approaches fall into two basic categories, statistical and 

for the cases where the estimator of XK is based only on the sample means of theoretical. The statistical approaches, largely motivated by the APT, involve 

the factors. We can also test the significance of any individual factor using building factors from a comprehensive set of asset returns (usually much 
larger than the set of returns used to estimate and test the model). Sample 
data on these returns are used to construct portfolios that represent factors. 
The theoretical approaches involve specifying factors based on arguments 
that the factors capture economy-wide systematic risks. 

A 

where f i j K  is the jth element of iKan d vl; is the (j,j )th element of Var[Agl. 
Testing if individual factors are priced is sensible for cases where the factors 6.4.1 Statistical Approaches 
have been theoretically specified. With empirically derived factors, such 
tests are not useful because, as we explain in Section 6.4.1, factors are iden- Our starting point for the statistical construction of factors is the linear 
tified only up to an orthogonal transformation; hence individual factors do  factor model. We present the analysis in terms of real returns. The same 
not have clear-cut economic interpretations. analysis will apply to excess returns in cases with a riskfree asset. Recall that 



234 6. Multifactor Pricing Models 6.4. Selection of Factors 235 

for the linear model we have One interpretation of the maximum likelihood estimator of B given 
the maximum likelihood estimator of D is that the estimator of B has the 
eigenvectors of D-'V associated with the K largest eigenvalues as its columns. 
For details of the estimation the interested reader can see these papers, or 
Morrison (1990, chapter 9) and references therein. 

where Rt is the (Nxl )  vector of asset returns for time period t ,  ft is the The second step in the estimation procedure is to estimate the factors 
( K x 1 ) vector of factor realizations for time period t ,  and E, is the (Nx 1) given B and X. Since the factors are derived from the covariance structure, 
vector of model disturbances for time period t .  The number of assets, N, is the means are not specified in (6.4.1). Without loss of generality, we can 
now very large and usually much larger than the number of time periods, T. restrict the factors to have zero means and express the factor model in terms 
There are two primary statistical approaches, factor analysis and principal of deviations about the means, 
components. 

Factor Analysis 
Estimation using factor analysis involves a two-step procedure. First the Given (6.4.5), a candidate to proxy for the factor realizations for time period 
factor sensitivity matrix B and the disturbance covariance matrix X are esti- t is the cross-sectional generalized least squares (GLS) regression estimator. 
mated and then these estimates are used to construct measures of the factor Using the maximum likelihood estimators of B and D we have for each t 
realizations. For standard factor analysis it is assumed that there is a stn'ct 
factor structure. With this structure K factors account for all the cross covari- 
ance of asset returns and hence E is diagonal. (Ross imposes this structure 

Here we are estimating f t  by regressing (Rt - b) onto B. The factor real- 
in his original development of the APT.) 

Given a strict factor structure and K factors, we can express the (Nx N) ization series, f t ,  t = 1, . . . , T, can be employed to test the model using the 
covariance matrix of asset returns as the sum of two components, the varia- approach in Section 6.2.3. 
tion from the factors plus the residual variation, Since the factors are linear combinations of returns we can construct 

portfolios which are perfectly correlated with the factors. Denoting R Ka~s  
the ( K x1 ) vector of factor portfolio returns for time period t ,  we have 

where E[ft c] = 5 2 a~n d E = D to indicate it is diagonal. With the factors 
unknown, B is identified only up to an orthogonal transformation. All trans- 
forms B G are equivalent for any ( K xK ) orthogonal transformation matrix where 
G, i.e., such that G G' = I .  This rotational indeterminacy can be eliminated by w = (~fi- lf i)- lf i lf i- l ,  

restricting the factors to be orthogonal to each other and to have unit vari- 
and A is defined as a diagonal matrix with 1/ T/IS as the jth diagonal element, 

ance. In this case we have flK=  I and B is unique. With these restrictions where is the jth element of WL. 
in place, we can express the return covariance matrix as 

The factor portfolio weights obtained for the jth factor from this pro- 
cedure are equivalent to the weights that would result from solving the 
following optimization problem and then normalizing the weights to sum 

With the structure in (6.4.4) and the assumption that asset returns arejointly to one: 
normal and temporally IID, estimators of B and D can be formulated using Min W ~ wD,  (6.4.8) 

Wl 
maximum likelihood factor analysis. Because the first-order conditions for 
maximum likelihood are highly nonlinear in the parameters, solving for the subject to 
estimators with the usual iterative procedure can be slow and convergence 
difficult. Alternative algorithms have been developed by Joreskog (1967) 
and Rubin and Thayer (1982) which facilitate quick convergence to the 
maximum likelihood estimators. 



236 6. Multifactor Pricing Models 6.4. Selection ofFactors 237 

That is, the factor portfolio weights minimize the residual variance subject The first sample principal component is x;'R, where the ( N  x 1) vector 
to the constraints that each factor portfolio has a unit loading on its own x; is the solution to the following problem: 
factor and zero loadings on other factors. The resulting factor portfolio 
returns can be used in all the approaches discussed in Section 6.2. 

If B and D are known, then the factor estimators based on GLS with 
the population values of B and D will have the maximum correlation with subject to 
the population factors. This follows from the minimum-variance unbiased x;x l  = 1 .  
estimator property of generalized least squares given the assumed normality fi is the sample covariance matrix of returns. The solution x; is the eigen- 
of the disturbance vector. But in practice the factors in (6.4.6) and (6.4.7) vector associated with the largest eigenvalue of 6. To facilitate the portfolio 
need not have the maximum correlation with the population common fac- interpretation of the factors we can define the first factor as wiRl where 
tors since they are based on estimates of B and D. Lehmann and Modest wl is x; scaled by the reciprocal of L ' Xs~o  that its elements sum to one. 
(1988) present an alternative to GLS. In the presence of measurement er- The second sample principal component solves the above problem for x2 
ror, they find this alternative can produce factor portfolios with a higher in the place of xl with the additional restriction x;'x2 = 0. The solution 
population correlation with the common factors. They suggest for the jth x$ is the eigenvector associated with the second largest eigenvalue of 6. x$ 
factor to use q~w~h ere the (Nx 1) vector Lj j  is the solution to the following can be scaled by the reciprocal of L'X;g iving w2, and then the second factor 
problem: portfolio will be whR,. In general the jth factor will be wiR, where wj is the 

Min W ~ wD3 (6.4.11) 
w, rescaled eigenvector associated with the jth largest eigenvalue of 6. The 

factor portfolios derived from the first K principal components analysis can 
subject to then be employed as factors for all the tests outlined in Section 6.2. 

Another principal components approach has been developed by Con- 
nor and Korajczyk (1986, 1988).~T hey propose using the eigenvectors as- 
sociated with the K largest eigenvalues of the (Tx T) centered returns cross- 
product matrix rather than the standard approach which uses the principal 
components of the (Nx N) sample covariance matrix. They show that as the 

This approach finds the portfolio which has the minimum residual variance cross section becomes large the (Kx T) matrix with the rows consisting of 
of all portfolios orthogonal to the other (K-1) factors. Unlike the GLS the K eigenvectors of the cross-product matrix will converge to the matrix 
procedure, this procedure ignores the information in the factor loadings of of factor realizations (up to a nonsingular linear transformation reflecting 
the jth factor. It is possible that this is beneficial because of the measurement the rotational indeterminancy of factor models). The potential advantages 
error in the loadings. Indeed, Lehmann and Modest find that this method of this approach are that it allows for time-varying factor risk premia and 
of forming factor portfolios results in factors with less extreme weightings on that it is computationally convenient. Because it is typical to have a cross 
the assets and a resulting higher correlation with the underlying common section of assets much larger than the number of time-series observations, 
factors. analyzing a (Tx T) matrix can be less burdensome than working with an 

( N xN )  sample covariance matrix. 
Principal Components 
Factor analysis represents only one statistical method of forming factor port- Fator Analysis or Principal Components ? 
folios. An alternative approach is principal components analysis. Principal We have discussed two statistical primary approaches for constructing the 
components is a technique to reduce the number of variables being stud- model factors-factor analysis and principal components. Within each ap- 
ied without losing too much information in the covariance matrix. In the proach there are possible variations in the process of estimating the factors. 
present application, the objective is to reduce the dimension from N asset A question arises as to which technique is optimal in the sense of providing 
returns to K factors. The principal components serve as the factors. The the most precise measures of the population factors given a fixed sample of 
first principal component is the (normalized) linear combination of asset returns. Unfortunately the answer in finite samples is not clear although all 
returns with maximum variance. The second principal component is the Procedures can be justified in large samples. 

(normalized) linear combination of asset returns with maximum variance 
of all combinations orthogonal to the first principal component. And so on. also Mei (1993). 



238 6 .  Multifactor Pricing Models 6.4.  Selection of Factors 239 

Chamberlain and Rothschild (1983) show that consistent estimates of Ross (1980) use this approach and conclude that three or four factors are 
the factor loading matrix B can be obtained from the eigenvectors associated adequate. 
with the largest eigenvalues of ~ - ' 5 2 ,w here Y is any arbitrary positive A potential drawback of using the test from maximum likelihood factor 
definite matrix with eigenvalues bounded away from zero and infinity. Both analysis is that the constrained model assumes a strict factor structure- 
standard factor analysis and principal components fit into this category, for an assumption which is not theoretically necessary. Connor and Korajczyk 
factor analysis Y = D and for principal components Y = I. However, (1993) develop an asymptotic test (N -, COf)o r the adequacy of K factors 
the finite-sample applicability of the result is unclear since it is required that under the assumption of an approximate factor structure. Their test uses the 
both the number of assets Nand the number of time periods T go to infinity. result that with an approximate factor structure the average cross-sectional 

The Connor and Korajczyk principal components approach is also con- variation explained by the K+llst factor approaches zero as N increases, 
sistent as N increases. It has the further potential advantage that it only 
requires T > K and does not require T to increase to infinity. However, 1 

lim - bk+lbK+l = 0, 
whether in finite samples it dominates factor analysis or standard principal N - t m  N 
components is an open question. 

where the dependence of b ~ +o1n  N is implicit. This implies that in a large 
cross section generated by a K-factor model, the average residual variance 

6.4 .2  Number of Factors in a linear factor model estimated with K+l factors should converge to the 
The underlying theory of the multifactor models does not specify the num- average residual variance with K factors. This is the implication Connor and 
ber of factors that are required, that is, the value of K. While, for the theory Korajczyk test. Examining returns from stocks listed on the NewYork Stock 
to be useful, K should be reasonably small, the researcher still has signifi- Exchange and the American Stock Exchange they conclude that there are 
cant latitude in the choice. In empirical work this lack of specification has up to six pervasive factors. 
been handled in several ways. One approach is to repeat the estimation 
and testing of the model for a variety of values of K and observe if the tests 

6.4 .3  Theoretical Approaches 
are sensitive to increasing the number of factors. For example Lehmann 
and Modest (1988) present empirical results for five, ten, and fifteen fac- Theoretically based approaches for selecting factors fall into two main cat- 
tors. Their results display minimal sensitivity when the number of factors egories. One approach is to specify macroeconomic and financial market 
increases from five to ten to fifteen. Similarly Connor and Korajczyk (1988) variables that are thought to capture the systematic risks of the economy. A 
consider five and ten factors with little sensitivity to the additional five fac- second approach is to specify characteristics of firms which are likely to ex- 
tors. These results suggest that five factors are adequate. plain differential sensitivity to the systematic risks and then form portfolios 

A second approach is to test explicitly for the adequacy of K factors. of stocks based on the characteristics. 
An asymptotic likelihood ratio test of the adequacy of K factors can be con- Chen, Roll, and Ross (1986) is a good example of the first approach. 
structed using -2 times the difference of the value of the log-likelihood The authors argue that in selecting factors we should consider forces which 
function of the covariance matrix evaluated at the constrained and un- will explain changes in the discount rate used to discount future expected 
constrained estimators. Morrison (1990, p. 362) presents this test. The cash flows and forces which influence expected cash flows themselves. Based 
likelihood ratio test statistic is on intuitive analysis and empirical investigation a five-factor model is pro- 

posed. The factors include the yield spread between long and short interest 
rates for US government bonds (maturity premium), expected inflation, 
unexpected inflation, industrial production growth, and the yield spread 
between corporate high- and low-grade bonds (default premium). Aggre- 

where h is the maximum likelihood estimator of 52 and B and D are the gate consumption growth and oil prices are found not to have incremental 
maximum likelihood estimators of B and D, respectively. The leading term effects beyond the five factors5 
is an adjustment to improve the convergence of the finite-sample null dis- 
tribution to the large-sample distribution. Under the null hypothesis that 
K factors are adequate, J5 will be asymptotically distributed ( T  + CO) as a 5 ~alnter native implementation of the first approach is given by Campbell (1996a) and is 

chi-square variate with $ discussed in Chapter 8. 
[(N - K )-~ N  - K] degrees of freedom. Roll and 



240 6. Multifactor Pricing Models 

The second approach of creating factor portfolios based on firm char- Table 6.1. Summary of results f m  tests of exact factm pricing using mintercept  F-test. 
acteristics has been used in a number of studies. These characteristics have 
mostly surfaced from the literature of CAPM violations discussed in Chap Study Time period Portfolio characteristic N K pvalue 
ter 5. Characteristics which have been found to be empirically important 

CK 64:Ol-83:12 market value of equity 10 5 0.002 
include market value of equity, price-toearnings ratio, and ratio of book CK 10 10 0.002 
value of equity to market value of equity. The general finding is that factor CKJ 10 5 0.236 
models which include a broad based market portfolio (such as an equal- CKJ 10 10 0.171 
weighted index) and factor portfolios created using these characteristics do C K ~ J  10 5 0.01 1 

C K ~ J  10 10 0.019 
a good job in explaining the cross section of returns. However, because 

LM 63:01-82:12 market value of equity 5 5 ** 
the important characteristics have been identified largely through empiri- LM 5 10 ** 
cal analysis, their importance may be overstated because of data-snooping LM 5 15 ** 
biases. We will discuss this issue in Section 6.6. LM 20 5 0.11 

LM 20 10 0.14 
LM 20 15 0.42 
LM 63:Ol-82:12 dividend yield 5 5 0.17 

6.5 Empirical Results LM 5 10 0.18 
LM 5 15 0.17 
LM 20 5 0.94 

Many empirical studies of multifactor models exist. We will review four of LM 20 10 0.97 
the studies which nicely illustrate the estimation and testing methodology LM 20 15 0.98 
we have discussed. Two comprehensive studies using statistical approaches LM 63:Ol-82:12 own variance 5 5 0.29 
to select the factors are Lehmann and Modest (1988) and Connor and Ko- LM 5 10 0.57 

rajczyk (1988). Lehmann and Modest [LM] use factor analysis and Connor LM 5 15 0.55 
LM 20 5 0.83 

and Korajczyk [CK] use (Tx  T) principal cqmponents. Two studies using the LM 20 10 0.97 
theoretical approach to factor identification are Fama and French (1993) LM 20 15 0.98 
and Chen, Roll, and Ross (1986). Fama and French [FF] use firm charac- FF 63:07-91:12 stocks and bonds 32 2 0.010 
teristics to form factor portfolios and Chen, Roll, and Ross [CRR] specify FF 32 3 0.039 
macroeconomic variables as factors. The first three studies include tests of FF 32 5 0.025 

the implications of exact factor pricing, while Chen, Roll, and Ross focus **Less than 0.001. 
on whether or not the factors are priced. The evidence supporting exact 
factor pricing is mixed. Table 6.1 summarizes the main results from LM, CK refers to Connor and Korajczyk (1988), LM refers to Lehmann and Modest (1988), and 

FF refers to Fama and French (1993). The CK factors are derived using (TxT )  principal 
CK, and FF. components, the LM factors are derived using maximum likelihood factor analysis, and the FF 

A number of general points emerge from this table. The strongest factors are prespecified factor portfolios. For the FF two-factor case the factors are the return on 
evidence against exact factor pricing comes from tests using dependent a portfolio of low market value of equity firms minus a portfolio of high market value of equity 

portfolios based on market value of equity and book-to-market ratios. Even firms and the return on a portfolio of high book-to-market value firms minus a portfolio of low 
book-to-market value firms. For the three-factor case the factors are those in the two-factor case 

multifactor models have difficulty explaining the "size" effect and "book to plus the return on the CRSP value-weighted stock index. For the five-factor case the returns 
market" effect. Portfolios which are formed based on dividend yield and on a term structure factor and a default risk factor are added. CK include tests separating 
based on own variance provide little evidence against exact factor pricing. the intercept for January from the intercept for other months. CKJ are results of tests of the 

hypothesis that the January intercept is zero and C K ~aJr e results of tests of the hypothesis that 
The CK results for January and non-January months suggest that the evi- the nonjanuary intercept is zero. CK and FF work with a monthly sampling interval. LM use 
dence against exact factor pricing does not arise from the January effect. a daily interval to estimate the factors and a weekly interval for testing. The test results from 

Using the statistical approaches, CK and LM find little sensitivity to CK and LM are based on tests from four five-year periods aggregated together. The portfolio 

increasing the number of factors beyond five. On the other hand FF find characteristic represents the firm characteristic used to allocate stocks into the dependent 
portfolios. FF use 25 stock portfolios and 7 bond portfolios. The stock portfolios are created 

some improvement going from two factors to five factors. In results not using a two way sort based on market value of equity and book-value-to-market-value ratios. 
included, FF find that with stocks only three factors are necessary and that The bond portfolios include five US government bond portfolios and two corporate bond 
when bond portfolios are included then five factors are needed. These Portfolios. The government bond portfolios are created based on maturity and the corporate 

bond portfolios are created based on the level of default risk. N is the number of dependent 
portfolios and K is the number of factors. The pvalues are reported for the zereintercept I.'-test. 



242 6. Multifactor Pricing Models 6.6. Interpreting Deviations from Exact Factor Pricing 243 

results are generally consistent with direct tests for the number of factors 6.6.1 Exact Factor Pricing Models, Mean-Variance Analysis, 
discussed in Section 6.4.2. and the Optimal Orthogonal Portfolio 

The LM results display considerable sensitivity to the number of depen- For the initial analysis we drop back to the level of the primary assets in the 
dent portfolios included. The pvalues are considerably lower with fewer economy. Let N be the number of primary assets. Assume that a riskfree 
portfolios. This is most likely an issue of the power of the test. For these asset exists. Let Z, represent the (Nx 1) vector of excess returns for period 
tests with an unspecified alternative hypothesis, reducing the number of t .  Assume Zt is stationary and ergodic with mean p and covariance matrix 
portfolios without eliminating the deviations from the null hypothesis can f l  that is full rank. We also take as given a set of K factor portfolios and 
lead to substantial increases in power, because fewer restrictions must be analyze the deviations from exact factor pricing. For the factor model, as in 
tested. (6.2.2), we have 

The CRR paper focuses on the pricing of the factors. They use a cross- Zt = a+BZKt + E , .  (6.6.1) 
sectional regression methodology which is similar to the approach presented 
in Section 6.3. As previously noted they find evidence of five priced factors. Here B is the (NxK) matrix of factor loadings, ZKt is the (Kxl)  vector of 

The factors include the yield spread between long and short interest rates time-t factor portfolio excess returns, and a and et  are (Nx1 ) vectors of asset 

for US government bonds (maturity premium), expected inflation, unex- return intercepts and disturbances, respectively. The variance-covariance 

pected inflation, industrial production growth, and the yield spread between matrix of the disturbances is X and the variance-covariance matrix of the 

corporate high- and low-grade bonds (default premium). factors is f l ~ a,s in (6.2.3)-(6.2.6). The values of a, B, and C will depend 
on the factor portfolios, but this dependence is suppressed for notational 
convenience. 

If we have exact factor pricing relative to the K factors, all the elements 
of the vector a will be zero; equivalently, a linear combination of the factor 

6.6 Interpreting Deviations from Exact Factor Pricing portfolios forms the tangency portfolio (the mean-variance efficient portfo- 
lio of risky assets given the presence of a riskfree asset). Let Zqt be the excess 

We have just reviewed empirical evidence which suggests that, while multi- return of the (ex ante) tangency portfolio and let wq be the (Nx 1) vector of 
factor models do a reasonable job of describing the cross section of returns, portfolio weights. From mean-variance analysis (see Chapter 5), 
deviations from the models do exist. Given this, it is important to consider 
the possible sources of deviations from exact factor pricing. This issue is 
important because in a given finite sample it is always possible to find an ad- In the context of the K-factor model in (6.6.1), we have exact factor pric- 
ditional factor that will make the deviations vanish. However the procedure ing when the tangency portfolio in (6.6.2) can be formed from a linear 
of adding an extra factor implicitly assumes that the source of the deviations combination of the K factor portfolios. 
is a missing risk factor and does not consider other possible explanations. Now consider the case where we do not have exact factor pricing, so the 

In this section we analyze the deviations from exact factor pricing for tangency portfolio cannot be formed from a linear combination of the factor 
a given model with the objective of exploring the source of the deviations. portfolios. Our interest is in developing the relation between the deviations 
For the analysis the potential sources of deviations are categorized into from the asset pricing model, a, and the residual covariance matrix, X. TO 

two groups-risk-based and nonrisk-based. The objective is to evaluate the facilitate this, we define the optimal orth~~onalportfolwioh,i~c h is the unique 
plausibility of the argument that the deviations from the given factor model portfolio that can be combined with the K factor portfolios to form the 
can be explained by additional risk factors. tangency portfolio and is orthogonal to the factor portfolios. 

The analysis relies on an important distinction between the two cate- Dt@nition (optimal orthogonal portfolio). Take as giuen K factor portfolios which 
gories, namely, a difference in the behavior of the maximum squared Sharpe cannot be combined to fonn the tangency portfolio or the global minimum-variance 
ratio as the cross section of securities is increased. (Recall that the Sharpe portfolio. A portfolio h will be de$ned as the optimal orthogonal portfolio with respect 
ratio is the ratio of the mean excess return to the standard deviation of to these K factor portfolios if 
the excess return.) For the risk-based alternatives the maximum squared 
Sharpe ratio is bounded and for the nonrisk-based alternatives the maxi- wq, = Wpw + wh(l-  L'W) (6.6.3) 
mum squared Sharpe ratio is a less useful construct and can, in ~rinciple, 

"see Roll (1980) for general properties of orthogonal portfolios. 
be unbounded. 



6.6. Interpreting Deviations from Exact Factor Pricing 
244 6. Multifactor Pricing Models 245 

component in the residual variance to prevent the formation of a portfolio 
and 

w p w p  = 0 with a positive deviation and a residual variance that decreases to zero as the 
number of securities in the portfolio grows, that is, an asymptotic arbitrage 

for a (K x 1) vector w where Wp is the (N x K) matrix of asset weights for the factor opportunity. 
portfolios, wh is the (N x 1) vector of asset weights for the optimal orthogonalportfolio, 
and wq is the (Nx  1 )  vector of asset weights for the tangency portfolio. Ifone considers 
a model without any factor portfolios (K = 0) then the optimal orthogonal portfolio 6.6.2 Squared Sharpe Ratios 
will be the tangency portfolio. The squared Sharpe ratio is a useful construct for interpreting much of 

The weights of portfolio h can be expressed in terms of the parameters the ensuing analysis. The tangency portfolio q has the maximum squared 
of the K-factor model. The vector of weights is Sharpe measure of all portfolios. The squared Sharpe ratio of q, $, is 

wh = ( ~ ' n - ~ a ) - ' K ~ a  

= ( ~ ' ~ ~ a ) - l X ~ a ,  (6.6.5) 

where the t superscript indicates the generalized inverse. The usefulness of Given that the K factor portfolios and the optimal orthogonal portfolio h 

this portfolio comes from the fact that when added to (6.6.1) the intercept can be combined to form the tangency portfolio, the maximum squared 

will vanish and the factor loading matrix B will not be altered. The optimality Sharpe ratio of these K+l portfolios will be s:. Since h is orthogonal to the 

restriction in (6.6.3) leads to the intercept vanishing, and the orthogonality portfolios K, MacKinlay (1995) shows that one can express si as the sum 
condition in (6.6.4) leads to B being unchanged. Adding in Zht: of the squared Sharpe ratio of the orthogonal portfolio and the squared 

maximum Sharpe ratio of the factor portfolios, 

where si = P:/a; and s i  = pkCli1pK.7 
Empirical tests of multifactor models employ subsets of the N assets. 

The factor portfolios need not be linear combinations of the subset of assets. 
Results similar to those above will hold within a subset of N assets. For 
subset analysis when considering the tangency portfolio (of the subset), the 
maximum squared Sharpe ratio of the assets and factor portfolios, and the 
optimal orthogonal portfolio for the subset, it is necessary to augment the 

We can relate the optimal orthogonal portfolio parameters to the factor N assets with the factor portfolios K. Defining Z; as the (N+Kx 1) vector 
model deviations by comparing (6.6.1) and (6.6.6). Taking the uncondi- [Z: ZktI1withm ean p:' and covariance matrix a:,f or the tangency portfolio 
tional expectations of both sides, of these N+K assets we have 

and by equating the variance of E, with the variance of PhZh+t  u,, 
The subscript s indicates that a subset of the assets is being considered. If 
any of the factor portfolios is a linear combination of the N assets, it will be 
necessary to use the generalized inverse in (6.6.16). 

The key link between the model deviations and the residual variances and 
covariances emerges from (6.6.13). The intuition for the link is straight-  his result is related to the work of Gibbons, Ross, and Shanken (1989) 
forward. Deviations from the model must be accompanied by a common 



246 6. Multifactor Pricing Models 6.6. Interpreting Deviations from Exact Factor Pricing 

The analysis (with a subset of assets) involves the quadratic a'x- 'a  com- be the alternative: 
puted using the parameters for the N assets. Gibbons, Ross, and Shanken 
(1989) and Lehmann (1987,1992) provide interpretations of this quadratic 
term using Sharpe ratios. Assuming C is of full rank, they show 

Ho can be tested using the test statistic Jl from (6.2.12): 

Consistent with (6.6.15),f or the subset of assets a ? - ' a  is the squared Sharpe 
ratio of the subset's optimal orthogonal portfolio h,. Therefore, for a given 
subset of assets: where T is the number of timeseries observations, N is the number of assets 

or  portfolios of assets included, and Kis the number offactor portfolios. The 
hat superscripts indicate the maximum likelihood estimators. Under the 

and null hypothesis, J1 is unconditionally distributed central F with N degrees 
s2 = sir + s; of freedom in the numerator and (T - N - K) degrees of freedom in the 

% denominator. 
Note that the squared Sharpe ratio of the subset's optimal orthogonal port- To interpret deviations from the null hypothesis, we require a general 
folio is less than or  equal to that of the population optimal orthogonal representation for the distribution ofJ . Conditional on the factor portfolio 
portfolio, that is, returns the distribution of Jl is 

2 2 
Sh, F sh. (6.6.20) 

Next we use the optimal orthogonal portfolio and the Sharpe ratios 
results together with the model deviation residual variance link to develop 
implications for distinguishing among asset pricing models. Hereafter the 
s where S is the noncentrality parameter of the F distribution. If K = 0 then 

subscript is suppressed. No ambiguity will result since, in the subsequent 
A ,  & - I *  

analysis, we will be working only with subsets of the assets. the term [l + pKQK will not appear in (6.6.21) or in (6.6.23), and 
Jl will be unconditionally distributed non-central F. 

We consider the distribution of Jl under two different alternatives, 
6.6.3 Implications for Separating Alternative Theories which are separated by their implications for the maximum value of the 

If a given factor model is rejected a common interpretation is that more (or squared Sharpe ratio. With the risk-based multifactor alternative there will 
different) risk factors are required to explain the risk-return relation. This be an upper bound on the squared Sharpe ratio, whereas with the nonrisk- 
interpretation suggests that one should include additional factors so that the based alternatives the maximum squared Sharpe ratio is unbounded as the 
null hypothesis will be accepted. A shortcoming of this popular approach number of assets increases. 
is that there are multiple potential interpretations of why the hypothesis First consider the distribution of Jl under the alternative hypothesis 
is accepted. One view is that genuine progress in terms of identifying the that deviations are due to missing factors. Drawing on the results for the 
"right" asset pricing model has been made. But it could also be the case that squared Sharpe ratios, the noncentrality parameter of the F distribution is 
the apparent success in identifying a better model has come from finding 
a good within-sample fit through data-snooping. The likelihood of this 
possibility is increased by the fact that the additional factors lack theoretical From (6.6.20), the third term in (6.6.24) is bounded above by sf and positive. 
motivation. The second term is bounded between zero and one. Thus there is an upper 

This section attempts to discriminate between the two interpretations. bound for 8, 
To do this, we compare the distribution of the test statistic under the null 8 < 7'; 5 Ts;. (6.6.25) 
hypothesis with the distribution under each of the alternatives. 

We reconsider the zero-intercept F-test of the null hypothesis that the The second inequality follows from the fact that the tangency portfolio g 
intercept vector a from (6.6.1) is 0. Let Ho be the null hypothesis and HA has the maximum Sharpe ratio of any asset or  portfolio. 



248 6. Multqactor Pricing Models 6.6. Interpreting Deviations from Exact Factor Pricing 249 

Given a maximum value for the squared Sharpe ratio, the upper bound MacKinlay (1995) argues that in a perfect capital markets setting, a 
on the noncentrality parameter can be important. With this bound, in- reasonable value for the Sharpe ratio squared of the tangency portfolio 
dependent of how one arranges the assets to be included as dependent for an observation interval of one month is 0.031 (or approximately 0.6 
variables in the pricing model regression and for any value of N,' there is a for the Sharpe ratio on an annualized basis). This value, for example, 
limit on the distance between the null distribution and the distribution of corresponds to a portfolio with an annual expected excess return of 10% 
the test statistic under the missing-factor alternative. All the assets can be and a standard deviation of 16%. If the maximum squared Sharpe ratio of 
mispriced and yet the bound will still apply. the included factor portfolios is the expost squared Sharpe ratio of the CRSP 

In contrast, when the alternative one has in mind is that the source value-weighted index, the implied maximum squared Sharpe ratio for the 
of deviations is nonrisk-based, such as data snooping, market frictions, or optimal orthogonal portfolio is 0.021. This monthly value of 0.021 would 
market irrationalities, the notion of a maximum squared Sharpe ratio is be consistent with a portfolio which has an annualized mean excess return 
not useful. The squared Sharpe ratio (and the noncentrality parameter) of 8% and annualized standard deviation of 16%. We work through the 
are in principle unbounded because the theory linking the deviations and analysis using this value. 
the residual variances and covariances does not apply. When comparing Using this squared Sharpe ratio for the optimal orthogonal portfolio to 
alternatives with the intercepts of about the same magnitude, in general, calculate 6, the distribution of Jl from equation (6.2.1) is 
one would expect to see larger test statistics in this nonrisk-based case. 

We examine the informativeness of the above analysis by considering 
alternatives with realistic parameter values. We consider the distribution of 
the test statistic for three cases: the null hypothesis, the missing risk factors This distribution will be used to characterize the risk-based alternative. 
alternative, and the nonrisk-based alternative. For the risk-based alternative, One can specify the distribution for two nonrisk-based alternatives by 
the framework is designed to be similar to that in Fama and French (1993). specifylngvalues ofa, B, and f i ; ~ ; ~ f ia~nd,  then calculating6 from (6.6.23). 
For the nonrisk-based alternative we use a setup that is consistent with the To specify the intercepts we assume that the elements of a are normally dis- 
analysis of Lo and MacKinlay (1990b) and the work of Lakonishok, Shleifer, tributed with a mean of zero. We consider two values for the standard devia- 
and Vishny (1994). tion, 0.0007 and 0.001. When the standard deviation of the elements of a is 

Consider a one-factor asset pricing model using a time series of the 0.001 about 95% of deviations will lie between -0.002 and +0.002,a n annual- 
excess returns for 32 portfolios as the dependent variable. The one factor ized spread of about 4.8%. A standard deviation of 0.0007 for the deviations 
(independent variable) is the excess return of the market so that the zero- would correspond to an annual spread of about 3.4%. These spreads are 
intercept null hypothesis is the CAPM. The length of the time series is 342 consistent with spreads that could arise from d a t a - ~ n o o ~ iTnh~ey. ~ ar e plau- 
months. This setup corresponds to that of Fama and French (1993, Table sible and even somewhat conservative given the contrarian strategy returns 
9, regression (ii)). The null distribution of the test statistic J1 is presented in papers such as Lakonishok, Shleifer, andvishny (1993). For B 

we use a sample estimate based on portfolios sorted by market capitalization 
for the Fama and French (1993) sample period 1963 to 1991. The effect of 
f i ; f 2 , l f i K  on 6 will typically be small, so it is set to zero. To get an idea of a 

To define the distribution of J1 under the alternatives of interest one reasonable value for the noncentrality parameter given this alternative, the 
needs to specify the parameters necessary to calculate the noncentrality pa- expected value of 6 given the distributional assumption for the elements 
rameter. For the risk-based alternative, given a value for the squared Sharpe of a conditional upon C = C is considered. The expected value of the 
ratio of the optimal orthogonal portfolio, the distribution corresponding noncentrality parameter is 39.4 for a standard deviation of 0.0007 and 80.3 
to the upper bound of the noncentrality parameter from (6.6.25) can be for a standard deviation of 0.001. Using these values for the noncentrality 
considered. The Sharpe ratio of the optimal orthogonal portfolio can be parameter, the distribution of Jl is 
obtained using (6.6.15) given the squared Sharpe ratios of the tangency 
portfolio and of the included factor portfolio. 

'with datasnooping the distribution o f j i s not exactly a noncentral F (see Lo and MacKin- 
'1n practice when using the I.'-test it will be necessary for N to be less than T-K so that C lay [1990b]). However, for the purposes of this analysis, the noncentral F will be a good 

will be of  full rank. approximation. 



6. MultiJactor Pricing Moakls 6.7. Conclusion 251 

tive with the lower standard deviation of the elements of' a. Several of 
the nonrisk-based alternatives could equally well explain the results. Dif- 
ferent nonrisk-based views can give the same noncentrality parameter and 
test-statistic distribution. The results are consistent with the data-snooping 
alternative of Lo and MacKinlay (1990b), with the related sample selection 
biases discussed by Breen and Korajczyk (1993) and Kothari, Shanken, and 
Sloan (1995), and with the presence of market inefficiencies. 

6.7 Conclusion 

In this chapter we have developed the econometrics for estimating and test- 
ing multifactor pricing models. These models provide an attractive alterna- 
tive to the single-factor CAPM, but users of such models should be aware of 

1 2 3 4 5 6 two serious dangers that arise when factors are chosen to fit existing data 
F statistic without regard to economic theory. First, the models may overfit the data 

because of data-snooping bias; in this case they will not be able to predict 
Figure 6.1. Distributions fm the CAPM Zerc-Intercept Test Statistic /&Four Hypoth-eses asset returns in the future. Second, the models may capture empirical reg- 

ularities that are due to market inefficiency or investor irrationality; in this 
case they may continue to fit the data but they will imply Sharpe ratios for 
factor portfolios that are too high to be consistent with a reasonable under- 
lying model of market equilibrium. Both these problems can be mitigated 
if one derives a factor structure from an equilibrium model, along the lines 

when a, = 0.001. discussed in Chapter 8. In the end, however, the usefulness of multifactor 
A plot of the four distributions from (6.6.26), (6.6.27), (6.6.28), and models will not be fully known until sufficient new data become available to 

(6.6.29) is in Figure 6.1. The vertical bar on the plot represents the value provide a true out-of-sample check on their performance. 
1.91 which Fama and French calculate for the test statistic. From this figure, 
notice that the distributions under the null hypothesis and the risk-based 
alternative hypothesis are quite close together.'' This reflects the impact of 
the upper bound on the noncentrality parameter. In contrast, the nonrisk- 
based alternatives' distributions are far to the right of the other two distri- 6.1 Consider a multiple regression of the return on any asset or portfolio 
butions, consistent with the unboundedness of the noncentrality parameter & on the returns of any set of portfolios from which the entire minimum- 
for these alternatives. variance boundary can be generated. Show that the intercept of this regres- 

Given that Fama and French find a test statistic of 1.91, these results sion will be zero and that the factor regression coefficients for any asset will 
suggest that the missing-risk-factors argument is not the whole story. From sum to unity. 
Figure 6.1 one can see that 1.91 is still in the upper tail when the distribution 
of J1 in the presence of missing risk factors is tabulated. The pvalue using 6.2 Consider two economies, economy A and economy B. The mean 
this distribution is 0.03 for the monthly data. Hence it seems unlikely that excess-return vector and the covariance matrix is specified below for each 
missing factors completely explain the deviations. of the economies. Assume there exist a riskfree asset, N risky assets with 

The data offer some support for the nonrisk-based alternative views. mean excess return p and nonsingular covariance matrix 0, and a risky 
The test statistic falls almost in the middle of the nonrisk-based alterna- factor portfolio with mean excess return pp and varianc e: a The factor 

portfolio is not a linear combination of the N assets. (This criterion can be 
met by eliminating one of the assets which is included in the factor portfolio 

"'see MacKinlay (1987) for detailed analysis of the risk-based alternative. 



252 6. Multqactor Pricing Models 

if necessary.) For both economies A and B: 7 
Present-Value Relations 

Given the above mean and covariance matrix and the assumption that the 
factor portfolio p is a traded asset, what is the maximum squared Sharpe 
ratio for the given economies? 

6.3 Returning to the above problem, the economies are further specified. 
Assume the elements of a are cross-sectionally independent and identically 
distributed, 

ai - IID(0,a:) i = 1 ,  . . . , N.  (6.7.3) 

The specification of the distribution of the elements of 6 conditional on a THE FIRST PART of this book has examined the behavior of stock returns 
differentiates economies A and B. For economy A: in some detail. The exclusive focus on returns is traditional in empirical 

research on asset pricing; yet it belies the name of the field to study only 
returns and not to say anything about asset prices themselves. Many of the 
most important applications of financial economics involve valuing assets, 

and for economy B: and for these applications it is essential to be able to calculate the prices 
that are implied by models of returns. In this chapter we discuss recent 
research that tries to bring attention back to price behavior. We deal with 

Unconditionally the cross-sectional distribution of the elements of 6 will common stock prices throughout, but of course the concepts developed in 

be the same for both economies, but for economy A conditional on a, 6 is this chapter are applicable to other assets as well. 

fixed. What is the maximum squared Sharpe ratio for each economy? What The basic framework for our analysis is the discounted-cash-$ow or p-esent- 

is the maximum squared Sharpe ratio for each economy as the N increases value model. This model relates the price of a stock to its expected future 

to infinity? cash flows-its dividends-discounted to the present using a constant or 
time-varying discount rate. Since dividends in all future periods enter the 
present-value formula, the dividend in any one period is only a small com- 
ponent of the price. Therefore long-lasting or persistent movements in div- 
idends have much larger effects on prices than temporary movements do. 
A similar insight applies to variation in discount rates. The discount rate 
between any one period and the next is only a small component of the 
long-horizon discount rate applied to a distant future cash flow; therefore 
persistent movements in discount rates have much larger effects on prices 
than temporary movements do. For this reason the study of asset prices is 
intimately related to the study of long-horizon asset returns. Section 7.1 
uses the present-value model to discuss these links between movements in 
prices, dividends, and returns. 

We mentioned at the end of Chapter 2 that there is some evidence for 
Predictability of stock returns at long horizons. This evidence is statistically 
weak when only past returns are used to forecast future returns, as in Chap 
ter 2, but it becomes considerably stronger when other variables, such as 
the dividend-price ratio or the level of interest rates, are brought into the 



254 7. Present-Value Relations 7.1. The Relation brtween Prices, Dividends, and Returns 255 

analysis. In Section 7.2, we use the formulas of Section 7.1 to help inter- Purchase of the stock at price Pt today gives one a claim to next period's 
pret these findings. We show how various test statistics will behave, both dividend per share D,+* but not to this period's dividend D,.' 
under the null hypothesis and under the simple alternative hypothesis that An alternative measure of return is the log or continuously compounded 
the expected stock return is time-varying and follows a persistent first-order return, defined in Chapter 1 as 
autoregressive (AR(1)) process. A major theme of the section is that recent 
empirical findings using longer-horizon data are roughly consistent with this 
persistent AR(1) alternative model. We also develop the implications of the 
AR(1) model for price behavior. Persistent movements in expected returns Here, as throughout this chapter, we use lowercase letters to denote log 
have dramatic effects on stock prices, making them much more volatile than variables. 
they would be if expected returns were constant. 

The source of this persistent variation in expected stock returns is an 7.1.1 The Linear Present-Value Relation with Constant Expected Returns 
important unresolved issue. One view is that the time-variation in expected 
returns and the associated volatility of stock prices are evidence against the In this section we explore the consequences of the assumption that the 
Efficient Markets Hypothesis (EMH). But as we argued in Chapter 1, the expected stock return is equal to a constant R: 
EMH can only be tested in conjunction with a model of equilibrium returns. 
This chapter describes evidence against the joint hypothesis that the EMH 
holds and that equilibrium stock returns are constant, but it leaves open the 

Taking expectations of the identity (7.1.1) , imposing (7.1.3),a nd rearrang- 
possibility that a model with time-varying equilibrium stock returns can be 

ing, we obtain an equation relating the current stock price to the next 
constructed to fit the data. We explore this possibility further in Chapter 8. 

period's expected stock price and dividend: 

7.1 The Relation between Prices, Dividends, and Returns 

In this section we discuss the present-value model of stock prices. Using This expectational difference equation can be solved forward by repeatedly 
the identity that relates stock prices, dividends, and returns, Section 7.1.1 substituting out future prices and using the Law of Iterated Expectations- 
presents the expected-present-value formula for a stock with constant ex- the result that El [E,+l [XI] = E, [ X I ,d iscussed in Chapter 1-to eliminate 
pected returns. Section 7.1.1 assumes away the possibility that there are future-dated expectations. After solving forward K periods we have 
so-called rational bubbles in stock prices, but this possibility is considered in 
Section 7.1.2. Section 7.1.3 studies the general case where expected stock 
returns vary through time. The exact present-value formula is nonlinear in 
this case, but a loglinear approximation yields some useful insights. Sec- 
tion 7.1.4 develops a simple example in which the expected stock return is 

The second term on the right-hand side of (7.1.5) is the expected discounted 
time-varying and follows an AR(1) process. 

value of the stock price K periods from the present. For now, we assume 
We first recall the definition of the return on a stock given in Chapter 

1. The net simple return is that this term shrinks to zero as the horizon K increases: 

This definition is straightforward, but it does use two notational conventions 
' ~ h e s etim ing assumptions are standard in the finance literature. However some of the lit- 

that deserve emphasis. First, Rt+1 denotes the return on the stock held from erature on volatility tests, for example Shiller (1981) and Campbell and Shiller (1987,1988a,b), 
time t to time t + 1. The subscript t + 1 is used because the return only uses the alternative timing convention that the stock price is measured at the beginning of the 

becomes known at time t + 1. Second, P, denotes the price of a share of Period or traded cumdividend. Differences between the formulas given in this chapter and 
those in the original volatility papers are due to this difference in timing conventions. 

stock measured at the end of period t ,  or equivalently an ex-dividend price: 



256 7. Present-Value Relations 7.1. The Relation between Prices, Dividends, and Returns 257 

Assumption (7.1.6) will be satisfied unless the stock price is expected to The expected stock price next period does not equal the stock price today as 
grow forever at rate R or faster. In Section 7.1.2 below, we discuss models would be required if the stock price were a martingale; rather, the expected 
of rational bubbles that relax this assumption. future stock price equals one plus the constant required return, (1 + R), 

Letting K increase in (7.1.5) and assuming (7.1.6),w e obtain a formula times the current stock price, less an adjustment for dividend payments." 
expressing the stock price as the expected present value of future dividends To obtain a martingale, we must construct a portfolio for which all dividend 
out to the infinite future, discounted at a constant rate. For future conve- payments are reinvested in the stock. At time t, this portfolio will have N, 
nience we write this expected present value as PDt: shares of the stock, where 

The value of this portfolio at time t, discounted to time 0 at rate R, is 

An unrealistic special case that nevertheless provides some useful in- 
tuition occurs when dividends are expected to grow at a constant rhte G 
(which must be smaller than R to keep the stock price finite): It is straightforward to show that Mt is a martingale. 

Even though the stock price P, is not generally a martingale, it will follow 
a linear process with a unit root if the dividend Dtf ollows a linear process 

Substituting (7.1.8) into (7.1.7), we obtain the well-known "Gordon growth with a unit root.4 In this case the expected present-value formula (7.1.7) 

model" (Gordon [1962]) for the price of a stock with a constant discount relates two unit-root processes for P, and D,. It can be transformed to a 

rate R and dividend growth rate G, where G < R: relation between stationary variables, however, by subtracting a multiple of 
the dividend from both sides of the equation. We get 

P, Et[Dt+11 - (1 + G)D, 
= 

R - G  R- G  ' 

The Gordon growth model shows that the stock price is extremely sensitive 
Equation (7.1.13) relates the difference between the stock price and 1/R 

to a permanent change in the discount rate R when R is close to G, since the 
times the dividend to the expectation of the discounted value of future 

elasticity of the price with respect to the discount rate is (dP/dR)(R/P) = 
changes in dividends, which is stationary if changes in dividends are station- 

-R/(R - G). 
ary. In this case, even though the dividend process is nonstationary and the 

It is important to avoid two common errors in interpreting these formu- 
price process is nonstationary, there is a stationary linear combination of 

las. First, note that we have made no assumptions about equity repurchases 
prices and dividends, so that prices and dividends are cointeg~ated.~ 

by firms. Equity repurchases affect the time pattern of expected future divi- 
dends per share in (7.1.7), but they do not affect the validity of the formula 

3 ~tnhe  special case where dividends are expected to grow at aconstant rate G, this simplifies 
itself. Problem 7.1 explores this point in more detail. to E,P,+, = (1 + G)P,. The stock price is expected to grow at the same rate as the dividend, 

Second, the hypothesis that the expected stock return is constant because the dividend-price ratio is constant in this case. 
through time is sometimes known as the martingale modelof stock prices.2 But 4~ooselya,  variable follows a stationary timeseries process if shocks to the variable have 

temporary but not permanent effects. Avariable follows a process with a unit root, also known 
a constant expected stock return does not imply a martingale for the stock as an integrated process, if shocks have permanent effects on the level of the variable, but not 
price itself. Recall that a martingale for the price requires Et[Pt+~=]  Pt, on the change in the variable. In this case the first difference of the variable is stationary, but 
whereas (7.1.4) implies that the level is not. A martingale is a unit-root process where the immediate effect of a shock is 

the same as the permanent effect. See Chapter 2 or a textbook in time-series analysis such as 
Hamilton (1994) for precise definitions of these concepts. 

'Two variables with unit roots are cointegrated if some linear combination of the variables 
is stationary. See Engle and Granger (1987) or Hamilton (1994) for general discussion, or 
Campbell and Shiller (1987) for this application of the concept. Note that here the stationary 

'see Chapter 2 for a careful discussion of the martingale hypothesis. LeRoy (1989) surveys 
the martingale literature from Samuelson (1965) on. More general martingale results for linear combination of the variables involves the constant discount rate R, which generally is 

not known a pion'. 
risk-neutralized price processes are discussed in Chapter 9. 



258 7. Present-Value Relations 7.1. The Relation between Prices, Dividends, and Returns 259 

Although this formulation of the expected present-value model has It is easiest to illustrate the idea of a rational bubble with an example. 
been explored empirically by Campbell and Shiller (l987), West (l988b), Blanchard and Watson (1982) suggest a bubble of the form 
and others, stock prices and dividends are like many other macroeconomic 
time series in that they appear to grow exponentially over time rather than I ("y" -r) +  {!+I, with probability 17; 
linearly. This means that a linear model, even one that allows for a unit root, &+I = (7.1.16) 

{t+l, with probability 1 - n. 
is less appropriate than a loglinear model. Below we develop a present-value 
framework that is appropriate when dividends follow a loglinear process. This obeys the restriction (7.1.15), provided that the shock satisfies 

Et{,+1 = 0. The Blanchard and Watson bubble has a constant probability, 

7.1.2 Rational Bubbles 1 - n,  of bursting in any period. If it does not burst, it grows at a rate 
- 1, faster than R, in order to compensate for the probability of bursting. 

In the previous section we obtained an expectational difference equation, Many other bubble examples can be constructed; Problem 7.2 explores an 
(7.1.4), and solved it forward to show that the stock price must equal PDt, example suggested by Froot and Obstfeld (1991), in which the bubble is a 
the expected present value of future dividends. The argument relied on nonlinear function of the stock's dividend. 
the assumption (7.1.5) that the expected discounted stock price, K periods Although rational bubbles have attracted considerable attention, there 
in the future, converges to zero as the horizon K increases. In this section are both theoretical and empirical arguments that can be used to rule out 
we discuss models that relax this assumption. bubble solutions to the difference equation (7.1.4). Theoretical arguments 

The convergence assumption (7.1.5) is essential for obtaining a unique may be divided into partial-equilibrium arguments and general-equilibrium 
solution Po, to (7.1.4). Once we drop the assumption, there is an infinite arguments. 
number of solutions to (7.1.4). Any solution can be written in the form In partial equilibrium, the first point to note is that there can never be a 

negative bubble on an asset with limited liability. If a negative bubble were 
to exist, it would imply a negative expected asset price at some date in the 
future, and this would be inconsistent with limited liability. A second im- 

where portant point follows from this: A bubble on a limited-liability asset cannot 
start within an asset pricing model. It must have existed since asset trading 
began if it exists today. The reason is that if the bubble ever has a zero value, 
its expected future value is also zero by condition (7.1.15). But since the 
bubble can never be negative, it can only have a zero expectation if it is zero 

The additional term B, in (7.1.14) appears in the price only because it is 
in the future with probability one (Diba and Grossman (1988)). 

expected to be present next period, with an expected value (1 + R) times 
Third, a bubble cannot exist if there is any upper limit on the price of 

its current value. 
an asset. Thus a commodity-price bubble is ruled out by the existence of 

The term Pol is sometimes called fundamental value, and the term Bt 
some high-priced substitute in infinitely elastic supply (for example, solar 

is often called a rational bubble. The word "bubble" recalls some of the 
energy in the case of oil). Stock-price bubbles may be ruled out if firms 

famous episodes in financial history in which asset prices rose far higher 
impose an upper limit on stock prices by issuing stock in response to price 

than could easily be explained by fundamentals, and in which investors 
increases. Finally, bubbles cannot exist on assets such as bonds which have 

appeared to be betting that other investors would drive prices even higher a fixed value on a terminal date. 
in the f ~ t u r e .T~h e adjective "rational" is used because the presence of Generalequilibrium considerations also limit the possibilities for ratio- 
B, in (7.1.14) is entirely consistent with rational expectations and constant nal bubbles. Tirole (1982) has shown that bubbles cannot exist in a model 
expected returns. with a finite number of infinite-lived rational agents. The argument is easi- 

est to see when short sales are allowed, although it does not in fact depend 
%ackay (1852) is a classic reference on early episodes such as the Dutch tulipmania on the possibility of short sales. If a positive bubble existed in an asset 

in the 17th Century and the London South Sea Bubble and Paris Mississippi Bubble in the infinite-lived agents could sell the asset short, invest some of the proceeds 
18th Century. Kindleberger (1989) describes these and other more recent episodes, while to pay the dividend stream, and have positive wealth left over. This arbitrage 
Garber (1989) argues that Dutch tulip prices were more closely related to fundamentals than opportunity rules out bubbles. 
is commonly realized. 



260 7. Present-Value Relations 7.1. The Relation between Prices, Dividends, and Returns 261  

Tirole (1985) has studied the possibility of bubbles within the Diamond proximation, as suggested by Campbell and Shiller (1988a,b). The loglin- 
(1965) overlapping-generations model. In this model there is an infinite ear relation between prices, dividends, and returns provides an accounting 
number of finite-lived agents, but Tirole shows that even here a bubble framework: High prices must eventually be followed by high future divi- 
cannot arise when the interest rate exceeds the growth rate of the economy, dends, low future returns, or some combination of the two, and investors' 
because the bubble would eventually become infinitely large relative to the expectations must be consistent with this, so high prices must be associated 
wealth of the economy. This would violate some agent's budget constraint. with high expected future dividends, low expected future returns, or some 
Thus bubbles can only exist in dynamically inefficient overlapping-generations combination of the two. Similarly, high returns must be associated with up- 
economies that have overaccumulated private capital, driving the interest ward revisions in expected future dividends, downward revisions in expected 
rate down below the growth rate of the economy. Many economists feel future returns, or some combination of the two (Campbell [1991]). 
that dynamic inefficiency is unlikely to occur in practice, and Abel, Mankiw, Thus the loglinear framework enables us to calculate asset price behav- 
Summers, and Zeckhauser (1989) present empirical evidence that it does ior under any model of expected returns, rather than just the model with 
not describe the US economy. constant expected returns. The loglinear framework has the additional ad- 

There are also some empirical arguments against the existence of bub- vantage that it is tractable under the empirically plausible assumption that 
bles. The most important point is that bubbles imply explosive behavior of dividends and returns follow loglinear driving processes. Later in this chap- 
various series. In the absence of bubbles, if the dividend Dt follows a linear ter we use the loglinear framework to interpret the large empirical literature 
process with a unit root then the stock price Pt has a unit root while the on predictability in long-horizon stock returns. 
change in the price AP, and the spread between price and a multiple of The loglinear approximation starts with the definition of the log stock 
dividends Pl - Dt/R are stationary. With bubbles, these variables all have an return rl+l. Using (7.1.1),  (7.1.2), and the convention that logs of variables 
explosive conditional expectation: limK,,(l/(l + II)")E, [x,+K] # 0 for are denoted by lowercase letters, we have 
X1 = P,, Apt, or Pt - Dt/R. Empirically, there is little evidence of explosive 
behavior in these series. A caveat is that stochastic bubbles are nonlinear, 
so standard linear methods may fail to detect the explosive behavior of the 
conditional expectation in these models. 

Finally, we note that rational bubbles cannot explain the observed pre- The last term on the right-hand side of (7.1.17) is a nonlinear function of the 
dictability of stock returns. Bubbles create volatility in prices without cre- log dividend-price ratio, f (dt+l- $I,+~)L. ike any nonlinear function f (x,+~), 
ating predictability in returns. To the extent that price volatility can be it can be approximated around the mean of x ,+~x,, using a first-order Taylor 
explained by return predictability, the bubble hypothesis is superfluous. expansion: 

Although rational bubbles may be implausible, there is much to be f (xt+l> % f + f1(2>(%+-l  2). (7.1.18) 
learned from studying them. An important theme of this chapter is that 

Substituting this approximation into (7.1.17), we obtain 
small movements in expected returns can have large effects on prices if they 
are persistent. Conversely, large persistent swings in prices can have small 
effects on expected returns in any one period. A rational bubble can be 
seen as the extreme case where price movements are so persistent-indeed, wher-e p an d k are -param eters of linearization defined by p = 1/(1 + 
explosive-that they have no effects on expected returns at all. exp(d - p)), where (d - p) is the average log dividend-price ratio, and k - 

- log(p) - (1 - p) log(l/p - 1). When the dividend-price ratio is constant, 
then p = 1/(1 + DIP), the reciprocal of one plus the dividend-price ratio. 

7.1.3A n Apfrroximate Present-Value Relation with Time-Varying Expected Returns Empirically, in US data over the period 1926 to 1994 the average dividend- 
SO far we have assumed that expected stock returns are constant. This price ratio has been about 4% annually, implying that ,o should be about 0.96 

in annual data, or about 0.997 in monthly data. The Taylor approximation 
assumption is analytically convenient, but it contradicts the evidence in 

(7.1.18) replaces the log of the sum of the stock price and the dividend in 
Chapter 2 and in Section 7.2 that stock returns are predictable. 

(7.1.17) with a weighted average of the log stock price and the log dividend 
It is much more difficult to work with present-value relations when ex- 

in (7.1.19). The log stock price gets a weight p close to one, while the log 
pected stock returns are time-varying, for then the relation between prices 
and returns becomes nonlinear. One approach is to use a loglinear ap- dividend gets a weight 1 - p close to zero because the dividend is on average 



262 7. Present-Value Relations 7.1. The Relation between Prices, Dividends, and Returns 263 

much smaller than the stock price, so a given proportional change in the div- Equation (7.1.21) is a dynamic accounting identity; it has been obtained 
idend has a much smaller effect on the return than the same proportional merely by approximating an identity and solving forward subject to a termi- 
change in the price. nal condition. The terminal condition (7.1.20) rules out rational bubbles 

that would cause the log stock price to grow exponentially forever at rate 1/ p  

Approximation Accuracy or faster. Equation (7.1.21) shows that if the stock price is high today, then 
The approximation (7.1.19) holds exactly when the log dividend-price ratio there must be some combination of high dividends and low stock returns 
is constant, for then and move together one-for-one and equation in the future.' 
(7.1.19) is equivalent to equation (7.1.17). Like any other Taylor expansion, Equation (7.1.21) holds expost, but it also holds ex ante. Taking expec- 
the approximation (7.1.19) will be accurate provided that the variation in tations of (7.1.21), and noting that pt = E, [ P I ] b ecause PI is known at time 
the log dividend-price ratio is not too great. One can get a sense for the t ,  we obtain 
accuracy of the approximation by comparing the exact return (7.1.17) with 
the approximate return (7.1 .l9) in actual data. Using monthly nominal 
dividends and prices on the CRSP value-weighted stock index over the pe- 
riod 1926:l to 1994:12, for example, the exact and approximate returns 
have means of 0.78% and 0.72% per month, standard deviations of 5.55% This should be thought of as a consistency condition for expectations, anal- 
and 5.56% per month, and a correlation of 0.99991. The approximation ogous to the statement that the expectations of random variables X  and Y 
error-the difference between the approximate and the exact return-has should add up to the expectation of the sum X +  Y. If the stock price is high 
a mean of -O.O6%, a standard deviation of O.O8%, and a correlation of 0.08 today, then investors must be expecting some combination of high future 
with the exact return. Using annual nominal dividends and prices on the dividends and low future returns. Equation (7.1.22) is a dynamic generaliza- 
CRSP value-weighted stock index over the period 1926 to 1994, the exact tion of the Gordon formula for a stock price with constant required returns 
and approximate returns have means of 9.20% and 9.03% per year, standard and dividend growth. Campbell and Shiller (1988a,b) call (7.1.22)-and 
deviations of 19.29% and 19.42% per year, and a correlation of 0.99993. The (7.1.24) below-the dynamic Gordon growth model or the dividend-ratio model. 
approximation error has a mean of -0.1796, a standard deviation of O.26%, Like the original Gordon growth model, the dynamic Gordon growth 
and a correlation of 0.51 with the exact return. Thus the approximation model says that stock prices are high when dividends are expected to grow 
misstates the average stock return but captures the dynamics of stock returns rapidly or when dividends are discounted at a low rate; but the effect on 
well, especially when it is applied to monthly dam7 the stock price of a high dividend growth rate (or a low discount rate) now 

depends on how long the dividend growth rate is expected to be high (or 
Implicationsfor Prices how long the discount rate is expected to be low), whereas in the original 
Equation (7.1.19) is a linear difference equation for the log stock price, model these rates are assumed to be constant at their initial levels forever. 
analogous to the linear difference equation for the level of the stock price One can use the definitions of p  and k to show that the dynamic Gordon 
that we obtained in (7.1.4) under the assumption of constant expected growth model reduces to the original Gordon growth model when dividend 
returns. Solving forward and imposing the condition that growth rates and discount rates are constant. 

For future convenience, we can simplify the notation in (7.1.22), rewrit- 
ing it as 

we obtain 

where pdt is the expected discounted value of (1 - p )  times future log div- 
idends in (7.1.22) and p,, is the expected discounted value of future log 
stock returns. This parallels the notation we used for the constant-expected- 
return case in Section 7.1.1. 

7 ~ ncran  also ron~pal-ex act and approximate real returns. The correction fol- inflation 
has no important effects on t l~ec wnparison. See Can~pbella nd Shiller (1988a) for a more %ampbell and Sh~ller( 1988a) evaluate the accuracy of the approxnnatlon In (7 1 21) 
detailed evaluation of approximation accuracy at short and long horizons. 



264 7. Present-Value Relations 7.1. The Relation between Prices, Dividends, and Returns 265 

Equation (7.1.22) can be rewritten in terms of the log dividend-price is not only simple, but also empirically relevant. Suppose that the expected 
ratio rather than the log stock price: log stock return is a constant r plus an observable zero-mean variable x,: 

We further assume that xt follows the first-order autoregressive ( U ( 1 ) )  
The log dividend-price ratio is high when dividends are expected to grow process 
only slowly, or when stock returns are expected to be high. This equation xt+l = 4%+  h+l?  -1 < 4 < 1. (7.1.28) 
is useful when the dividend follows a loglinear unit-root process, so that When the AR coefficient 4 is close to one, we will say that the x, process 
log dividends and log prices are nonstationary. In this case changes in 

is highly persistent. Equation (7.1.28) implies that the variance of x, and 
log dividends are stationary, so from (7.1.24) the log dividend-price ratio its innovation $,, which we write as a: and a; respectively, are related by 
is stationary provided that the expected stock return is stationary. Thus 
log stock prices and dividends are cointegrated, and the stationary linear  :a = (1 - $*)a:. 

combination of these variables involves no unknown parameters since it is Under these assumptions, it is straightforward to show that 

just the log ratio. This simple structure makes the loglinear model easier to 
use in empirical work than the linear cointegrated model (7.1.13). 

So far we have written asset prices as linear combinations of expected 
future dividends and returns. We can use the same approach to write asset 
returns as linear combinations of revisions in expected future dividends and Equation (7.1.29) gives the effect on the stock price of variation through 
returns (Campbell [1991]).  Substituting (7.1.22) into (7.1.19), we obtain time in the expected stock return. The equation shows that a change in the 

expected return has a greater effect on the stock price when the expected 
return is persistent: Since p is close to one, a 1% increase in the expected 
return today reduces the stock price by about 2% if 4 = 0.5, by about 4% if 
q5 = 0.75, and by about 10% if 4 = 0.9. 

This example illustrates an important point. The variability of expected 
stock returns is measured by the standard deviation of xt. If this standard 
deviation is small, it is tempting to conclude that changing expected returns 

This equation shows that unexpected stock returns must be associated with have little influence on stock prices, in other words, that variability in prt 
changes in expectations of future dividends or real returns. An increase in is small. Equation (7.1.29) shows that this conclusion is too hasty: The 
expected future dividends is associated with a capital gain today, while an standard deviation of prt is the standard deviation of xt divided by (1 - p4) ,  
increase in expected future returns is associated with a capital loss today. so if expected returns vary in a persistent fashion, prt can be very variable 
The reason is that with a given dividend stream, higher future returns can even when x1 itself is not. This point was stated by Summers (1986), and 
only be generated by future price appreciation from a lower current price. particularly forcefully by Shiller (1984): 
For convenience, we can simplify the notation of (7.1.25) to Returns on speculative assets are nearly unforecastable; this fact is the 

basis of the most important argument in the oral tradition against a role 
for mass psychology in speculative markets. One form of this argument 

where vt+l is the unexpected stock return, 1]d,,+1 is the change in expecta- claims that because real returns are nearly unforecastable, the real price 
tions of future dividends in (7.1.25),a nd I],, t+l is the change in expectations of stocks is close to the intrinsic value, that is, the present value with 
of future returns. constant discount rate of optimally forecasted future real dividends. 

This argument . . . is one of the most remarkable errors in the history 
7.1.4 Prices and Returns in a Simple Example of economic thought. 

The formulas developed in the previous section may be easier to understand In our example the stock price can be written as the sum of two terms. 
in the context of a simple example. Later we will argue that the example The first term is the expected discounted value of future dividends, pdt; this 



266 7. Present-Value Relations 7.2. Present-Value Relations and US Stock Price Behavior 267 

is not quite a random walk for the reasons given in Section 7.1.1 above, but be positively autocorrelated if dividend news and expected-return news have 
it is close to a random walk when the dividend stream is not too large or a sufficiently large positive covariance. The covariance between dividend 
variable. The second term is a stationary AR(1) process, -p,,. This two- news and expected-return news can also be chosen to make stock returns 
component description of stock prices is often found in the literature (see serially uncorrelated. This case, in which stock returns follow a serially 
Summers [1986], Fama and French [1988b], Poterba and Summers [1988], uncorrelated white noise process while expected stock returns follow a per- 
and Jegadeesh [1991])  . sistent AR(1) process, illustrates the possibility that an asset market may be 

The AR(1) example also yields a particularly simple formula for the weak-form efficient (returns are unforecastable from the history of returns 
one-period stock return rt+l. The general stock-return equation (7.1.25) themselves) but not semistrong-form efficient (returnsa re forecastable from 
simplifies because the innovation in expected future stock returns, ~ ~ , ~is + l ,  the information variable xt).  
given by p(t+l/(l - p@).T hus we have This possibility seems to be empirically relevant for the US stock market. 

The statisticalIy insignificant long-horizon autocorrelations reported at the 
end of Chapter 2 imply that there is only weak evidence for predictability of 
long-horizon stock returns given past stock returns; but in the next section 

To understand the implications of this expression, assume for simplicity we show that there is stronger evidence for predictability of long-horizon 
that news about dividends and about future returns, ~ d , ~ +anl d  Ct+l, are returns given other information variables. 
~ncorre la tedT.~h en using the notation V a r [ ~ ~ ,=~ a+: ~(S]O  a; represents 
the variance of news about all future dividends, not the variance of the 
current dividend), and using the fact that a; = (1 - @2)a:, we can calculate 7.2 Present-Value Relations and US Stock Price Behavior 
the variance of rt+l as 

We now use the identities discussed in the previous section to interpret 
recent empirical findings on the time-series behavior of US stock prices. 
Section 7.2.1 discusses empirical work that predicts stock returns over long 

where the approximate equality holds when @ << ,o and p is close to one. horizons, using forecasting variables other than past returns themselves. We 
Persistence in the expected return process increases the variability of real- present illustrative empirical results when dividend-price ratios and interest 
ized returns, for small but persistent changes in expected returns have large rate variables are used to forecast stock returns. Section 7.2.2 relates long- 
effects on prices and thus on realized returns. horizon return behavior to price behavior, in particular stock price volatility. 

Equations (7.1.28) and (7.1.30) can also be used to show that realized Section 7.2.3 shows how time-series models can be used to calculate the long- 
stock returns follow an ARMA(1,l) process and to calculate the autocorre- horizon implications of short-horizon asset market behavior. 
lations of this process. There are offsetting effects: The positive autocorre- 
lations of expected returns in (7.1.28) appear in realized returns as well, but 
a positive innovation to future expected returns causes a contemporaneous 7.2.1 Long-Horizon Regressions 

capital loss, and this introduces negative autocorrelation into realized re- Recently there has been much interest in regressions of returns, measured 
turns. In the ARMA(1,l) representation the AR coefficient is the positive over various horizons, onto forecasting variables. Popular forecasting vari- 
persistence parameter 9, but the MA coefficient is negative. Problem 7.3 ables include ratios of price to dividends or earnings (see Campbell and 
explores these effects in detail, showing that the latter effect dominates pro- Shiller [1988a,b], Fama and French [1988a], Hodrick [1992], and Shiller 
vided that @ < p. Thus there is some presumption that changing expected [1984]), and various interest rate measures such as the yield spread be- 
returns create negative autocorrelations in realized returns. tween long- and short-term rates, the quality yield spread between low- and 

Problem '7.3 also generalizes the example to allow for a nonzero covari- highgrade corporate bonds or commercial paper, and measures of recent 
ance between dividend news and expected-return news. Stock returns can changes in the level of short rates (see Campbell [1987], Fama and French 

[1989], Hodrick [1992], and Keim and Stambaugh [I9861) . 
 his might be the case, for example, if expected returns are determined by the volatility Here we concentrate on the dividend-price ratio, which in US data is the 

of the dividend growth process, and dividend volatility is driven by a GARCH model of the type most successful forecasting variable for long-horizon returns, and on a short- 
discussed in Chapter 12 so that shocb to volatility are uncorrelated with shocks to the level of term nominal interest-rate variable. We start with prices and dividends on 
dividends. 



268 7. Present-Value Relations 7.2. Present-Value Relations and US Stock Pn'ce Behavior 269 

the value-weighted CRSP index of stocks traded on the NYSE, the AMEX, and 
Table 7.1. Long-hm'wn regressions of log stock returns on the log dividend-price ratio. 

the NASDAQ. The dividend-price ratio is measured as the sum of dividends 
paid on the index over the previous year, divided by the current level of the 
index; summing dividends over a full year removes any seasonal patterns in 
dividend payments, but the current stock index is used to incorporate the Forecast Horizon ( K )  
most recent information in stock prices.10 

The interest-rate variable is a transformation of the one-month nominal 
US Treasury bill rate motivated by the fact that unit-root tests often fail 
to reject the hypothesis that the bill rate has a unit root. We subtract a 
backward one-year moving average of past bill rates from the current bill rate 
to get a stochastically &ended interest rate that is equivalent to a triangularly 
weighted moving average of past changes in bill rates, where the weights 
decline as one moves back in time. Accordingly the detrended interest rate 
is stationary if changes in bill rates are stationary. This stochastic detrending 
method has been used by Campbell (1991) and Hodrick (1992). 

Table 7.1 shows a typical set of results when the dividend-price ratio 
is used to forecast returns. The table reports monthly regressions of log 
real stock returns onto the log of the dividend-price ratio at the start of the 
holding period. Returns are measured over a holding period of K months, 
which ranges from one month to 48 months (four years); whenever K > 1, 
the regressions use overlapping monthly data. Results are reported for the 
period 1927 to 1994 and also for subsamples 1927 to 1951 and 1952 to 1994. r is the log real return on a value-weighted index of NYSE, AMEX, and NASDAQstocks. (d - p) 
For each regression Table 7.1 reports the R2 statistic and the t-statistic for is the log ratio of dividends over the last year to the current price. Regressions are estimated 

by OLS, with Hansen and Hodrick (1980) standard errors, calculated from equation (A.3.3) 
the hypothesis that the coefficient on the log dividend-price ratio is zero. in the Appendix setting autocovariances beyond lag K - 1 to zero. Newey and West (1987) 
The t-statistic is corrected for heteroskedasticity and serial correlation in standard errors with q = K - 1 or q = 2(K - 1) are very similar and typically are slightly smaller 
the equation error using the asymptotic theory discussed in the Appendix. than those reported in the table. 

Table 7.1 follows Fama and French (1988a) except that the regressor is the 
log dividend-price ratio rather than the level of the dividend-price ratio 
(a change which makes very little difference to the results), overlapping statistics also increase dramatically with the forecast horizon, although they 
monthly data are used for all horizons, and the sample periods are updated. are fairly stable within the range 3.0 to 3.5 in the postwar subsample. 
Although the results in the table are for real stock returns, almost identical It is interesting to compare the results in Table 7.1 with those obtained 
results are obtained for excess returns over the one-month Treasury bill rate. when stock returns are regressed onto the stochastically detrended short- 

At a horizon of one month, the regression results in Table 7.1 are rather term interest rate in Table 7.2. The regressions reported in Table 7.2 are 
unimpressive: The R* statistics never exceed 2%, and the t-statistics exceed run in just the same way as those in Table 7.1. Once again almost identical 
2 only in the post-World War I1 subsample. The striking fact about the table results are obtained if real returns are replaced by excess returns over the 
is how much stronger the results become when one increases the horizon one-month Treasury bill rate. 
K. At a tweyear horizon the R2 statistic is 14% for the full sample, 22% for Table 7.2 shows that, like the dividend-price ratio, the stochastically 
the prewar subsample, and 32% for the postwar subsample; at a four-year detrended short rate has some ability to forecast stock returns. However this 
horizon the R~ statistic is 26% for the full sample and 42% for each of the forecasting power is very different in two respects. First, it is concentrated in 
subsamples. In the full sample and the prewar subsample the regression t- the postwar subsample; this is not surprising since short-term interest rates 

were pegged by the Federal Reserve during much of the 1930s and 1940s, 
and so the detrended short rate hardly varies in these years. Second, the 

'O~hiws ay of measuring the dividend-price ratio is standard in the academic literature, and forecasting power of the short rate is at much shorter horizons than the 
i t  is also commonly used in the financial industry. 



270 7.  Present-Value Relations 7.2. Present-Value Relations a n d  US Stock Price Behavior 271 

the log dividend-price ratio will be a better proxy for expectations of long- 
Table 7.2. Long-horizon regressions of log stock returns on the stochnstically &trended short- horizon returns than for expectations of short-horizon returns, because 
term interest rate. 

the expectations on the right-hand side of (7.1.24) are of a discounted 
value of all returns into the infinite future. This may help to explain the 
improvement in forecast power as the horizon increases in Table 7.1. 

Forecast Horizon ( K )  Even in the absence of this effect, however, it is possible to obtain results 
like those in Tables 7.1 and 7.2. To see this we now return to our AR(1) 
example in which the variable xt,  a perfect proxy for the expected stock 
return at any horizon, is observable and can be used as a regressor by the 
econometrician. Problem 7.4 develops a structural model of stock prices 
and dividends in which a multiple of the log dividend-price ratio has the 
properties of the variable xt in the AR(1) example. 

We use the AR(1) example to show that when xt is persistent, the R2 

of a return regression on xt is very small at a short horizon; as the horizon 
increases, the R2 first increases and then eventually decreases. We also 
discuss finite-sample difficulties with statistical inference in long-horizon 
regressions. 

R2 Statistics 
First consider regressing the one-period return rt+l on the variable xt. For 
simplicity, we will ignore constant terms since these are not the objects of 
interest; constants could be included in the regression, or we could simply 

r is the log real return on a value-weighted index of NYSE, AMEX, and NASDAQ stocks. y1.t work with demeaned data. In population, / ? ( I )  = 1 ,  so the fitted value 
is the 1-month nominal Treasury bill rate. Regressions are estimated by OLS, with Hansen is just x, itself, with variance a:, while the variance of the return is given 
and Hodrick (1980) standard errors, calculated from equation (A.3.3)i n the Appendix setting 
autocovariances beyond lagK-l to zero. Newey and West (1987) standard errors, with q = by equation (7.1.31)a bove. It follows that the one-period regression R2 
(K - I),  are used when the Hansen and Hodrick (1980) covariance matrix estimator is not statistic, which we write as R2(1),is  
positive definite. The cases where this occurs are marked *. 

forecasting power of the dividend-price ratio. The postwar R2 statistics are 
comparable to those in Table 7.1 at horizons of one or three months, but 

where for simplicity we are using the approximate version of (7.1.31) that 
they peak at 0.10 at a horizon of one year and then rapidly decline. The 

holds when 4 << p and p is close to one. ~ ~ (re1ach) es  an upper bound of 
regression t-statistics are likewise insignificant beyond a one-year horizon. 

How can we understand the hump-shaped pattern of R2 ( 1  - 4)/2 when the variability of dividend news, ,:a is zero. Thus even when 
statistics and 

a stock is effectively a real consol bond with known real dividends, so that 
t-statistics in Table 7.2 and the strongly increasing pattern in Table 7.1? At all variation in its price is due to changing expected returns, the one-period 
one level, the results in Table 7.1 can be understood by recalling the formula R2 statistic will be small when 4 is large. The reason is that innovations to 
relating the log dividend-price ratio to expectations of future returns and expected returns cause large unforecastable changes in stock prices when 
dividend growth rates, given above as (7.1.24): expected returns are persistent. 

The behavior of the R2 statistic in a long-horizon regression is somewhat 
more complicated. A regression with horizon K takes the form 

This expression shows that the log dividend-price ratio will be a good proxy 
for market expectations of future stock returns, provided that expectations In the AR(1) example, the best forecast of the one-period return -j  -p eriods 
of future dividend growth rates are not too variable. Moreover, in general ahead is always E , [ X , + ~ _ ~=]  @I-' x,. The best forecast of the cumulative 



272 7. Present-Value Relations 7.2. Present-Value Relations and US Stock Price Behavior 273 

return over K months is found by summing the forecasts of one-period simplifying approximation that holds when 4 << p and p is close to one 
returnsup to horizon K, soB(K) = (I+++. ..+4K-1) = (l-4K)/(1-$). yield 

The R~ statistic for the K-period regression is given by 

The ratio in (7.2.5) approaches (1 + 4) as az/a: approaches zero, so a two- 
period regression may have an R2 statistic almost twice that of a one-period 

Dividing by the one-period R2 statistic and rearranging, we obtain regression if expected returns are persistent and highly variable. On the 
other hand the ratio approaches (1 + 4 ~ ) ~a/s2 a j/a: approaches infinity, so 
a two-period regression may have an R2 statistic only half thatofa one-period 
regression if expected returns have only small and transitory variation. 

Calculations for horizons beyond two periods become very messy, but 
Campbell (1993b) reports some numerical results. When $I = 0.98, p = 
0.995, and az/a: = 0, for example, a one-period regression has an R2 

statistic of only 1.5%, but the maximum R~ is 63% for a 152-period regres- 
sion. When the forecasting variable is highly persistent, the R2 statistic can 

The first ratio on the right-hand side of (7.2.4) is just the square of continue to rise out to extremely long horizons. 
the K-period regression coefficient divided by the square of the one-period 
regression coefficient. In the AR(1) example this is (1- 4K)2/(1- 4)2, which DzfJiculties with Inference in Finite Samples 
is approximately equal to K2 for large 4 and small K. The second ratio on The t-statistics reported in Tables 7.1 and 7.2 are based on the asymptotic 
the right-hand side of (7.2.4) is closely related to the variance ratio discussed theory summarized in the Appendix. There are however a number of pitfalls 
in Chapter 2. In fact, it can be rewritten as l/(KV(K)),w here V(K) is the K- in applying this theory to regressions of returns onto the information vari- 
period variance ratio for stock returns. In the AR(1) example, Problem 7.3 able xt. 
shows that the autocorrelations of stock returns are all negative. It follows A first problem arises from the fact that in the regression of the one- 
that V(K) < l ,sol/(KV(K)) > 1/K. period return rt+l on xt, rt+l = B(l)xt+  qt+l,t he regressor xt is correlated 

Putting the two terms on the right-hand side of (7.2.4) together, we find with past error terms qtPl for i 2 0, even though it is not correlated with 
that if expected stock returns are very persistent, the multiperiod R~s tatistic contemporaneous or future error terms ~ ~ + lT+he~se. c orrelations exist be- 
grows at first approximately in proportion to the horizon K. This behavior cause shocks to the state variable xt are correlated with shocks to returns, and 
is well illustrated by the results in Table 7.1. Intuitively, it occurs because the variable xt is persistent. In the language of econometrics, the regressor 
forecasts of expected returns several periods ahead are only slightly less xt is fm~kkminedb, ut it is not exogenous. This leads to finitesample bias in 
variable than the forecast of the next period's expected return, and they the coefficient of a regression of returns on xt. In the AR(1) example, there 
are perfectly correlated with it. Successive realized returns, on the other is a simple formula for the bias when the regression horizon is one period: 
hand, are slightly negatively correlated with one another. Thus at first the 
variance of the multiperiod fitted value grows more rapidly than the variance 
of the multiperiod realized return, increasing the multiperiod R2 statistic. 
Eventually, of course, forecasts of returns in the distant future die out so the 

The term - ( I+  34)/ T is the Kendall (1954) expression for the bias in the 
first ratio on the right-hand side of (7.2.4) converges to a fixed limit; but OLS estimate of the persistence parameter 4 obtained by regressing xt+l 
the variability of realized multiperiod returns continues to increase, SO the 

on xt. As Stambaugh (1986) has shown, this bias leads to a bias in the OLS 
second ratio on the right-hand side of (7.2.4) becomes proportional to 1/K. estimate of the coefficient B(1) when the return innovation qt+l covaries 
Thus eventually multiperiod R2 statistics go to zero as the horizon increases. with the innovation in the forecasting variable In our simple example 

It may be helpful to give an even more explicit formula for the AR(1) with uncorrelated news about dividends and future returns driving current 
example in the case where the long horizon is just two periods, that is returns, the ratio uqF/a? = -p/(l - p@),w hich produces the second 
where K = 2. In this case tedious but straightforward calculations and the 



274 7. Present-Value Relations 7.2. Present-Value Relations and US Stock Price Behavior 275 

equality in (7.2.6). This bias can be substantial: With p = 0.997, for 7.2.2 Volatility Tests 
example, i t  equals 36/ T when C#J = 0.9,73/T when 4 = 0.95,a nd 171/T  In the previous section we have explored regressions whose dependent vari- 
when @ = 0.98." ables are returns measured over long horizons. One motivation for such 

A second problem is that the asymptotic theory given in the Appendix regressions is that asset prices are influenced by expectations of returns into 
may be misleading in finite samples when the horizon K is large relative the distant future, so long-horizon procedures are necessary if we are to un- 
to the sample size. Hodrick (1992) and Nelson and Kim (1993)u se Monte derstand price behavior. We now turn to empirical work that looks at price 
Carlo methods to illustrate this for the case where stock returns are regressed variability more directly. 
on dividend-price ratios. Richardson and Stock (1989)s how that the finite- LeRoy and Porter (1981)a nd Shiller (1981)s tarted a heated debate in 
sample properties of regressions with large K can be accounted for using an the early 1980s by arguing that stock prices are too volatile to be rational 
alternative asymptotic theory in which K increases asymptotically at the same forecasts of future dividends discounted at a constant rate. This contro- 
rate as the sample size. Section 2.5.1 of Chapter 2 discusses the application versy has since died down, partly because it is now more clearly understood 
of their theory to univariate regressions of returns on past returns. that a rejection of constant-discount-rate models is not the same as a rejec- 

One way to avoid this problem is to transform the basic regression so tion of the Efficient Markets Hypothesis, and partly because regression tests 
that it no longer has overlapping residuals. This has been proposed by have convinced many financial economists that expected stock returns are 
Jegadeesh (1991) for the Fama and French (1988b)r egression of returns time-varying rather than constant. Nonetheless the volatility literature has 
on lagged returns, and it has been advocated more generally by Cochrane introduced some important ideas that are closely connected with the work 
(1991).F or example, we might estimate on multiperiod return regressions discussed in the previous section. Useful 

surveys of this literature include Gilles and LeRoy (1991),L eRoy (1989), 
Shiller (1989,C hapter 4), and West (l988a). 

The early papers in the volatility literature used levels of stock prices 
where the error term v , + l , ~is  now serially uncorrelated. The numerator and dividends, but here we restate the ideas in logarithmic form. This is 
of the regression coefficient y( K) in (7.2.7)i s the same as the numerator consistent with the more recent literature and with the exposition in the 
of the regression coefficient B(K) in (7.2.2)b,e cause the covariance of x rest of this chapter. We begin by defining a log perfect-fwesight stock Fee, 
measured at one date and r measured at another date depends only on the 
difference between the two dates. Hence y(K) = 0 in (7.2.7) if and only 
if @(K) = 0 in (7.2.2). However it does not necessarily follow that tests 
of y (K) = 0 and @(K) = 0 have the same asymptotic properties under 
the null or general alternative hypotheses. Hodrick (1992) presents Monte The perfect-foresight price p: is so named because from the ex Post stock 

Carlo evidence on the distributions of both kinds of test statistics; he finds price identity (7.1.21)i t is the price that would prevail if realized returns 
were constant at some level r, that is, if there were no revisions in expecta- 

that they both tend to reject the null too often if asymptotic critical values 
are used, so that the long-horizon t-statistics reported in Tables 7.1 and 7.2 tions driving unexpected returns. Equivalently, from the ex ante stock price 

identity (7.1.22)i t is the price that would prevail if expected returns were 
should be treated with caution. However he also finds that these biases 

constant and investors had perfect knowledge of future dividends. Substi- 
are not strong enough to account for the evidence of return predictability 

tuting (7.2.8)i nto (7.1.21)w, e find that 
reported in the tables. 

An important unresolved question is whether there are circumstances 
under which long-horizon regressions have greater power to detect devia- 
tions from the null hypothesis than do short-horizon regressions. Hodrick 
(1992)a nd Mark (1995)p resent some suggestive Monte Carlo evidence that The difference between p: and p, is just a discounted sum of future de- 
this may be the case, and Campbell (1993b)a lso studies the issue, but the meaned stock returns. 
literature has not reached any firm conclusion at this stage. If we now take expectations and use the definition given in (7.1.22)a nd 

(7.1.23)o f the price component p,,, we find that 

"~imilarb iases afflict the regression when the horizon is greater than one period. See 
Hodrick (3992) and Mark (1995) for Monte Carlo evidence on this point. 



7.2. Present-Value Relations and US Stock Price Behavior 277 
276 7. Present-Value Relations 

Recall that p,, can be interpreted as that component of the stock price which The equality in (7.2.12) holds because under the null hypothesis (7.2.11) 
p: - p, must be uncorrelated with pt so no covariance term appears in the 

is associated with changing expectations of future stock returns. Thus the 
variance of p:; the variance inequality follows directly. Equation (7.2.12) 

conditional expectation of pf - p, measures the effect of changing expected 
stock returns on the current stock price. In the AR(1) example developed can also be understood by noting that an optimal forecast cannot be more 

earlier, the conditional expectation of pf -pt isjust x,/(1 -&) from (7.1%). variable than the quantity it is forecasting. With constant expected returns 
the stock price forecasts only the present value of future dividends, so it 

If expected stock returns are constant through time, then the right-hand 
cannot be more variable than the realized present value of future dividends. 

side of (7.2.10) is zero. The constant-expected-return hypothesis implies 
that pf - p, is a forecast error uncorrelated with information known at time Tests of this and related propositions are known as variance-bounds tests. 

t. Equivalently, it implies that the stock price is a rational expectation of the As Durlauf and Phillips (1988) point out, variance-bounds tests can 

perfect-foresight stock price: be restated as orthogonality tests. To see this, consider a regression of pt 
on pf - p,. This is the reverse of the regression considered above, but it 
too should have a zero coefficient under the null hypothesis. The reverse 
regression coefficient is always 0 = Cov[pf - pt, ptl/Var[pf - p,]. It is 

How can these ideas be used to test the hypothesis that expected stock straightforward to show that 
returns are constant? For simplicity of exposition, we begin by making two 
unrealistic assumptions: first, that log stock prices and dividends follow sta- 
tionary stochastic processes, so that they have welldefined first and second 
moments; and second, that log dividends are observable into the infinite so the variance inequality (7.2.12) will be satisfied whenever the reverse 
future, so that the perfect-foresight price pf is observable to the econome- regression coefficient 0 > -112. This is a weaker restriction than the or- 
trician. Below we discuss how these assumptions are relaxed. thogonality condition 0 = 0, so the orthogonality test clearly has power 

in some situations where the variance-bounds test has none. The justifi- 
Orthogonality and Variance-Bounds Tests cation for using a variance-bounds test is not increased power; rather it is 
Equation (7.2.11 ) implies that p: - pt is orthogonal to information variables that a variance-bounds test helps one to describe the way in which the null 
known at time t. An orthogonality test of (7.2.11) regresses pf - pt onto hypothesis fails. 
information variables and tests for zero coefficients. If the information 
variables include the stock price p, itself, this is equivalent to a regression of Unit Roots 
p: onto p, and other variables, where the hypothesis to be tested is now that Our analysis so far has assumed that the population variances of log prices 
p, has a unit coefficient and the other variables have zero coefficients. These and dividends exist. This will not be the case if log dividends follow a unit- 
regressions are variants of the long-horizon return regressions discussed in root process; then, as Kleidon (1986) points out, the sample variances of 
the previous section. Equation (7.2.9) shows that p: - pt is just a discounted prices and dividends can be very misleading. Marsh and Merton (1986) 
sum of future demeaned stock returns, so an orthogonality test of (7.2.11 ) provide a particularly neat example. Suppose that expected stock returns 
is a return regression with an infinite horizon, where more distant returns are constant, so the null hypothesis is true. Suppose also that a firm's man- 
are geometrically downweighted.12 agers use its stock price as an indicator of "permanent earnings," setting the 

Instead of testing orthogonality directly, much of the literature tests the firm's dividend equal to a constant fraction of its stock price last period. In 
implications of orthogonality for the volatility of stock prices. The most log form, we have 

famous such implication, derived by LeRoy and Porter (1981) and Shiller dt+l = 8 + pt, (7.2.14) 

(1981), is the uariance inequality for the stock price: where there is a unique constant 8 that satisfies the null hypothesis (7.2.11 ). 
It can be shown that both log dividends and log prices follow unit-root 
processes in this example. Substituting (7.2.14) into (7.2.8), we find that 
the perfect-foresight stock price is related to the actual stock price by 

I 2 ~ h deo wnweighting allows the R ' statistic in the regression to be positive, whereas we 
showed in Section 7.2.1 that the R~ statistic in an unweighted finite-horizon return regression 
converges to zero as the horizon increases. Durlauf and Hall (1989), Scott (1985),a nd Shiller 
(1989, Chapter 1 1 )  have run regressions of this sort. 



278 7. Present-Value Relations 7.2. Present-Value Relations and US Stock Price Behavior 279 

This is just a smoothed version of the actual stock price p,, so its variance worth noting about the variable pf T .  First, if expected returns are constant, 
depends on the variance and autocorrelations of p,. Since autocorrelations (7.2.11) continues to hold when ptT  is substituted for p;. Thus tests of the 
can never be greater than one, pf must have a lower variance than p,. The constant-expected-return model can use p;T. Second, a rational bubble in 
importance of this result is not that it applies to population variances (which the stock price will affect both p, and pt T .  Thus tests using p; include bub- 
are not well defined in this example because both log prices and log div- bles in the null rather than the alternative hypothesis. Third, the difference 
idends have unit roots), but that it applies to sample variances in every p;T - pt can be written as a discounted sum of demeaned stock returns, 
sample. Thus the variance inequality (7.2.12) will always be violated in the with the sum terminating at the end of the sample period T rather than at 
Marsh-Merton example. some fixed horizon from the present date t .  Thus orthogonality tests using 

This unit-root problem is important, but it is also easy to circumvent. pzT - pt are just long-horizon return regressions, where future returns are 
The variable pf - p, is always stationary provided that stock returns are geometrically discounted and the horizon is the end of the sample period. 
stationary, so any test that - p, is orthogonal to stationary variables will be As one might expect, the asymptotic theory for statistical inference in 
well-behaved. The problems pointed out by Kleidon (1986) and Marsh and orthogonality and variance-bounds tests is essentially the same as the theory 
Merton (1986) arise when pf -p,  is regressed on the stock price p,, which has used to conduct statistical inference in long-horizon return regressions.15 
a unit root. These problems can be avoided by using unit-root regression As always, in finite samples it is important to look at the effective order of 
theory or by choosing a stationary regressor, such as the log dividend-price overlap (that is, the number of periods in (7.2.9) during which discounted 
ratio. Some other ways to deal with the unit-root problem are explored in future returns make a nonnegligible contribution to today's value of ptT  - 
Problem 7.5.13 p,). If this is large relative to the sample size, then asymptotic theory is 

unlikely to be a reliable guide for statistical inference. 
Finite-Sample Consi&rations Flavin (1983) gives a particularly clear intuition for why this might be 
So far we have treated the perfect-foresight stock price as if it were an ob- a problem in the context of variance-bounds tests. She points out that 
servable variable. But as defined in (7.2.8), the perfect-foresight price is whenever a sample variance around a sample mean is used to estimate a 
unobservable in a finite sample because it is a discounted sum of dividends population variance, there is some downward bias caused by the fact that 
out to the infinite future. The definition of pf implies that the true mean of the process is unknown. When the process is white noise, it 

is well-known that this bias can be corrected by dividing the sum of squares 
by T - 1 instead of T. Unfortunately, the downward bias is more severe 
for serially correlated processes (intuitively, there is a smaller number of 
effective observations for these processes), so this correction becomes inad- 

Given data up through time T the first term on the right-hand side of equate in the presence of serial correlation. Now ptT is more highly serially 
(7.2.16) is observable but the second term is not. correlated than p,, since pf changes only as dividends drop out of the 

Following Shiller (1981), one standard response to this difficulty is to present-value formula and discount factors are updated, while pt is affected 
replace the unobservable pf by an observable proxy ptT that uses only in- by new information about dividends. Thus the ratio of the sample variance 
sample information: of p;T to the sample variance of p, is downward-biased, and this can cause 

the variance inequality in (7.2.12) to be violated too often in finite samples. 
From the equivalence of variance-bounds and orthogonality tests, the same 
problem arises in a regression context. 

Here the terminal value of the actual stock price, pT, is used in place of the 
terminal value of the perfect-foresight stock price, p;.l4 Several points are 7.2.3 Vector Auturegressive Methods 

The methods discussed in the previous two sections have the common fea- 
I3~urlauaf nd Hall (1989) apply unit-root regression theory, while Campbell and Shiller ture that they try to look directly at long-horizon properties of the data. This 

(1988a,b) replace the log stock price with the log dividend-price ratio. Problem 7.5 is based 
on the work of Mankiw, Romer, and Shapiro (1985) and West (1988b). 

I4shiller (1981) used the sample average price instead of the end-of-sample price in his 
terminal condition, but later work, including Shiller (1989), follows the approach discussed '"RO~ and Ste~genvald( 1992) use Monte Carlo methods to study the power of orthogo- 
here. nality and variance-bounds tests. 



280 7.  Present-Value Relations 7.2. Present-Value Relations and US Stock Price Behauim 28 1 

can lead to statistical difficulties in finite samples. An alternative approach constant, then pt = pdt,w hich imposes the restriction 
is to assume that the dynamics of the data are well described by a simple 
time-series model; long-horizon properties can then be imputed from the e l '  = ( 1  - p)e2'A(I - PA)-' (7.2.21) 
short-run model rather than estimated directly. In the variance-bounds liter- 
ature, this is the approach of LeRoy and Porter (1981) .  These authors note on the VAR system. This can be tested using a nonlinear Wald test.16 
that a variance-bounds test does not require observations of the perfect- So far we have assumed that the vector x ,  includes all the relevant vari- 
foresight price pt itself; it merely requires an estimate of the variance of p:, ables observed by market participants. Fortunately this very strong assump- 
which can be obtained from a univariate time-series model for dividends. tion can be relaxed. Even if xt includes only a subset of the relevant in- 
West (1988b) develops a variant of this procedure. formation, under the constant-expected-return null hypothesis the stock 

To see how this approach can work, suppose that one observes the price pt should still equal the best VAR forecast of the discounted value 
complete vector of state variables xt used by market participants, and that of future dividends as given on the right-hand side of (7 .2 .21) .I ntuitively, 
x, follows a vector autoregressive (VAR) process. Any VAR model can be when the null hypothesis is true the stock price perfectly reveals investors' 
written in first-order form by augmenting the state vector with suitable lags information about the discounted value of future dividends. Another way 
of the original variables, so without loss of generality we write: to see the same point is to interpret the restriction (7.2.21) as enforcing 

the unforecastability of multiperiod stock returns. If multiperiod returns 
are unforecastable given investors' information, they will also be unfore- 
castable given any smaller set of information variables, and thus the VAR 

Here A is a matrix of VAR coefficients, and et+l is a vector of shocks to the test of (7.2.21) is a valid test of the null hypothesis. 
VAR. We have dropped constants for simplicity; one can think of the state One can also show that (7.2.21) is a nonlinear transformation of the 
vector as including demeaned variables. restrictions implied by the unforecastability of single-period stock returns. 

Equation (7.2.18)i mplies that multiperiod forecasts of the state vector In the VAR system the single-period stock return is unforecastable if and 
xt can be formed by matrix multiplication: only if 

e l l ( I-  pA) = (1 - p)e2'A, (7.2.22) 

which is obtained from (7.2.21)b y postmultiplying each side by ( I -P A) .T he 
economic meaning of this is that multiperiod returns are unforecastable 

This makes it easy to calculate the long-horizon forecasts that determine 
if and only if one-period returns are unforecastable. However Wald test 

prices in (7.1.22) and ( 7 . l . 2 3 ) ,o r the revisions of long-horizon forecasts 
statistics are sensitive to nonlinear transformations of hypotheses; thus Wald 

that determine returns in (7.1.25)a nd (7 .1 .26) .  
tests of the VAR coefficient restrictions may behave differently when the 
restrictions are stated in the infinite-horizon form (7.2.21) than when they 

Vector Autoregressions and Price Volatility are stated in the single-period form (7.2.22).  An interesting question for 
As a first example, suppose that the state vector includes the stock price 
pt future research is how alternative VAR test statistics behave in simple models 

as its first element and the dividend dt as its second element, while the of time-varying expected returns such as the AR(1)  example developed in 
remaining elements are other relevant forecasting variables. We define this chapter. 
vectors e l '  = [ l  0 0 . . . 01 and e2' = [ 0  1 0 . . . 01. These vectors pick An important caveat is that when the constant-expected-return null hy- 
out the first element ( p , )a nd the second element ( d t )f rom the state vector pothesis is false, the VAR estimate of pdt will in general depend on the 
x,. Using these definitions and equations (7 .1 .22) ,( 7 .1 .23) ,a nd (7 .2 .19) ,  information included in the VAR. Thus one should be cautious in interpret- 
the dividend component of the stock price is ing VAR estimates that reject the null hypothesis. As an example, consider 

an updated version of the VARs ystem used by Campbell and Shiller (1988a) 
which includes the log dividendprice ratio and the real log dividend growth 
rate. These variables are used in place of the real log price and log dividend 

The stock price itself is p, = e l l x t ,s o the expected-return component of "For details see Campbell and Shiller (1987, 1988a,b). Campbell and Shiller also show 
the stock price is the difference between the two. If expected returns are how to test other models of expected stock returns in the VAR 

--L-- 



282 7. Present-Value Relations 7.2. Present-Value Relations and US Stock Price Behavior 283 

Figure 7.1. Log Real Stock Price and Dividad Senis, Annual US Data, 1872 to 1994 Figuw 7.2. Log Real Stock Price and Estimated Dividend Component, Annual US Data, 
1876 to 1994 

in order to ensure that the VAR system is stationary. The system is estimated 
or fewer, pdt moves closely with dl .  As one increases the lag length towards 

with four lags using annual data from 1871 to 1994. Figure 7.1 shows the 
ten lags, pd, becomes much smoother and more like a trend line. The same 

real log stock price as a solid line and the real log dividend as a dashed line. 
thing happens if one adds to the VAR system a ratio of price to a 10-year or 

Both series have been demeaned so that the sample mean in the Figure is 
30-year moving average of earnings, as suggested by Campbell and Shiller 

zero. The two lines tend to move together, but the movements of lrhe log 
(1988b). There appears to be some long-run mean-reversion in dividend 

stock price are larger than those of the log dividend; thus the priceclividend 
growth which is captured by these expanded VAR systems. 

ratio is procyclical and the dividend-price ratio is countercyclica1. Figure 7.2 
In conclusion, the VAR approach strongly suggests that the stock market 

again shows the demeaned real log stock price as a solid line, but now the 
is too volatile to be consistent with the view that stock prices are optimal fore- 

dashed line is the demeaned VAR estimate of pdt, the present value of fu- 
casts of future dividends discounted at a constant rate. Some VAR systems 

ture dividends. Because the real log dividend is close to a random walk, the 
VAR estimate of pdl suggest that the optimal dividend forecast is close to the current dividend, 

is close to dl itself and thus the variation in PI is larger others that the optimal dividend forecast is even smoother than the cur- 
than that in pdc. Figure 7.3 shows the log dividend-price ratio as a solid line rent dividend; neither type of system can account for the tendency of stock 
and the log ratio of dividend to pdt as a dashed line." The present-value prices to move more than one-for-one with dividends.18 Strictly speaking, 
model with constant discount rates explains very little of the variation in however, once the null hypothesis pt = pdt is rejected, any interpretation 
stock prices relative to dividends. 

While this general conclusion is robust, the smoothness of pdt is sensitive Ia~arskyan d De Long (1993) point out that stock price behavior could be rationalized 
to the specification of the VAR system. In a low-order system with four lags if there were a unit-root component in dividend growth, but they do not present any direct 

econometric evidence for such a component. Donaldson and Kamstra (1996) argue that 
a nonlinear dividend forecasting model delivers more volatile forecasts of long-run future 

I 7 ~ h e ssee ries have not been demeaned; the level of the dividend-price ratio can be recov- dividends. This remains an active research area. 
ered from the figure by exponentiating the plotted solid line. 



284 7. Present-Value Relations 7.2. Present-Value Relations and US Stock Price Behauior 285 

Recall that the revision in expectations of all future stock returns, vr,l+l,  is 
defined as 

This becomes 

From (7.1.26) the revision in expectations of future dividends, ~ d , ~ca+n ~ ,  
be treated as a residual: 

As a concrete example, consider an updated version of the system estimated 
by Campbell (1991) in which the state vector has three elements; the real 
stock return (r,), the log dividend-price ratio (xZt),a nd the level of the 
stochastically detrended short-term interest rate ( ~ 3 ~U)s.in g monthly data 
over the period 1952:l to 1994:12, the estimated first-order VAR for these 

Figure 7.3. Log Dividend-Price Ratio and Estimated Dividend Component, Annual US variables, with asymptotic standard errors in parentheses, is 
data, 1876 to 1994 

of the behavior of pdt is conditional on the information variables included 
in the VAR. 

Vector Autoregressions and Return Volatility 
A common criticism of volatility tests, which applies equally to VAR systems 
including prices and dividends, is that the time-series process driving divi- 
dends may not be stable through time.lg Fortunately it is possible to analyze 
the variability of stock returns without modeling the dividend process. Con- 
sider a state vector xt whose first element is the one-period stock return and 
whose other elements are relevant forecasting variables for returns. With The matrix in (7.2.25) is a numerical example of the VAR coefficient matrix 
this system the unexpected stock return becomes k The R ' statistics for the three regressions summarized in (7.2.25) are 

0.040, 0.996, and 0.537, respectively, indicating a modest degree of fore- 
castability for monthly stock returns.20 

'AS the coefficient estimates suggest, the log dividend-price ratio has a root extremely close 
to unity in this sample period. Although there are theoretical reasons for believing that the log 

I g ~ hMe o digliani-Miller theorem says that firm-value maximization by managers does not dividend-price ratio is stationary, and unit-root tests reject a unit root over long sample periods 

constrain the form of dividend policy. Lehmann (1991) uses this to argue that the stochastic in US data, the persistence of the log dividend-price ratio does lead to inference problems in 
the VAR system. Hodrick (1992) is a careful Monte Carlo study of these problems. 

process describing dividends is unlikely to be stable. 



286 7. Present-Value Relations Problems 287 

We can substitute the estimated A matrix into the formula (7.2.24) and and this has broad implications for both academics and investment profes- 
use the estimated variance-covariance matrix of the error vector to sionals. 
calculate the sample variances and covariance of the expected-return and At the academic level, there is an explosion of research on the determi- 
dividend components of stock returns. The estimated expected-return com- nants of time-varying expected returns. Economists are exploring a great 
ponent has a sample variance equal to 0.75 times the variance of realized variety of ideas, from macroeconomic models of real business cycles to more 
returns, while the estimated dividend component has a sample variance heterodox models of investor psychology. We discuss some of these ideas 
only 0.12 times the variance of realized returns. The remaining variance of in Chapter 8. At a more practical level, dynamic asset-allocation models 
realized returns (0.13 of the total) is attributed to covariance between the are becoming increasingly popular. The techniques discussed here can 
expected-return and dividend components." provide quantitative inputs for these investment strategies. In this context 

The reason for this result is that the log dividend-price ratio forecasts long-horizon return regressions may be attractive not only for their poten- 
stock returns, and it is itself a highly persistent process. Thus revisions in the tial statistical advantages, but also because investment strategies based on 
log dividend-price ratio are associated with persistent changes in expected long-horizon return forecasts are likely to incur lower transactions costs. 
future returns, and this can justify large changes in stock prices. The esti- 
mated VAR process is somewhat more complicated than the simple AR(1) 
example developed earlier in this chapter, for it includes two forecasting Problems-Chapter 7 
variables, each of which is close to a univariate AR(1). However the main 
effect of the interest-rate variable is to increase the forecastability of one- 7.1 In the late 1980s corporations began to repurchase shares on a large 
period stock returns; it has a rather modest effect on the long-run behavior scale. In this problem you are asked to analyze the effect of repurchases on 
of the system, which is dominated by the persistent movements of the log the relation between stock prices and dividends. 
dividend-price ratio. Thus the long-run properties of the VAR system are Consider a firm with fixed cash flow per period, X. The total market 
similar to those of the AR(1) example. From this and our previous analysis, value of the firm (including the current cash flow X) is V .  This is the 
one would expect that VAR systems like (7.2.25) could account for the pat- present value of current and future cash flow, discounted at a constant rate 
tern of long-horizon regression results, and this indeed seems to be the case R: V = (1 + R)X/R. Each period, the firm uses a fraction A of its cash 
as shown by Campbell (1991), Hodrick (1992), and Kandel and Stambaugh flow to repurchase shares at cumdividend prices, and then uses a fraction 
(1989). Of course, VAR systems impose more structure on the data; but (1 - A) of its cash flow to pay dividends on the remaining shares. The firm 
Hodrick (1992) presents some Monte Carlo evidence that when the struc- has Nt shares outstanding at the beginning of period t (before it repurchases 
ture is correct, the finite-sample behavior of VAR systems is correspondingly shares). 
better than that of long-horizon regressions with a large horizon relative to 7.1.1 What are the cumdividend price per share and dividend per share 
the sample size. at time t? 

7.1.2 Derive a relation between the dividend-price ratio, the growth rate 
7.3 Conclusion of dividends per share G, and the discount rate R. 

The research described in this chapter has helped to transform the way fi- 7.1.3 Show that the price per share equals the expected present value 
nancial economists view asset markets. It used to be thought that expected of dividends per share, discounted at rate R. Explain intuitively why this 
asset returns were approximately constant and that movements in prices formula is correct, even though the firm is devoting only a portion of its 
could be attributed to news about future cash payments to investors. Today cash flow to dividends. 
the importance of time-variation in expected returns is widely recognized, 7.2 Consider a stock whose log dividend d, follows a random walk with 

drift: 
"1  Over a longer period 1926 to 1988, the two variances and the covariance have roughly 

equal shares of the overall variance of realized stock returns. Asymptotic standard errors for the 
variance decomposition can be ralculated using the delta method explained in Section A.4 of where el+,  - N(0,u 2) .A ssume that the required log rate of return on the 
the Appendix. As in the pricedividend VAR discussed above, the decomposition is conditional stock is a constant r .  
on the information var-iablrs inrlrrded in the VAR system. I 



288 7. Present-Value Relations 

7.2.1 We use the notation F, (for "fundamental value") to denote the 7.4.2 Now suppose that the managers of the company pay dividends 
expected present value of dividends, discounted using the required rate according to the rule 
of return. Show that f i  is a constant multiple of the dividend Dt. Write 
the ratio fi/Dt as a function of the parameters of the model. 

where c and h are constants (0 < h < I ) ,  and rlt is a white noise error 
7.2.2 Show that another formula for the stock price which gives the same uncorrelated with E , .  Managers partially adjust dividends towards funda- 
expected rate of return is mental value, where the speed of adjustment is gven by h. Marsh and 

Merton (1986) have argued for the empirical relevance of this dividend 
policy. Show that if the price of the stock equals its fundamental value, the 
log dividend-price ratio follows an AR(1) process. What is the persistence 

where h > 0 is a function of the other parameters of the model. Solve 
of this process as a function of h and p? 

for A. 
7.4.3 Now suppose that the stock price does not equal fundamental 

7.2.3 Discuss the strengths and weaknesses of this model of a rational value, but rather satisfies pt = v,-  y (dt-  vt), where y > 0. That is, price 
bubble as compared with the Blanchard-Watson bubble, (7.1.16) in the exceeds fundamental value whenever fundamental value is high relative 
text. to dividends. Show that the approximate log stock return and the log 
Note: This problem is based on Froot and Obstfeld (1991). dividend-price ratio satisfy the AR(1) model (7.1.27) and (7.1.28),w here 

7.3 Consider a stock whose expected return obeys the optimal forecaster of the log stock return, xt, is a positive multiple of 
the log dividend-price ratio. 

7.4.4 Show that in this example innovations in stock returns are nega- 
Assume that xt follows an AR(1) process, tively correlated with innovations in xt. 

7.5 Recall the definition of the perfect-foresight stock price: 
7.3.1 Assume that tt+lis  uncorrelated with news about future &vidend P: - x p o p j [ ( l  - ~ ) d t + l + ~k  -  r]. (7.2.8) 
payments on the stock. Using the loglinear approximate framework de- 

The hypothesis that expected returns are constant implies that the actual 
veloped in Section 7.1.3, derive the autocovariance function of realized 

stock price pt is a rational expectation of p:, given investors' information. 
stock returns. Assume that 4 < p, where p is the parameter of lin- 

NOWc onsider forecasting dividends using a smaller information set J .  De- 
earization in the loglinear framework. Show that the autocovariances of 
stock returns are all negative and die off at rate 4. Give an economic fine& 5 E[p: I J 1 .  
interpretation of your formula for return autocovariances. 7.5.1 Show that Var(pt) > var(jt). Give some economic intuition for 

this result. 
7.3.2 Now allow tt+1t o be correlated with news about future dividend 
payments. Show that the autocovariances of stock returns can be positive 7.5.2 Show that Var(p: - jt>>  Var(p: - pt) and that Var(p: - jt) 2 
if tt+la nd dividend news have a sufficiently large positive covariance. Var(pt - jt).  Give some economic intuition for these results. Discuss 

circumstances where these variance inequalities can be more useful than 
7.4 Suppose that the log "fundamental value" of a stock, vt, obeys the the inequality in part (a). 
process 

1 - P  7.5.3 Now define Pt+1 E k+p j t+ l  +(1 -p)dt+1 -j t .  Pt+l is the return that 
vt = rr - (p(d)t - vt -l) + vt-1 +el,  would prevail under the constant-expected-return model if dividends were 

forecast using the information set J .  Show that Var(rt+l) i Var(k+l). 
where p is a constant, p is the parameter of linearization defined in Section Give some economic intuition for this result and discuss circumstances 
7.1.3, dt is the log dividend on the asset, and ct is a white noise error term. where it can be more useful than the inequality in part (a). 

7.4.1 Show that if the price of the stock equals its fundamental value, Note: This problem is based on Mankiw, Romer, and Shapiro (1985) and 
then the approximate log stock return defined in Section 7.1.3 is unfore- West (l988b). 
castable. 



Intertemporal Equilibrium Models 

THIS CHAPTER RELATES asset prices to the consumption and savings deci- 
sions of investors. The static asset pricing models discussed in Chapters 
5 and 6 ignore consumption decisions. They treat asset prices as being 
determined by the portfolio choices of investors who have preferences de- 
fined over wealth one period in the future. Implicitly these models assume 
that investors consume all their wealth after one period, or at least that 
wealth uniquely determines consumption so that preferences defined over 
consumption are equivalent to preferences defined over wealth. This sim- 
plification is ultimately unsatisfactory. In the real world investors consider 
many periods in making their portfolio decisions, and in this intertemporal 
setting one must model consumption and portfolio choices simultaneously. 

Intertemporal equilibrium models of asset pricing have the potential 
to answer two questions that have been left unresolved in earlier chapters. 
First, what forces determine the riskless interest rate (or more generally the 
rate of return on a zero-beta asset) and the rewards that investors demand for 
bearing risk? In the CAPM the riskless interest rate or zero-beta return and 
the reward for bearing market risk are exogenous parameters; the model 
gives no account of where they come from. In the APT the single price 
of market risk is replaced by a vector of factor risk prices, but again the 
risk prices are determined outside the model. We shall see in this chapter 
that intertemporal models can yield insights into the determinants of these 
parameters. 

A second, related question has to do with predictable variations in asset 
returns. The riskless real interest rate moves over time, and in Chapter 7 we 
presented evidence that forecasts of stock returns also move over time. Im- 
portantly, excess stock returns appear to be just as forecastable as real stock 
returns, suggesting that the reward for bearing stock market risk changes 
over time. Are these phenomena consistent with market efficiency? Is it 
possible to construct a model with rational, utility-maximizing investors in 
which the equilibrium return on risky assets varies over time in the way de- 

291  



292 8. Intertemporal Equilibrium Models 

scribed in Chapter 7? We shall use intertemporal equilibrium models to puzzle of Mehra and Prescott (1985). Third, there are some predictable 
explore these questions. movements in short-term real interest rates, but there is little evidence of 

Section 8.1 begins by stating the proposition that there exists a stochastic accompanying predictable movements in consumption growth. This sug- 
discount factor such that the expected product of any asset return with the gests that the elasticity of intertemporal substitution is small, which in the 
stochastic discount factor equals one. This proposition holds very gener- power utility model again implies a large coefficient of relative risk aver- 
ally in models that rule out arbitrage opportunities in financial markets. sion. Finally, there are predictable variations in excess returns on stocks over 
Equilibrium models with optimizing investors imply tight links between the short-term debt which do not seem to be related to changing covariances of 
stochastic discount factor and the marginal utilities of investors' consump- stock returns with consumption growth. These lead formal statistical tests 
tion. Thus by studying the stochastic discount factor one can relate asset to reject the power-utility model. 
prices to the underlying preferences of investors. In Sections 8.3 and 8.4 we explore some ways in which the basic model 

In Section 8.1 we show how the behavior of asset prices can be used can be modified to fit these facts. In Section 8.3 we discuss the effects 
to reach conclusions about the behavior of the stochastic discount factor. of market frictions such as transactions costs, limits on investors' ability to 
In particular we describe Hansen and Jagannathan's (1991) procedure for borrow or sell assets short, exogenousvariation in the asset demands of some 
calculating a lower bound on the volatility of the stochastic discount factor, investors, and income risks that investors are unable to insure. We argue that 
given any set of asset returns. Using long-run annual data on US short-term many plausible frictions make aggregate consumption an inadequate proxy 
interest rates and stock returns over the period 1889 to 1994, we estimate for the consumption of stock market investors, and we discuss ways to get 
the standard deviation of the stochastic discount factor to be 30% per year testable restrictions on asset prices even when consumption is not measured. 
or more. We also discuss a generalization of power utility that breaks the tight link 

Consumption-based asset pricing models aggregate investors into a sin- between risk aversion and the elasticity of intertemporal substitution. 
gle representative agent, who is assumed to derive utility from the aggre- In Section 8.4 we explore the possibility that investors have more com- 
gate consumption of the economy. In these models the stochastic discount plicated preferences than generalized power utility. For example, the utility 
factor is the intertemporal marginal rate of substitution-the discounted ratio function of the representative agent may be nonseparable between con- 
of marginal utilities in two successive periods-for the representative agent. sumption and some other good such as leisure. We emphasize models in 
The Eulerequations-the first-order conditions for optimal consumption and which utility is nonseparable over time because investors derive utility from 
portfolio choices of the representative agent--can be used to link asset re- the level of consumption relative to a time-varying habit or subsistence level. 
turns and consumption. Finally, we consider some unorthodox models that draw inspiration from 

Section 8.2 discusses a commonly used consumption-based model in experimental and psychological research. 
which the representative agent has time-separable power utility. In this 
model a single parameter governs both ~k aversion and the elasticity ofin- 
tertemporal substitution-the willingness of the representative agent to adjust 8.1 The Stochastic Discount Factor 
planned consumption growth in response to investment opportunities. In 
fact, the elasticity of intertemporal substitution is the reciprocal of risk aver- We begin our analysis of the stochastic discount factor in the simplest pos- 
sion, so in this model risk-averse investors must also be unwilling to adjust sible way, by considering the intertemporal choice problem of an investor 
their consumption growth rates to changes in interest rates. The model who can trade freely in asset i and who maximizes the expectation of a 
explains the risk premia on assets by their covariances with aggregate con- time-separable utility function: 
sumption growth, multiplied by the risk-aversion coefficient for the repre- 
sentative investor. 

Using long-run annual US data, we emphasize four stylized facts. First, Max El ~j U(Cl+j) , 
the average excess return on US stocks over short-term debt-the equity [jyo ] 
premium-is about 6% per year. Second, aggregate consumption is very 
smooth, so covariances with consumption growth are small. Putting these where S is the time discount factor, Ct+j is the investor's consumption in 
facts together, the power utility model can only fit the equity premium if the period t + j, and U(Ct+j) is the period utility of consumption at t + j. 
coefficient of relative risk aversion is very large. This is the equity premium One of the first-order conditions or E u h e quations describing the investor's 



294 8. Intertemporal Equilibn'um Models 8.1. The Stochastic Discount Factor 295 

optimal consumption and portfolio plan is wealth precisely when wealth is most valuable to the investor. The investor 
therefore demands a large risk premium to hold it. 

Although it is easiest to understand (8.1.3)b y reference to the intertem- 

The left-hand side of (8 .1 .2)i s the marginal utility cost of consuming one poral choice problem of an investor, the equation can be derived merely 

real dollar less at time t; the right-hand side is the expected marginal utility from the absence of arbitrage, without assuming that investors maximize 

benefit from investing the dollar in asset i at time t,  selling it at time t+l  well-behaved utility functions.' We show this in a discrete-state setting with 

for ( 1  + dollars, and consuming the proceeds. The investor equates states s = 1 . . . S and assets i = 1 . . .  N .  Define q, as the price of asset i and 

marginal cost and marginal benefit, so (8.1.2)d escribes the optimum. q as the ( N x 1 ) vector of asset prices, and define X,, as the payoff of asset i 

If we divide both the left- and right-hand sides of (8.1.2) by U f ( C t )w,  e in state s and X as an ( S xN )  matrix giving the payoffs of each asset in each 
state. Provided that all asset prices are nonzero, we can further define G as 

get an ( S xN ) matrix giving the gross return on each asset in each state. That 
1 = + &,t+l)Mt+l], (8.1.3) is, the typical element of G is G,, = 1 + R,, = X,,/q,. 

where Mt+l = 6 U1(Ct+l)U/  1 (Ct )T.  he variable MI+]i n (8.1.3)i s known as the Now define an ( S x  1 )  vector p, with typical element p,, to be a state 
stochastic discount factor, or pricingkernel. In the present model it is equivalent pice vector if it satisfies X'p = q. An asset can be thought of as a bundle 
to the discounted ratio of marginal utilities 6 U'(Ct+l) /U 1(Ct)w,  hich is called of state-contingent payoffs, one for each state; the sth element of the state 
the intertemporal marginal rate of substitution. Note that the intertemporal price vector, p,, gives the price of one dollar to be paid in state s, and we 
marginal rate of substitution, and hence the stochastic discount factor, are represent each asset price as the sum of its statecontingent payoffs times the 
always positive since marginal utilities are positive. appropriate state prices: q, = x,p ,X,,. Equivalently, if we divide through by 

Expectations in (8.1.3)a re taken conditional on information available q,, we get 1 = C s p S ( l+  RSZo)r  G'p = L, where L is an ( S x l )v ector of ones. 
at time t;  however, by taking unconditional expectations of the left- and An important result is that there exists a positive state price vector if 
right-hand sides of (8.1.3)a nd lagging one period to simplify notation, we and only if there are no arbitrage opportunities (that is, no available assets 
obtain an unconditional version: or combinations of assets with nonpositive cost today, nonnegative payoffs 

tomorrow, and a strictly positive payoff in at least one state). Furthermore, 
if there exists a positive state price vector, then (8.1.3)i s satisfied for some 
positive random variable M. To see this, define M, = p,/~r, ,w here n ,  is the 

These relationships can be rearranged so that they explicitly determine 
probability of state s. For any asset i the relationship G'p = L implies 

expected asset returns. Working with the unconditional form for conve- 
nience, we have E[(1+  &)Mt]  = E[1 + &IEIMt] + Cov[&, M , ] ,s o 

which is the static discrete-state equivalent of (8 .1 .3) .M , is the ratio of the 
If there is an asset whose unconditional covariance with the stochastic dis- state price of state s to the probability of state s; hence it is positive because 
count factor is zero-an "unconditional zero-beta" asset-then (8.1.5)i m- state prices and probabilities are both positive. 
plies that this asset's expected gross return E [ l  + Rot] = l / E I M t ] .T h'1 s can If M, is small, then state s is "cheap" in the sense that investors are 
be substituted into (8.1.5)t o obtain an expression for the excess return Zit unwilling to pay a high price to receive wealth in state s. An asset that tends 
on asset i over the zero-beta return: to deliver wealth in cheap states has a return that covaries negatively with 

M. Such an asset is itself cheap and has a high return on average. This is 
the intuition for (8.1.6)w ithin a discretestate framework. 

In the discrete-state model, asset markets are complete if for each state 
This shows that an asset's expected return is greater, the smaller its covari- s, one can combine available assets to get a nonzero payoff in s and zero 
ance with the stochastic discount factor. The intuition behind this result is 
that an asset whose covariance with Mt+l is small tends to have low returns 'The theory underlyinge quation (8.1.3)i s discussed at length in textbooks such as Ingersoll 
when the investor's marginal utility of consumption is high-that is, when (1987). The role of conditioning information has been explored by Hansen and Richard 
consumption itself is low. Such an asset is risky in that it fails to deliver (1987). 



296 8. Intertemporal Equilibrium Models 8. I. The Stochastic Discount Factor 297 

payoffs in all other states. A further important result is that the state price Expanding the expectation of the product E[(L+  R,)M:(%)], we have 
vector is unique if and only if asset markets are complete. In this case M 
is unique, but with incomplete markets there may exist many M's satisfying 
equation (8.1.3). This result can be understood by considering an economy 
with several utility-maximizing investors. The first-order condition (8.1.2) 
holds for each investor, so each investor's marginal utilities can be used 
to construct a stochastic discount factor that prices the assets in the econ- 
omy. With complete markets, the investors' marginal utilities are perfectly 
correlated so they all yield the same, unique stochastic discount factor; with 
incomplete markets there may be idiosyncratic variation in marginal utilities where 52 is the unconditional variance-covariance matrix of asset returns. It 
and hence multiple stochastic discount factors that satisfy (8.1.3). follows then that 

Pz = ~ ~ - ~ ( L - M E [ L + R , ] ) ,  (8.1.11) 
8.1.1 Volatility Bounds 

and the variance of the implied stochastic discount factor is 
Any model of expected asset returns may be viewed as a model of the stochas- 
tic discount factor. Before we discuss methods of testing particular models, 
we ask more generally what asset return data may be able to tell us about the 
behavior of the stochastic discount factor. Hansen and Jagannathan (1991) 
have developed a lower bound on the volatility of stochastic discount factors 
that could be consistent with a given set of asset return data. They begin 

The right-hand side of (8.1.12) is a lower bound on the volatility of any 
with the unconditional equation (8.1.4) and rewrite it in vector form as 

stochastic discount factor with mean M. To see this, note that any other 
Mt(X) satisfjmg (8.1.8) must have the property 

where L is an N-vector of ones and Rt is the N-vector of time-t asset returns, 
with typical element &. 

Hansen and Jagannathan assume that Rt has a nonsingular variance- 
covariance matrix 52, in other words, that no asset or combination of assets Since M,*(M) is just a linear combination of asset returns, it follows that 
is unconditionally riskless. There may still exist an unconditional zero-beta COV[M; (M), M,(M) - M,*( M)] = 0. Thus 
asset with gross mean return equal to the reciprocal of the unconditional 
mean of the stochastic discount factor, but Hansen and Jagannathan assume Var [M, (%)I = var [M,*( M)] + ~ a[Mr, ( M) - M : (M)] 
that if there is such an asset, its identity is not known a p-iori. Hence they 
treat the unconditional mean of the stochastic discount factor as an un- + COV[M,* ( a ) , M ,(M) - M,*( MI] 

known parameter M. For each possible M, Hansen and Jagannathan form = ~ar[M,(*M )] + var[Mt (M) - M,*( M)] 
a candidate stochastic discount factor M,*(M) as a linear combination of as- 
set returns. They show that the variance of M,*(M)p laces a lower bound on 2 ~ a[Mr,*( M)].  (8.1.14) 
the variance of any stochastic discount factor that has mean M and satisfies 
(8.1.8). In fact, we can go beyond this inequality to show that 

Hansen andJ agannathan first show how asset pricing theory determines 
the coefficients Pji?.i n 

1f M:(M) is to be a stochastic discount factor it must satisfy (8.1.8), so a stochastic discount factor can only have a variance close to the lower 
bound if it is highly correlated with the combination of asset returns M,+(M). 



298 8. Intertemporal Equilibrium Models 8. I .  The Stochastic Discount Factor 299 

The Benchmark Portfolio 
We can restate these results in a more familiar way by introducing the idea 
of a benchmark portfolio. We first augment the vector of risky assets with an 
artificial unconditionally riskless asset whose return is 11% - 1 .  Recall that 
we have proceeded under the assumption that no unconditionally riskless 
asset exists; but if it were to exist, its return would have to be 11% - 1. We 
then define the benchmark portfolio return as 

It is straightforward to check that this return can be obtained by forming a 
portfolio of the risky assets and the artificial riskless asset, and that it satisfies 
the condition (8.1.8) on returns. Problem 8.1 is to prove that ht has the 
following properties: 

(PI) & is mean-variance efficient. That is, no other portfolio has 
smaller variance and the same mean. 

(P2) Any stochastic discount factor ~ ~ ( 3h7as) a  greater correlation 
with & than with any other portfolio. For this reason is sometimes 
referred to as a maximum-correlation portfolio (see Breeden, Gibbons, and 
Litzenberger [1989])  . 

(P3) All asset returns obey a beta-pricing relation with the benchmark 
portfolio. That is, 

where Bib = Cov[&t, &] /Var [&I. When an unconditional zero-beta 
asset exists, then it can be substituted into (8.1.17) to get a conventional Figure 8.1. (a) Mean-Standard Deviation Diagram for Asset Returns; (6) Implied Standard 
beta-pricing equation. Deuiation-Mean Diagram for Stochastic Discount Factors 

Two further properties are useful for a geometric interpretation of the 
Hansen-Jagannathan bounds. Consider Figure 8.1. Panel (a) is the famil- 
iar meanstandard deviation diagram for asset returns, with the mean gross 
return plotted on the vertical axis and the standard deviation of return discount factor. In panel (a), the feasible set of risky asset returns is shown. 
on the horizontal. Panel (b) is a similar diagram for stochastic discount We augment this with a riskless gross return 1/37 on the vertical axis; the 
factors, with the axes rotated; standard deviation is now on the vertical axis minimum-variance set is then the tangent line from 1/M to the feasible 
and mean on the horizontal. This convention is natural because in panel set of risky assets, and the reflection of the tangent in the vertical axis. 

(a) we think of assets' second moments determining their mean returns, Property (PI) means that the benchmark portfolio return is in the minimum- 
while in panel (b) we vary the mean stochastic discount factor exogenously variance set. It plots on the lower branch because its positive correlation with 

and trace out the consequences for the standard deviation of the stochastic the stochastic discount factor gives it a lower mean gross return than l/m. 



300 8. Intertemporal Equilibrium Models 8.1. The Stochastic Discount Factor 301 

We can now state two more properties: If we have only a single excess return Zt, then this condition simplifies 

(P4) The ratio of standard deviation to gross mean for the benchmark to~ar[M:(M)] = x ~ ( E [ z ~ ] ) ~ / v ~ ~o[ zr , ] ,  
portfolio satisfies 

This is illustrated in Figure 8.2, which has the same structure as Figure 8.1. 
But the right-hand side of this equation is just the slope of the tangent Now the restriction on the stochastic discount factor in panel (b) is that it 
line in panel (a) of Figure 8.1. As explained in Chapter 5, this slope is should lie above a ray from the origin with the same slope as a ray from the 
the maximum Sharpe ratio for the set of assets. origin through the single risky excess return in panel (a). 

(P5) The ratio of standard deviation to mean for the benchmark port- Implications of Nonnegatiuity 
folio gross return is a lower bound on the same ratio for the stochastic So far we have ignored the restriction that Mt must be nonnegative. Hansen 
discount factor. That is, and Jagannathan (1991) show that this can be handled fairly straightfor- 

wardly when an unconditionally riskless asset exists. In this case the mean 
of Mt is known, and the problem can be restated as finding coefficients a 
that define a random variable 

Properties (P4) and (P5) establish that the stochastic discount factor 
in panel (b) must lie above the point where a ray from the origin, with 
slope equal to the maximum Sharpe ratio in panel (a), passes through the where X+ = max(X, 0) is the nonnegative part of X, subject to the con- 
vertical line at M. As we vary M, we trace out a feasible region for the straint 
stochastic discount factor in panel (b). This region is higher, and thus 
more restrictive, when the maximum Sharpe ratio in panel (a) is large for 
a variety of mean stochastic discount factors M. A set of asset return data In the absence of the nonnegativity constraint, this yields the previous solu- 
with a high maximum Sharpe ratio over a wide range of % is challenging tion for the case where there is an unconditionally riskless asset. With the 
for asset pricing theory in the sense that it requires the stochastic discount nonnegativity constraint, it is much harder to find a coefficient vector a 
factor to be highly variable. By looking at panel (a) one can see that such a that satisfies (8.1.24);  Hansen andJ agannathan (1991) discuss strategies for 
data set will contain portfolios with very different mean returns and similar, transforming the problem to make the solution easier. Once a coefficient 
small standard deviations. A leading example is the set of returns on US vector is found, however, it is easy to show that M:+ has minimum variance 
Treasury bills, which have differences in mean returns that are absolutely among all nonnegative random variables Mt satisfying (8.1.8). To see this, 
small, but large relative to the standard deviations of bill returns. consider any other Mt and note that 

The above analysis applies to returns themselves; the calculations are 
somewhat simpler if excess returns are used. Writing the excess return on 
asset i over some reference asset k (not necessarily riskless) as Zit = Rt -&t, 
and the vector of excess returns as Zt, the basic condition (8.1.4) becomes 

Proceeding as before, we form M,*(%) = M + (Zt - E [ z ~ ] ) ' @w~h,e re the But if E[MtMTf ] > E[(M,*+)~t]h, en E[M:] > E[(M:+)~] since the cor- 
relation between these variables cannot be greater than one. 

tilde is u-s ed to indicate that @ is defined with excess returns. We find that 
-1 The above analysis can be generalized to deal with the more realistic 

= fl (-ME[Z,]), where f2 is the variance-covariance matrix of excess case in which there is no unconditionally riskless asset, by augmenting the 
returns. It follows that the lower bound on the variance of the stochastic return vector with a hypothetical riskless asset and varying the return on this 
discount factor is now asset. This introduces some technical complications which are discussed by 

var[M;(%)] = ~ 2 ~ [ ~ , l f f 2 - 1 ~ [ ~ t ] (.8 .1.21) Hansen and Jagannathan (1991). 



8. Intertemporal Equilibrium Modpls 8.1. The Stochastic Discount Factor 303 

R 
i

Figure 8.3. Feasible Reg on for Stochastic Discount Factors Implied ty Annual US Data, 
1891 to I994 

equity premium-is too high to be easily explained by standard asset pricing 
models. They make this point in the context of a tightly parametrized 
consumption-based model, but it can be made more using the 
excess-return restriction (8.1.22). Over the period 1889 to 1994, the annual 
excess simple return on the Standard and Poors stock index over commercial 
paper has a standard deviation of 18% and a mean of 6%.3 Thus the slope 
of the rays from the origin in Figure 8.2 should be 0.06/0.18 = 0.33, 

Figure 8.2. (a )M ean-Standard Deviation Diagram for a Single Excess Asset Return; (b) Im- meaning that the standard deviation of the stochastic discount factor must 
plied Standard Deuiation-Mean Diagram for Stochastic Discount Factom be at least 33% if it has a mean of one. As we shall see in the next section, the 

standard consumption-based model with a risk-aversion coefficient in the 
conventional range implies that the stochastic discount factor has a mean 
near one, but an annual standard deviation much less than 33%. 

A First Look a t  the Equity Premium Puzzle 
The Hansen-Jagannathan approach can be used to understand the well-  he return on six-month commercial paper, rolled over in January and July, is used 
known equity premium puzzle of Mehra and Prescott (1985).' Mehra and instead of a Treasury bill return because Treasury bill data are not available in the early part 

Prescott argue that the average excess return on the US stock market-the of this long sample period. Mehra and Prescott (1985) splice together commertial paper and 
Treasury bill rates, whereas here we use commercial paper rates throughout the sample period 
for consistency. The choice of short-term rates makes little difference to the results. Table 8.1 

'~ochranea nd Hansen (1992) approach the equlty premlum puzzle from t h ~ sp o~n to f below gives some sample moments for log asset returns, but the moments stated here are for 
mew Kocherlakota (1996) surveys the large l~teratureo n the puzzle simple returns. 



304 8. IntertemporalE quilibrium Models 8.2. Consumption-Based Asset Pricing with Power Utility 305 

Figure 8.3, whose format follows Hansen and Jagannathan (1991),a lso In this section we examine the empirical implications of the CCAPM. 
illustrates the equity premium puzzle. The figure shows the feasible region We begin by assuming that there is a representative agent who maximizes a 
for the stochastic discount factor implied by equation (8.1.12) and the an- time-separable power utility function, so that 
nual data on real stock and commercial paper returns over the period 1891 
to 1994. The figure does not use the nonnegativity restriction discussed 
in the previous section. The global minimum standard deviation for the 
stochastic discount factor is about 0.33, corresponding to a mean stochastic 
discount factor of about 0.98 and an unconditional riskless return of about where y is the coefficient of relative risk aversion. As y approaches one, 
2%. As the mean moves away from 0.98, the standarddeviation bound the utility function in (8.2.2) approaches the log utility function u(Ct) = 
rapidly increases. The difference between the feasible region in Figure 8.3 log(Ct). 
and the region above a ray from the origin with slope 0.33 is caused by The power utility function has several important properties. First, it 
the fact that Figure 8.3 uses bond and stock returns separately rather than is scale-invariant: With constant return distributions, risk premia do not 
merely the excess return on stocks over bonds. The figure also shows mean- change over time as aggregate wealth and the scale of the economy in- 
standard deviation points corresponding to various degrees of risk aversion crease. A related property is that if different investors in the economy have 
and a fixed time discount rate in a consumption-based representative agent the same power utility function and can freely trade all the risks they face, 
asset pricing model of the type discussed in the next section. The first point then even if they have different wealth levels they can be aggregated into 
above the horizontal axis has relative risk aversion of one; successive points a single representative investor with the same utility function as the indi- 
have risk aversion of two, three, and so on. The points do not enter the vidual  investor^.^ This provides some justification for the use of aggregate 
feasible region until relative risk aversion reaches a value of 25. consumption, rather than individual consumption, in the CCAPM. 

In interpreting Figure 8.3 and similar figures, one should keep in mind A property of power utility that may be less desirable is that it rigidly 
that both the volatility bound for the stochastic discount factor and the links two important concepts. When utility has the power form the elasticity of 
points implied by particular asset pricing models are estimated with error. intertemporal substitution (the derivative of planned log consumption growth 
Statistical methods are available to test whether a particular model satisfies with respect to the log interest rate), which we write as $, is the reciprocal 
the volatility bound (see for example Burnside [1994], Cecchetti, Lam, and of the coefficient of relative risk aversion y .  Hall (1988) has argued that this 
Mark [1994], and Hansen, Heaton, and Luttmer [1995]). These methods linkage is inappropriate because the elasticity of intertemporal substitution 
use the Generalized Method of Moments of Hansen (1982), discussed in concerns the willingness of an investor to move consumption between time 
the Appendix. periods-it is well-defined even if there is no uncertainty-whereas the co- 

efficient of relative risk aversion concerns the willingness of an investor to 
move consumption between states of the world-it is welldefined even in a 

8.2 Consumption-Based Asset Pricing with Power Utility one-period model with no time dimension. In Section 8.3.2 below we discuss 
a more general utility specification, due to Epstein and Zin (1991) and Weil 
(1989), that preserves the scale-invariance of power utility but breaks the 

In Section 8.1 we showed how an equation relating asset returns to the 
tight link between the coefficient of relative risk aversion and the elasticity 

stochastic discount factor, (8.1.3), could be derived from the first-order 
of intertemporal substitution. 

condition of a single investor's intertemporal consumption and portfolio 
Taking the derivative of (8.2.2) with respect to consumption, we find 

choice problem. This equation is restated here for convenience: that marginal utility U1(Ct) = cLY.S ubstituting into (8.2.1) we get 

1 = &[(I-+ & , t + i ) ~ t + ~ l .  (8.2.1) 

It is common in empirical research to assume that individuals can be ag- 
gregated into a single representative investor, so that aggregate consump- 
tion can be used in place of the consumption of any particular individual. 4~rossmanan d Shiller (1982) show that this result generalizes to a model with nontraded 

Equation (8.2.1) with M,+l = SU'(Ct+l)/U 1(C,)w, here Ct is aggregate con- assets (uninsurable idiosyncratic risks) if consumption and asset prices follow diffusion pro- 
cesses in a continuoustime model. 

sumption, is known as the consumption CAPM, or CCAPM. 



306 8. Intertemporal Equilibrium Models 8.2. Consumption-Based Asset Pricing with Power Utility 307 

which was first derived by Grossman and Shiller (1981),f ollowing the closely The assumption of homoskedasticitym akes the log risk premium on any 
related continuous-time model of Breeden (1979). A typical objective of asset over the riskless real rate constant, so expected real returns on other 
empirical research is to estimate the coefficient of relative risk aversion y (or assets are also linear in expected consumption growth with slope coefficient 
its reciprocal $) and to test the restrictions imposed by (8.2.3). It is easiest y . We have 
to do this if one assumes that asset returns and aggregate consumption ai2 
are jointly homoskedastic and lognormal. Although this implies constant Et[ri,t+l - rf,t+lI + - = Yaic. (8.2.7) 

2 
expected excess log returns, and thus cannot fit the data, it is useful for The variance term on the left-hand side of (8.2.7) is a Jensen's Inequality 
building intuition and understanding methods that can be applied to more adjustment arising from the fact that we are describing expectations of log 
realistic models. Accordingly we make this assumption in Section 8.2.1 returns. We can eliminate the need for this adjustment by rewriting the 
and relax it in Section 8.2.2, where we discuss the use of Hansen's (1982) equation in terms of the log of the expected ratio of gross returns: 
Generalized Method of Moments (GMM) to test the power utility model 
without making distributional assumptions on returns and consumption. 
Our discussion follows closely the seminal work of Hansen and Singleton 
(1982, 1983). Equation (8.2.7) shows that risk premia are determined by the coefficient of 

relative risk aversion times covariance with consumption growth. Of course, 
we have already presented evidence against the implication of this model 

8.2.1 Power Utility in a Lognormal Model that risk premia are constant. Nevertheless we explore the model as a useful 
When a random variable X is conditionally lognormally distributed, it has way to develop intuition and understand econometric techniques used in 
the convenient property (mentioned in Chapter 1) that more general models. 

log Et [XI = Et[ log XI + i vart[ log XI,  (8.2.4) A Second Look at the Equity Premium Puzzle 
Equation (8.2.7) clarifies the argument of Mehra and Prescott (1985) that 

where Vart[ log XI - Et[ (log X - E, [log x ] ) ~ ] .  If in addition X is con- the equity premium is too high to be consistent with observed consumption 
ditionally homoskedastic, then Var, [log XI = E[(log X - E~[ log x ] ) ~ ]=  behavior unless investors are extremely risk averse. Mehra and Prescott's 
Var [log X - Et[ log XI 1. Thus with joint conditional lognormality and ho- analysis is complicated by the fact that they do not use observed stock re- 
moskedasticity of asset returns and consumption, we can take logs of (8.2.3), turns, but instead calculate stock returns implied by the (counterfactual) 
use the notational convention that lowercase letters denote logs, and obtain assumption that stock dividends equal consumption. Problem 8.2 carries 

out a loglinear version of this calculation. 
One can appreciate the equity premium puzzle more directly by ex- 

amining the moments of log real stock and commercial paper returns and 
Here the notation axyd enotes the unconditional covariance of innovations consumption growth shown in Table 8.1. The asset returns are measured 
Cov[xt+1 - Et[~t+llY,t +l - Et[~t+ll la,n d 0 : - ax,. annually over the period 1889 to 1994. The mean excess log return of stocks 

Equation (8.2.5), which was first derived by Hansen and Singleton over commercial paper is 4.2% with a standard deviation of 17.7%; using 
(1983), has both time-series and cross-sectional implications. In the time the formula for the mean of a lognormal random variable, this implies that 
series, the riskless real interest rate obeys the mean excess simple return is 6% as stated earlier. 

AS is conventional in the literature, the consumption measure used in 
Table 8.1 is consumption of nondurables and  service^.^ The covariance of 
the excess log stock return with log consumption growth is the correlation 

The riskless real rate is linear in expected consumption growth, with slope of the two series, times the standard deviation of the log stock return, times 
coefficient equal to the coefficient of relative risk aversion. The equation the standard deviation of log consumption growth. Because consumption 
can be reversed to express expected consumption growth as a linear function of nondurables and services is a smooth series, log consumption growth has 
of the riskless real interest rate, with slope coefficient $ = l /y ;  in fact this 
relation between expected consumption growth and the interest rate is what 5 ~ h iism plicitly assumes that utility is separable across this form of consumption and other 

"Urces of utility. In Section 8.4 we discuss ways in which this assumption can be relaxed. 
defines the elasticity of intertemporal substitution. 



8. Intertemporal Equilibrium Models 8.2. Consumption-Based Asset Pricing with Power Utility 309 

These calculations can be related to the work of Hansen and Jagan- 
Table 8.1. Momats of consumption growth and asset returns. nathan (1991) in the following way. In the representative-agent model 

with power utility, the stochastic discount factor Mt+I = 6(Ct+l/Ct)-~,  
Correlation with Covariance with and the log stochastic discount factor mt+l = log@)-  y  Act+,.  If we are 

Variable Mean Standard consumption consumption 
deviation growth growth willing to make the approximation m,+l Mt+l - 1 ,  which will be accu- 

rate if M,+1 has a mean close to one and is not too variable, then we have 
Consumption growth 0.0172 0.0328 1 .OOOO 0.001 1 Var[Mt+l] Var[m,+ll = y 2  Var[Act+ll. Equivalently, the standard devi- 
Stock return 0.0601 0.1674 0.4902 0.0027 ation of the stochastic discount factor must be approximately the coefficient 

CP return 0.0183 0.0544 -0.1157 -0.0002 of relative risk aversion times the standard deviation of consumption growth. 
Using the Hansen-Jagannathan methodology we found that the standard de- 

Stock-CP return 0.0418 0.1774 0.4979 0.0029 viation of the stochastic discount factor must be at least 0.33 to fit our annual 
stock market data. Since the standard deviation of consumption growth is 

Consumption growth is the change in log real consumption of nondurables and services. The 0.033, this by itself implies a coefficient of risk aversion of at least 0.33/0.033 
stock return is the log real return on the S&P 500 index since 1926, and the return on a 
comparable index from Grossman and Shiller (1981) before 1926. CP is the real return on = 10. But a coefficient of risk aversion this low is consistent with the data 
&month commercial paper, bought in January and rolled over in July. All data are annual, only if stock returns and consumption are perfectly correlated. If we use 
1889 to 1994. the fact that the empirical correlation is about 0.5, we can use the tighter 

volatility bound in equation (8.1.15) to double the required standard devi- 
ation of the stochastic discount factor and hence double the risk-aversion 

a small standard deviation of only 3.3%;h ence the excess stock return has a coefficient to about 20. 
covariance with log consumption growth of only 0.003 despite the fact that 
the correlation of the two series is about 0.5. Substituting the moments in The Riskfree Rate Puzzle 

Table 8.1 into (8.2.7)s hows that a risk-aversion coefficient of 19 is required One response to the equity premium puzzle is to consider larger values for 

to fit the equity premium.6 This is much greater than 10, the maximum the coefficient of relative risk aversion y .  Kandel and Stambaugh (1991) 
value considered plausible by Mehra and Prescott. have advocated this8 However this leads to a second puzzle. Equation 

It is worth noting that the implied risk-aversion coefficient is sensitive (8.2.5) implies that the unconditional mean riskless interest rate is 

to the timing convention used for consumption. While asset prices are 
measured at the end of each period, consumption is measured as a flow 
during a period. In correlating asset returns with consumption growth 
one can assume that measured consumption represents beginning-of-period where g is the mean growth rate of consumption. The average riskless in- 
consumption or end-of-period consumption. The former assumption leads terest rate is determined by three factors. First, the riskless rate is high if 
one to correlate the return over a period with the ratio of the next period's the time preference rate - log 6 is high. Second, the riskless rate is high if 
consumption to this period's consumption, while the latter assumption leads the average consumption growth rate g is high, for then the representative 
one to correlate the return over a period with the ratio of this period's agent has an incentive to try to borrow to reduce the discrepancy between 
consumption to last period's consumption. The former assumption, which consumption today and in the future. The strength of this effect is inversely 
we use here, gives a much higher correlation between asset returns and proportional to the elasticity of intertemporal substitution in consumption; 
consumption growth and hence a lower risk-aversion coefficient than the in a power utility model where risk aversion equals the reciprocal of in- 
latter a s s u n ~ ~ t i o n . ~  tertemporal substitution, the strength of the effect is directly proportional 

to y .  Finally, the riskless rate is low if the variance of consumption growth 
is high, for then the representative agent has a precautionary motive for 

"able 8.1 reports the moments of asset returns and consumption growth whereas equation 
(8.2.7) requires the moments of innovations in these series. However the variation in condi- 'one m~ghth ink that introspection would be sufficlent to rule out very large values of y 
tional expected returns and consumption growth seems to be small enough that the moments However Kandel and Starnbaugh (1991)  pomt out that mtrospecuon can delwer very d~fferent 
of innovations are similar to the moments of the raw series. estimates of nsk averson dependmg on the sue of the gamble considered T h ~ su ggests that 

'~rossman,M elino, and Shiller (1987) handle this problem more carefully by assuming inmospecuon can be mlslead~ngo r that some more general model of uul~ty1s needed 
an underlying continuous-time model and deriving its implications for time-averaged data. 



310 8. Intertemporal Equilibrium Models 8.2. Consumption-Based Asset Pricing with Power Utility 311 

saving. The strength of this precautionary saving effect is proportional to may appear to be irrational in the sample. While i t  may seem implausible 
the square of risk aversion, y2 .  that this could be an important problem in 100 years of data, Rietz (1988) 

Given the historical average short-term real interest rate of 1.8%, the argues that an economic catastrophe that destroys almost all stock-market 
historical average consumption growth rate of 1.8%, and the historical av- value can be extremely unlikely and yet have a major depressing effect on 
erage standard deviation of consumption growth of 3.3% shown in Table stock prices. 
8.1, a y of 19 implies a discount factor 6 of 1.12; this is greater than one, A related point has been made by Brown, Goetzmann, and Ross (1995). 
corresponding to a negative rate of time preference. Weil (1989) calls this These authors argue that financial economists concentrate on the US stock 
the riskfree ratepuzzk Intuitively, the puzzle is that if investors are extremely market precisely because it has survived and grown to become the ~0r-d'~ 
risk-averse ( y  is large), then with power utility they must also be extremely largest market. In some other markets, such as those of Russia, investors 
unwilling to substitute intertemporally ($ is small). Given positive average have had all their wealth expropriated during the last 100 years and so there 
consumption growth, a low riskless interest rate and a positive rate of time is no continuous record of market prices; in others, such as the Argentine 
preference, such investors would have a strong desire to borrow from the market, returns have been so poor that today these markets are regarded as 
future. A low riskless interest rate is possible in equilibrium only if investors comparatively less important emerging markets. If this survivorship effect 
have a negative rate of time preference that reduces their desire to borrow. is important, estimates of average US stock returns are biased upwards. 

Of course, these calculations depend on the exact moments given in Ta- Although these points have some validity, they are unlikely to be the 
ble 8.1. In some data sets an even larger coefficient of relative risk aversion whole explanation for the equity premium puzzle. The difficulty with the 
is needed to fit the equity premium: Kandel and Stambaugh (1991), for Rietz (1988) argument is that it requires not only an economic catastrophe, 
example, consider a risk-aversion coefficient of 29. With risk aversion this but one which affects stock market investors more seriously than investors 
large, the precautionary savings term - y2q? /2  in equation (8.2.8) reduces in short-term debt instruments. The Russian example suggests that a catas- 
the equilibrium riskfree rate and so Kandel and Stambaugh do not need a trophe causes very low returns on debt as well as on equity, in which case 
negative rate of time preference to fit the riskfree rate. A visual impression the peso problem affects estimates of the average levels of returns but not 
of this effect is given in Figure 8.3, which shows the mean stochastic dis- estimates of the equity premium. Also, there seems to be a surprisingly large 
count factor first decreasing and then increasing as y increases with a fixed equity premium not only in the last 100 years of US data, but also in US data 
6. Since the riskless interest rate is the reciprocal of the mean stochastic from earlier in the 19th Century as shown by Siege1 (1994) and in data from 
discount factor, this implies that the riskless interest rate first increases and other countries as shown by Campbell (199613). 
then decreases with y.  The behavior of the riskless interest rate is always a 
problem for models with high y ,  however, as the interest rate is extremely Time-Variation in Expected Asset Returns and Consumption Growth 
sensitive to the parameters g and a2a nd reasonable values of the interest Equation (8.2.5) gives a relation between rational expectations of asset re- 
rate are achieved only as a knife-edge case when the effects of g and a 2  turns and rational expectations of consumption growth. It implies that 
almost exactly offset each other. expected asset returns are perfectly correlated with expected consumption 

growth, but the standard deviation of expected asset returns is y times as 
large as the standard deviation of expected consumption growth. Equiva- 

Is the Equity Premium Puzzb a Robust Phenomenon? lently, the standard deviation of expected consumption growth is $ = l /y  
Another response to the equity premium puzzle is to argue that it is an times as large as the standard deviation of expected asset returns. 
artefact of the particular data set on US stock returns. While we have not This suggests an alternative way to estimate y or $. Hansen and Single- 
reported standard errors for risk-aversion estimates, careful empirical re- ton (1983),f ollowed by Hall (1988) and others, have proposed instrumental 
search by Cecchetti, Lam, and Mark (1994), Kocherlakota (1996),a nd oth- variables (IV)r egression as a way to approach the problem. If we define an 
ers shows that the data can reject risk-aversion coefficients lower than about error term ~ i , ~ +=l   ri,t+l - E , [ q t + l ]-  ~ ( A C-~ E+t[A~c t+l] )t,h en we can 
10 using standard statistical methods. However, the validity of these tests rewrite equation (8.2.5) as a regression equation, 
depends on the characteristics of the data set in which they are used. 

Rietz (1988) has argued that there may be a pesoprobkm in these data. A 
peso problem arises when there is a small positive probability of an important 
event, and investors take this probability into account when setting market In general the error term ~ i , ~ +wlil l be correlated with realized consumption 
prices. If the event does not occur in a particular sample ~ e r i o di,n vestors growth so OLS is not an appropriate estimation method. However r ] i , t + l  is 



312 8. Intertemporal Equilibrium Models 8.2. Consumption-Based Asset Pricing with Power Utility 313 

Table 8.2. Instrumental variables regressionsfor returns and consumption growth. "Test (8.2.9)"-the R* statistic for a regression of the residual on the in- 
struments together with the associated significance level of a test of the 
over-identifying restrictions of the model. This test is discussed at the end 
of Section A.l of the Appendix. 

Table 8.2 shows strong evidence that the real commercial paper rate is 
-- 

Return First-stage regressions 6 Test Test forecastable, and weaker evidence that the real stock return is forecastable. 
(instruments) r A c (s.e.) (s.e.) (8.2.9) (8.2.10) There is very little evidence that consumption growth is foreca~tableT.~h e 

IV estimates of y are negative rather than positive as implied by the underly- 
Commercial paper 0.275 0.034 -1.984 -0.088 0.106 0.028 ing theory, but they are not significantly different from zero. The overiden- 

(1) (0.000) (0.485) (1.318) (0.11 3) (0.004) (0.234) tifylng restrictions of the model are strongly rejected when the commercial 
paper rate is used as the asset. 

Stock index 0.080 0.034 -6.365 -0.100 0.008 0.007 One problem with IV estimation of (8.2.9) is that the instruments are 
(1) (0.071) (0.485) (5.428) (0.091) (0.673) (0.705) only very weakly correlated with the regressor because consumption growth 

Commercial paper 0.297 0.102 -0.953 -0.118 0.221 0.091 is hard to forecast in this data set. Nelson and Startz (1990) have shown 
that in this situation asymptotic theory can be a poor guide to inference 

(1 and 2) (0.000) (0.145) (0.567) (0.109) (0.000) (0.096) in finite samples; the asymptotic standard error of the coefficient tends 
Stock index 0.110 0.102 -0.235 -0.008 0.105 0.097 to be too small and the overidentifjmg restrictions of the model may be 

rejected even when it is true. To circumvent this problem, one can reverse 
(1 and 2) (0.105) (0.145) (1.650) (0.059) (0.056) (0.075) 

the regression (8.2.9) and estimate 

Log real consumption growth rates and asset returns are measured in annual US data, 1889 
to 1994. The columns headed "Firststage regressions" report the R ' statistics and joint signif- 
icance levels of the explanatory variables in regressions of returns and consumption growth 
on the instruments. The columns headed ); and C report two-stage least squares instrumental- 
variables (IV) estimates of the parameters y and u in regressions (8.2.9) and (8.2.10) respec- If the orthogonality conditions hold, then as we have already discussed the 
tively. The columns headed "Test (8.2.9)" and "Test (8.2.10)" report the R2s tatistics andjoint estimate of + in (8.2.10) will asymptotically be the reciprocal of the estimate 
significance levels of the explanatory variables in regressions of IV regression residuals (8.2.9) of y in (8.2.9). In a finite sample, however, if y is large and + is small then 
and (8.2.10) on the instruments. The instruments include either one lag (in rows marked 
l ) ,o r one and two lags (in rows marked 1 and 2) of the real commercial paper rate, the real IV estimates of (8.2.10) will be better behaved than IV estimates of (8.2.9). 
consumption growth rate, and the log dividend-price ratio. In Table 8.2 + is estimated to be negative, like y ,  but is small and in- 

significantly different from zero. The overidentifying restrictions of the 
model ("Test (8.2.10)") are not rejected when only 1 lag of the instruments 
is used, and they are rejected at the 10% level when 2 lags of the instruments 

uncorrelated with any variables in the information set at time t .  Hence any are used. Table 8.2 also shows that the residual from the IV regression is 
lagged variables correlated with asset returns can be used as instruments in only marginally less forecastable than consumption growth itself. These 
an IV regression to estimate the coefficient of relative risk aversion y. results are not particularly encouraging for the consumption model, but 

Table 8.2 illustrates two-stage least squares estimation of (8.2.9). In this equally they do not provide strong evidence against the view that investors 
table the asset returns are the real commercial paper rate and real stock have power utility with a very high y (which would explain the equity pre- 
return from Table 8.1, and consumption growth is the annual growth rate mium puzzle) and a correspondingly small + (which would explain the 
of real nondurables and services consumption. The instruments are either unpredictability of consumption growth in the face of predictable asset re- 
one lag, or one and two lags, of the real commercial paper rate, the real turns). 
consumption growth rate, and the log dividend-price ratio. 

For each asset and set of instruments, the table first reports the R* 
statistics and significance levels for first-stage regressions of the asset return ' ~ np ostwar quarterly data there is stronger evidence of predictable variation in consump 
and consumption growth rate onto the instruments. The table then shows tion growth. Campbell and Mankiw (1990) show that thisvariation is associatedw ith predictable 
the IV estimate of y with its standard error, and-in the column headed Income growth. 



314 8. Intertemporal Equilibrium Models 8.3. Market Frictions 315 

8.2.2 Power Utility and Generalized Method of Moments or sell assets short, then they may have only a limited ability to exploit the 
empirical patterns in returns. In Section 8.3.1 we show how this can alter 

So far we have worked with a restrictive loglinear specification and have the basic Hansen and Jagannathan (1991) analysis of the volatility of the 
discussed cross-sectional and time-series aspects of the data separately. The stochastic discount factor. 
Generalized Method of Moments (GMM) of Hansen (1982), applied to the The same sorts of frictions may make aggregate consumption an inade- 
consumption CAPM by Hansen and Singleton (1982), allows us to estimate quate proxy for the consumption of stock market investors. In Section 8.3.2 
and test the power utility model without making distributional assumptions we discuss some of the evidence on this point, and then follow Campbell 
and without ignoring either dimension of the data. Section A.2 of the (1993a, 1996) in developing a representative-agent asset pricing theory in 
Appendix summarizes the GMM approach, and explains its relation to linear which the consumption of the representative investor need not be observed. 
instrumental variables. The theory uses a generalization of power utility, due to Epstein and Zin 

When GMM is used to estimate the consumption CAPM with power util- (1989, 1991) and Weil (1989), that breaks the link between risk aversion 
ity, using the same asset returns and instruments as in Table 8.2 and assuming and intertemporal substitution. The resulting model, in the spirit of Mer- 
white noise errors, the overidentifymg restrictions of the model are strongly ton (1973a), is a multifactor model with restrictions on the risk prices of the 
rejected whenever stocks and commercial paper are included together in factors; hence it can be tested using the econometric methods discussed in 
the system. The weak evidence against the model in Table 8.2 becomes much Chapter 6. 
stronger. This occurs because there is predictable variation in excess returns 
on stocks over commercial paper.10 Such predictable variation is ruled out 
by the loglinear homoskedastic model (8.2.5) but could in principle be ex- 8.3.1 Market Frictions and Hansen-Jagannathan Bounds 
plained by a heteroskedastic model in which conditional covariances of as- The volatility bounds of Hansen and Jagannathan (1991),d iscussed in Sec- 
set returns with consumption are correlated with the forecasting variables." tion 8.1.1, assume that investors can freely trade in all assets without in- 
The GMM system allows for this possibility, without linearizing the model curring transactions costs and without limitations on borrowing or short 
or imposing distributional assumptions, so the GMM rejection is powerful sales. These assumptions are obviously rather extreme, but they have been 
evidence against the standard consumption CAPM with power utility. relaxed by He and Modest (1995) and Luttmer (1994). To understand the 

Faced with this evidence, economists have explored two main directions approach, note that if asset i cannot be sold short, then the standard equality 
for research. A first possibility is that market frictions invalidate the standard restriction E [(I+  &)M,] = 1 must be replaced by an inequality restriction 
consumption CAPM. The measured returns used to test the model may 
not actually be available to investors, who may face transactions costs and 
constraints on their ability to borrow or shortsell assets. Market frictions may 
also make aggregate consumption an inadequate proxy for the consumption If the inequality is strict, then an investor would like to sell the asset but is 
of stock market investors. A second possibility is that investors have more prevented from doing so by the shortsales constraint. Instead, the investor 
complicated preferences than the simple power specification. We explore holds a zero position in the asset. 
each of these possibilities in the next two sections. Shortsales constraints may apply to some assets but not others; if they 

apply to all assets, then they can be interpreted as a solvency constraint, in that 
they ensure that an investor cannot make investments today that deliver neg- 

8.3 Market Frictions ative wealth tomorrow. Assuming limited liability for all risky assets, so that 
the minimum value of each asset tomorrow is zero, a portfolio with nonneg- 

We now consider various market frictions that may be relevant for asset 
ative weights in every asset also has a minimum value of zero tomorrow. 

pricing. If investors face transactions costs or limits on their ability to borrow 
Investors may also face borrowing constraints that limit their ability to 

sell assets to finance consumption today. Such constraints deliver inequality 
'O~ecallt hat in Chapter 7 we presented evidence that the dividend-price ratio forecasts restrictions of the form (8.3.1) for all raw asset returns, but the standard 

excess stock returns. The dividend-price ratio is one of the instruments used here. 
"One can understand this by considering a heteroskedasticv ersion of the linearized model equality constraint holds for excess returns since the investor is free to short 

(8.2.5) in which the variances have time subscripts. Campbell (1987) and Harvey (1989) apply One asset in order to take a long position in another asset. 
GMM to models of this type which impose the restriction that asset returns' conditional means Shortsales constraints can also be used to model proportional transac- 
are linear functions of their conditional second moments. We discuss this work further in tions costs of the type that might result from a bid-ask spread that does not 
Chapter 12. 



316 8. Intertemporal Equilibn'um Models 8.3. Market Frictions 317 

depend on the size of a trade. When there are transactions costs, the after- Melino, and Shiller (1987) and Wheatley (1988) have tested the model al- 
transactioncost return on an asset bought today and sold tomorrow is not lowing for time-aggregation and measurement error, respectively. Roughly 
the negative of the after-transactioncost return on the same asset sold today speaking, these data problems can cause asset returns weighted by measured 
and bought back tomorrow. These two returns can be measured separately marginal utility of consumption, (1  +&,,+I )6 (Ct+l / Ct)-Y,t o be forecastable 
and can both be included in the set of returns if they are made subject to in the short run but not the long run. Thus one can allow for such problems 
shortsales constraints. by lagging the instruments more than one period when testing the model.lZ 

In the presence of shortsales constraints, the vector equality (8.1.8) is Doing this naturally weakens the evidence against the consumption CAPM, 
replaced by another vector equality but the model is still rejected at conventional significance levels unless very 

long lags are used. 
A more radical suggestion is that aggregate consumption is not an ad- 

equate proxy for the consumption of stock market investors even in the 
where 8 is an unknown vector. The model implies various restrictions on long run. One simple explanation is that there are two types of agents in 
8 such as the restriction that Bi 5 1 for all i. Volatility bounds can now the economy: constrained agents who are prevented from trading in asset 
be found for each M by choosing, subject to the restrictions, the value of markets and simply consume their labor income each period, and uncon- 
8 that delivers the lowest variance for M:(%). He and Modest (1995) find strained agents. The consumption of the constrained agents is irrelevant to 
that by combining borrowing constraints, a restriction on the short sale of the determination of equilibrium asset prices, but it may be a large fraction 
Treasury bills, and asset-specific transaction costs they can greatly reduce of aggregate consumption. Campbell and Mankiw (1990) argue that pre- 
the volatility bound on the stochastic discount factor. dictable variation in consumption growth, correlated with predictable vari- 

This analysis is extremely conservative in that 8 is chosen to minimize the ation in income growth, suggests an important role for constrained agents, 
volatility bound without asking what underlying equilibrium would support while Mankiw and Zeldes (1991) use panel data to show that the consump- 
this choice for 0. If there are substantial transactions costs, for example, tion of stockholders is more volatile and more highly correlated with the 
then even risk-neutral traders will not sell one asset to buy another asset stock market than the consumption of nonstockholders. 
with a higher return unless the return difference exceeds the transactions The constrained agents in the above model do not directly influence 
costs. But the one-period transaction costs will not be relevant if traders asset prices, because they are assumed not to hold or trade financial assets. 
can buy the high-return asset and hold it for many periods, or if a trader Another strand of the literature argues that there may be some investors who 
has new wealth to invest and must pay the cost of purchasing one asset or buy and sell stocks for exogenous, perhaps psychological reasons. These 
the other. Thus the work of He and Modest (1995) and Luttmer (1994) noise traders can influence stock prices because other investors, who are 
is exploratory, a way to get a sense for the extent to which market frictions rational utility-maximizers, must be induced to accommodate their shifts 
loosen the bounds implied by a frictionless market. in demand. If utility-maximizing investors are risk-averse, then they will 

Some authors have tried to solve explicitly for the asset prices that are only buy stocks from noise traders who wish to sell if stock prices fall and 
implied by equilibrium models with transactions costs. This is a difficult expected stock returns rise; conversely they will only sell stocks to noise 
task because transactions costs make the investor's decision problem com- traders who wish to buy if stock prices rise and expected stock returns fall. 
paratively intractable except in very special cases (see Davis and Norman Campbell and Kyle (1993), Cutler, Poterba, and Summers (1991), DeLong 
(1990)). Aiyagari and Gertler ( lggl) ,  Amihud and Mendelson (l986), et al. (1990a, 1990b), and Shiller (1984) develop this model in some detail. 
Constantinides (1986),H eaton and Lucas (l996),a nd Vayanos (1995) have The model implies that rational investors do not hold the market portfolio- 
begun to make some progress on this topic. instead they shift in and out of the stock market in response to changing 

demand from noise traders-and do not consume aggregate consumption 
8.3.2 Market Frictions and Aggregate Consumption Data since some consumption is accounted for by noise traders. This makes the 

The rejection of the standard consumption CAPM may be due in part to dif- 
'2~ampbelal nd Mankiw (1990) discuss this in the context of a linearized model. Breeden, 

ficulties in measuring aggregate consumption. The consumption CAPM ap- Gibbons, and Litzenberger (1989) make a related point, arguing that at short horizons one 
plies to true consumption measured at a point in time, but the available data should replace consumption with the return on a portfolio constructed to be highly correlated 
are time-aggregated and measured with error. Wilcox (1992) describes the with longer-run movements in consumption. Brainard, Nelson, and Shapiro (1991) find that 

the consumption CAPM works better at long horizons than at short horizons. 
sampling procedures used to construct consumption data, while Grossman, 



318 8. Intertemporal Equilibrium Models 8.3. Market Frictions 319 

model hard to test without having detailed information on the investment model using the utility specification developed by Epstein and Zin (1989, 
strategies of different market participants. 1991) and Weil (l989),w hich we now summarize. 

It is also possible that utility-maximizing stock market investors are het- 
erogeneous in important ways. If investors are subject to large idiosyncratic Separating Risk Aversion and Intertemporal Substitution 
risks in their labor income and can share these risks only indirectly by trading Epstein, Zin, and Weil build on the approach of Kreps and Porteus (1978) to 

a few assets such as stocks and Treasury bills, their individual consumption develop a more flexible version of the basic power utility model. That model 
paths may be much more volatile than aggregate consumption. Even if indi- is restrictive in that it makes the elasticity of intertemporal substitution, +, 
vidual investors have the same power utility function, so that any individual's the reciprocal of the coefficient of relative risk aversion, y .  Yet it is not clear 
consumption growth rate raised to the power -y would be avalid stochastic that these two concepts should be linked so tightly. Risk aversion describes 
discount factor, the aggregate consumption growth rate raised to the power the consumer's reluctance to substitute consumption across states of the 
-y may not be a valid stochastic discount factor.I3 Problem 8.3, based on world and is meaningful even in an atemporal setting, whereas the elasticity 

Mankiw (1986), explores this effect in a simple twc-period model. of intertemporal substitution describes the consumer's willingness to sub- 
Recent research has begun to explore the empirical relevance of im- stitute consumption over time and is meaningful even in a deterministic 

perfect risk-sharing for asset pricing. Heaton and Lucas (1996) calibrate setting. The Epstein-Zin-Weil model retains many of the attractive features 
individual income processes to micro data from the Panel Study of Income of power utility but breaks the link between the parameters y and +. 
Dynamics (PSID). Because the PSID data show that idiosyncratic income The Epstein-Zin-Weil objective function is defined recursively by 
variation is largely transitory, Heaton and Lucas find that investors can min- 
imize its effects on their consumption by borrowing and lending. Thus they 
find only limited effects on asset pricing unless they restrict borrowing or 
assume the presence of large transactions costs. Constantinides and Duffie where6 = (1 - y)/(l -I /+).  When6 = 1 therecursion (8.3.3) becomes 
(1996) construct a theoretical model in which idiosyncratic shocks have per- linear; it can then be solved forward to yield the familiar time-separable 
manent effects on income; they show that this type of income risk can have power utility model. 
large effects on asset pricing. The intertemporal budget constraint for a representative agent can be 

Given this evidence, it seems important to develop empirically testable written as 
intertemporal asset pricing models that do not rely so heavily on aggregate W+l = (1 + & , t + d  (W - Ct), (8.3.4) 
consumption data. One approach is to substitute consumption out of the 

where is the representative agent's wealth, and (1 + & , t + l )  is the return 
consumption CAPM to obtain an asset pricing model that relates mean 
returns to covariances with the underlying state variables that determine on the "market" portfolio of all invested wealth. This form of the budget 

consumption. The strategy is to try to characterize the preferences that constraint is appropriate for a complete-markets model in which wealth 
includes human capital as well as financial assets. Epstein and Zin use 

an investor would have to have in order to be willing to buy and hold the 
a complicated dynamic programming argument to show that (8.3.3) and 

aggregate wealth portfolio, without necessarily assuming that this investor 
(8.3.4) together imply an Euler equation of the formI4 

also consumes aggregate consumption. 
There are several classic asset pricing models of this type set in con- 

tinuous time, most notably Cox, Ingersoll, and Ross (1985a) and Merton 
(1973a). But those models are hard to work with empirically. Campbell 
(1993a) suggests a simple way to get an empirically tractable discrete-time 

If we assume that asset returns and consumption are homoskedastic and 
jointly lognormal, then this implies that the riskless real interest rate is 

'"his is an example ofJensen3sI nequality. Since marginal utility is nonlinear, the average 
of investors' marginal utilities of consumption is not generally the same as the marginal utility 
of average consumption. This problem disappears when investors' individual consumption 
streams are perfectly correlated with one another as they will be in a complete markets setting. I4~heraer e in fact typos in equations (10) through (12) of Epstein and Zin (1991) which 
Grossman and Shiller (1982) point out that it also disappears in a continuous-time model when give intermediate steps in the derivation. 
the processes for individual consumption streams and asset prices are diffusions. 



320 8. Intertemporal Equilibrium Models 8.3. Market Frictions 321  

The premium on risky assets, including the market portfolio itself, is to the linearized formula for the log dividend-price ratio in Chapter 7. Here 
wealth can be thought of as an asset that pays consumption as its dividend.15 

If we now combine the budget constraint (8.3.9) with the loglinear Euler 
equations for the Epstein-Zin-Weil model, (8.3.6) and (8.3.7), we obtain a 
closed-form solution for consumption relative to wealth: 

This says that the risk premium on asset i is a weighted combination of 
asset i's covariance with consumption growth (divided by the elasticity of in- 
tertemporal substitution @) and asset i's covariance with the market return. 
The weights are 6 and 1 - 6 respectively. The Epstein-Zin-Weil model thus 
nests the consumption CAPM with power utility (6 = 1) and the traditional Here pm is a constant related to the conditional variances of consump- 
static CAPM (6 = 0). tion growth and the market portfolio return. The log consumption-wealth 

It is tempting to use (8.3.7) together with observed data on aggregate ratio is a constant, plus (1 - @) times the discounted value of expected 
consumption and stock market returns to estimate and test the Epstein- future returns on invested wealth. If $ is less than one, the consumer 
Zin-Weil model. Epstein and Zin (1991) report results of this type. In a is reluctant to substitute intertemporally and the income effect of higher 
similar spirit, Giovannini and Weil (1989) use the model to reinterpret the returns dominates the substitution effect, raising today's consumption rel- 
results of Mankiw and Shapiro (1986),w ho found that betas with the market ative to wealth. If @ is greater than one, the substitution effect dominates 
have greater explanatory power for the cross-sectional pattern of returns and the consumption-wealth ratio falls when expected returns rise. Thus 
than do betas with consumption; this is consistent with a value of 8 close (8.3.10) extends to a dynamic context the classic comparative statics results 
to zero. However this procedure ignores the fact that the intertemporal of Samuelson (1969). 
budget constraint (8.3.4) also links consumption and market returns. We (8.3.10) implies that the innovation in consumption is 
now show that the budget constraint can be used to substitute consumption 
out of the asset pricing model. 

Substituting Consumption Out of the Model 
Campbell (1993a) points out that one can loglinearize the intertemporal 
budget constraint (8.3.4) around the mean log consumption-wealth ratio 

An unexpected return on invested wealth has a one-for-one effect on con- 
to obtain 

sumption, no matter what the parameters of the utility function: This fol- 
lows from the scale independence of the objective function (8.3.3). An 
increase in expected future returns raises or lowers consumption depend- 
ing on whether @ is greater or less than one. Equation (8.3.11) also shows 

where p - 1 - exp(=) and k is a constant that plays no role in what when consumption will be smoother than the return on the market. When 
follows. Combining this with the trivial equality Awt+1 = Act+1-  A(ct+1-  the market return is mean-reverting, there is a negative correlation between 
wt+1),s olving the resulting difference equation forward, and taking expec- current returns and revisions in expectations of future returns. This reduces 
tations, we can write the budget constraint in the form the variability of consumption growth if the elasticity of intertemporal s u b  

stitution @ is less than one but amplifies it if @ is greater than one. 
Equation (8.3.11 ) implies that the covariance of any asset return with 

consumption growth can be rewritten in terms of covariances with the re- 

'5~ampbel(l1 993a) and Campbell and Koo (1996) explore the accuracy of the loglinear 
This equation says that if the consumption-wealth ratio is high, then the approximation in this context by comparing the approximate analytical solution for optimal 
agent must expect either high returns on wealth in the future or low con- consumption with a numerical solution. In an example calibrated to US stock market data, 
sumption growth rates. This follows just from the approximate budget con- the two solutions are close together provided that the investor's elasticity of intertemporal 

substitution is less than about 3. 
straint without imposing any behavioral assumptions. It is directly analogous 



322 8. Intertemporal Equilibrium Modelr 8.3. Market Frictions 323 

turn on the market and revisions in expectations of future returns on the than one, such assets have higher mean returns. The intuitive explanation 
market: is that such assets are desirable because they enable the consumer to profit 

from improved investment opportunities, but undesirable because they re- 
duce the consumer's ability to hedge against a deterioration in investment 
opportunities. When y < 1 the former effect dominates, and consumers 

where are willing to accept a lower return in order to hold assets that pay off when 
wealth is most productive. When y > 1 the latter effect dominates, and 
consumers require a higher return to hold such assets. 

There are several possible circumstances under which assets can be 
priced using only their covariances with the return on the market portfolio, 

aih  is defined to be the covariance of the return on asset i with "news" as in the logarithmic version of the static CAPM. These cases have been dis- 
about future returns on the market, i.e., revisions in expected future re- cussed in the literature on intertemporal asset pricing, but (8.3.14)m akes it 
turns. particularly easy to understand them. First, if the coefficient of relative risk 

Substituting (8.3.12) into (8.3.7) and using the definition of 0 in terms aversion y = 1, then the opposing effects of covariance with investment 
of the underlying parameters a and y ,  we obtain a cross-sectional asset opportunities cancel out so that only covariance with the market return is 
pricing formula that makes no reference to consumption: relevant for asset pricing. Second, if the investment opportunity set is con- 

stant, then aih  is zero for all assets, so again assets can be priced using only 
their covariances with the market return. Third, if the return on the market 
follows a univariate stochastic process, then news about future returns is per- 
fectly correlated with the current return; thus, covariance with the current 

Equation (8.3.14) has several striking features. First, assets can be priced 
return is a sufficient statistic for covariance with news about future returns 

without direct reference to their covariance with consumption growth, us- 
and can be used to price all assets. Campbell (1996a) argues that the first 

ing instead their covariances with the return on invested wealth and with 
two cases do not describe US data even approximately, but that the third 

news about future returns on invested wealth. This is a discrete-time ana- case is empirically relevant. 
logue of Merton's (1973a) continuous-time model in which assets are priced 
using their covariances with certain hedgeportfolios that index changes in the 

A Third Look at the Equity Premium Puzzle 
investment opportunity set. 

(8.3.14) can be applied to the risk premium on the market itself. When i = 
Second, the only parameter of the utility function that enters (8.3.14) m, we get 

is the coefficient of relative risk aversion y. The elasticity of intertemporal 
substitution + does not appear once consumption has been substituted out 
of the model. This is in striking contrast with the important role played by 
+ in the consumption-based Euler equation (8.3.7). Intuitively, this result 
comes from the fact that + plays two roles in the theory. A low value of m e n t he market return is unforecastable, there are no revisions of expec- 
+ reduces anticipated fluctuations in consumption, but it also increases tations in future returns, so a m h  = 0. In this case the equity premium with 
the risk premium required to compensate for any contribution to these theJ ensen's Inequality adjustment isjust and the coefficient of relative 
fluctuations. These offsetting effects lead + to cancel out of the asset-based risk aversion can be estimated in the manner of Friend and Blume (1975) 
pricing formula (8.3.14). by taking the ratio of the equity premium to the variance of the market 

Third, (8.3.14)e xpresses the risk premium, net of the Jensen's Inequal- return. Using the numbers from Table 8.1, the estimate of risk aversion is 
ity adjustment, as a weighted sum of two terms. The first term is the asset's 0.0575/0.0315 = 1.828. This is the risk-aversion coefficient of an investor 
covariance with the market portfolio; the weight on this term is the coeffi- uith power utility whose wealth is entirely invested in a portfolio with an 
cient of relative risk aversion y. The second term is the asset's covariance unforecastable return, a risk premium of 5.75% per year, and a variance 
with news about future returns on the market; this receives a weight of y - 1. of 0.0315 (standard deviation of 17.74% per year). The consumption of 
When y is less than one, assets that do well when there is good news about such an investor would also have a standard deviation of 17.74% per year. 
future returns on the market have lower mean returns, but when y is greater This is far greater than the volatility of measured aggregate consumption in 



324 8. Intertemporal Equilibrium Models 8.3. Market Frictions 325 

Table 8.1, which explains why the risk-aversion estimate is much lower than It follows that the discounted sum of revisions in forecast returns can be 
the consumption-based estimates discussed earlier. written as 

The Friend and Blume (1975) procedure can be seriously misleading if 
the market return is serially correlated. If high stock returns are associated 
with downward revisions of future returns, for example, then a m h  is negative 
in (8.3.15). With y > 1, this reduces the equity risk premium associated 
with any level of y and increases the risk-aversion coefficient needed to 
explain a given equity premium. Intuitively, when a m h  < 0 the long-run 
risk of stock market investment is less than the short-run risk because the 
market tends to mean-revert. Investors with high y care about long-run risk where cp' is defined to equal ellpA(I - PA)-', a nonlinear function of the 
rather than short-run risk, so the Friend and Blume calculation overstates VAR coefficients. The elements of the vector cp measure the importance of 
risk and correspondingly understates the risk aversion needed to justify the each state variable in forecasting future returns on the market. If a particular 
equity premium. element qk is large and positive, then a shock to variable k is an important 

Campbell (1996a) shows that the estimated coefficient of relative risk piece of good news about future investment opportunities. 
aversion rises by a factor of ten or more if one allows for the empirically es- We now define 
timated degree of mean-reversion in postwar monthly US data. In long-run a i k  Covt [ri,t+ltc k,t+ll, (8.3.19) 
annual US data the effect is less dramatic but still goes in the same direction. 

where ck,,+l is the kth element of €,+I. Since the first element of the state 
Campbell also shows that risk-aversion estimates increase if one allows for vector is the return on the market, ail = aim.T hen (8.3.14) implies that 
human capital as a component of wealth. In this sense one can derive the 
equity premium puzzle without any direct reference to consumption data. 

An Equilibrium Multifactor Asset Pricing Model 
With a few more assumptions, (8.3.14) can be used to derive an equilibrium where qk is the kth element of cp. This is a standard K-factor asset pricing 
multifactor asset pricing model of the type discussed in Chapter 6. We write model of the type discussed in Chapter 6. The contribution of the intertem- 
the return on the market as the first element of a K-element state vector xt+l. poral optimization problem is a set of restrictions on the risk prices of the 
The other elements are variables that are known to the market by the end factors. The first factor (the innovation in the market return) has a risk 
of period t+l and are relevant for forecasting future returns on the market. price of hl = y + (y - l)ql. The sign of q1 is the sign of the correlation 
We assume that the vector xt+l follows a first-order vector autoregression between market return innovations and revisions in expected future market 
(VAR):  returns. As we have already discussed, this sign affects the risk price of the 

market factor; with a negative pl, for example, the market factor risk price 
is reduced if y is greater than one. 

The other factors in this model have risk prices of kk = ( y  - l)qk 
for k > 1. Factors here are innovations in variables that help to forecast 

The assumption that the VAR is first-order is not restrictive, since a higher- the return on the market, and their risk prices are proportional to their 
order VAR can always be stacked into first-order form. forecasting importance as measured by the elements of the vector 9. If a 

Next we define a K-element vector e l ,  whose first element is one and particular variable has a positive value of qk,t his means that innovations in 
whose other elements are all zero. This vector picks out the real stock re- that variable are associated with good news about future investment oppor- 
turn rm,,+lf rom the vector x,+l: rm,t+l = e l ' ~ , +a~n,d  r,,,+l - Et rm,t+l = tunities. Such a variable will have a negative risk price if the coefficient of 

The first-order VAR generates simple multiperiod forecasts of fu- relative risk aversion y is less than one, and a positive risk price if y is greater 
ture returns: than one. 

Campbell (1996a) estimates this model on long-term annual and post- 
World War 11 monthly US stock market data. He estimates to be negative 
and large in absolute value, so that the price of stock market risk A1 is much 



326 8. Intertemporal Equilibrium Models 8.4. More General Utility Functions 327 

smaller than the coefficient of risk aversion y .  The other factors in the q u e d f or the importance of habit formation, a positive effect of today's con- 

model have imprecisely estimated risk prices. Although some of these risk sumption on tomorrow's marginal utility of consumption. Here we discuss 

prices are substantial in magnitude, the other factors have minor effects on some simple ways to implement this idea. 
the mean returns of the assets in the study, because these assets typically Several modeling issues arise at the outset. We write the period util- 

have small covariances with the other factors. ity function as U(Ct,X t), where Xt is the time-varying habit or subsistence 
level. The first issue is the functional form for U(.).  Abel (1990, 1996) has 
p p o s e d  that U(.)s hould be a power function of the ratio Ct/Xt ,  while 

8.4 More General Utility Functions campbell and Cochrane (1995) ,  Constantinides (1990) ,a nd Sundaresan 
(1989) have used a power function of the difference Ct-Xt. The second 

One straightforward response to the difficulties of the standard consump- issue is the effect of an agent's own decisions on future levels of habit. In 
tion CAPM is to generalize the utility function. We have already discussed standard internal-habit models such as those in Constantinides (1990) and 
the Epstein-Zin-Weil model, but there are other plausible ways to vary the Sundaresan (1989), habit depends on an agent's own consumption and 
utility specification while retaining the attractive scale-independence prop- the agent takes account of this when choosing how much to consume. In 
erty of power utility. external-habit models such as those in Abel (1990, 1996) and Campbell and 

For example, the utility function may be nonseparable in consumption Cochrane (1995),h abit depends on aggregate consumption which is unaf- 
and some other good. This is easy to handle in a loglinear model if utility is fected by any one agent's decisions. Abel calls this catching up with theJ oneses. 
CobbDouglas, so that the marginal utility of consumption can be written as The third issue is the speed with which habit reacts to individual or aggre- 

gate consumption. Abel (1990, 1996),  Dunn and Singleton (1986) ,a nd 
Ferson and Constantinides (1991)  make habit depend on one lag of con- 
sumption, whereas Constantinides ( 19 90) ,  Sundaresan (1989) ,C ampbell 

for some good Xt and parameter y2. The Euler equation now becomes and Cochrane (1995),a nd Heaton (1995) make habit react only gradually 
to changes in consumption. 

Ratio Models 
Following Abel (1990, 1996),s uppose that an agent's utility can be written 

Assumingjoint lognormality and homoskedasticity, this can be written as 
as a power function of the ratio Ct /X t ,  

Eichenbaum, Hansen, and Singleton (1988)h ave considered a model of this 
form where Xt is leisure. Aschauer (1985)a nd Startz (1989) have developed 
models in which Xt is government spending and the stock of durable goods, where X, summarizes the influence of past consumption levels on today's 
respectively. Unfortunately, none of these extra variables greatly improve utility. Xt can be specified as an internal habit or as an external habit. Using 
the ability of the consumption C M M  to fit the data. The difficulty is that, one lag of consumption for simplicity, we may have 
at least in data since World War 11, these variables are not noisy enough to 
have much effect on the intertemporal marginal rate of substitution.16 

the internal-habit specification where an agent's own past consumption mat- 
8.4.1 Habit Formation ters, or 

A more promising variation of the basic model is to allow for nonseparabil- 
ity in utility over time. Constantinides (1990) and Sundaresan (1989) have the external-habit specification where aggregate past consumption zt-1 

matters. Since there is a representative agent, in equilibrium the agent's 
Consumption must of course equal aggregate consumption, but the two for- 

'"SO, as Campbell and Mankiw (1990) point out, in postwar data there is ~redictable 
variation in consumption growth that is uncorrelated with predictable variation in real interest mulations yield different Euler equations. In both equations the parameter 
rates even after one allows for predictable variation in leisure, government spending, or durable K governs the degree of time-nonseparability. 
goods. 



328 8. Intertemporal Equilibrium Models 8.4. More General Utility Functions 329 

In the internal-habit specification, the derivation of the Euler equation - logs- y 2 a g / 2 + ( y - ~ ( y-  l ) ) g ,w here gis the average consumption growth 
is complicated by the fact that time-t consumption affects the summation in rate. When risk aversion y is very large, a positive K reduces the average 
(8.4.4) through the term dated t + 1 as well as the term dated t. We have riskless rate. Thus catching up with the Joneses enables one to increase 

risk aversion to solve the equity premium puzzle without encountering the 
riskfree rate puzzle. Second, a positive K is likely to make the riskless real 
interest rate more variable because of the term -~(y-l) Act in (8.4.1 1 ) .  

This is random at time t because it depends on consumption at time t+ l .  If one solves for the stock returns implied by the assumption that stock 
Substituting in for Xt and imposing the condition that the agent's own con- dividends equal consumption, a more variable real interest rate increases 
sumption equals aggregate consumption, this becomes the covariance of stock returns and consumption ai, and drives up the equity 

premium. 
The second of these points can be regarded as a weakness rather than 

If this model is to capture the idea of habit formation, then we need a strength of the model. The equity premium puzzle shown in Table 8.1 is 
that the ratio of the measured equity premium to the measured covariance 

~ ( -y 1 )  p 0 to ensure that an increase in yesterday's consumption in- 
creases the marginal utility of consumption today. The Euler equation can oi, is large; increasing the value ai, implied by a model that equates stock 
now be written as dividends with consumption does not improve matters. Also the real interest 

rate does not vary greatly in the short run; the standard deviation of the ex 
post real commercial paper return in Table 8.1 is 5.5%, and Table 8.2 shows 
that about a third of the variance of this return is forecastable, implying a 

where the expectations operator on the left-hand side is necessary because standard deviation for the expected real interest rate of only 3%. Since the 
of the randomness of a U / aC t. standard deviation of consumption growth is also about 3%, large values of 

The analysis simplifies considerably in the external-habit specification. K and y in equation (8.4.1 1 )  tend to produce counterfactual volatility in 
In this case (8.4.8) and (8.4.9) can be combined to give the expected real interest rate. Similar problems arise in the internal-habit 

model. 
This difficulty with the riskless real interest rate is a fundamental prob- 

lem for habit-formation models. Time-nonseparable preferences make 
If we assume homoskedasticity and joint lognormality of asset returns and marginal utility volatile even when consumption is smooth, because con- 
consumption growth, this implies the following restrictions on risk premia sumers derive utility from consumption relative to its recent history rather 
and the riskless real interest rate: than from the absolute level of consumption. Rut unless the consumption 

ql+l = - logs - ~ * a % /+2 y Et[Act+ll - ~ (-y l ) A c t ,  (8.4.11) and habit processes take particular forms, time-nonseparability also creates 
large swings in expected marginal utility at successive dates, and this implies 
large movements in the real interest rate. We now present an alternative 
specification in which it is possible to solve this problem. 

Equation (8.4.11) says that the riskless real interest rate equals its value 
under power utility, less K ( y  - 1 )A ct. Holding consumption today and ex- Difference Models 
pected consumption tomorrow constant, an increase in consumption yes- Consider a model in which the utility function is 
terday increases the marginal utility of consumption today. This makes the 
representative agent want to borrow from the future, driving up the real 
interest rate. Equation (8.4.12) describing the risk premium is exactly the 
same as (8.2.7), the risk premium formula for the power utility model. The 
external habit simply adds a term to the Euler equation (8.4.10) which is 
known at time t, and this does not affect the risk premium. and for simplicity treat the habit level X, as external. This model differs 

Abel (1990,1996) nevertheless argues that catching up with the Joneses from the ratio model in two important ways. First, in the difference model 
the agent's risk aversion varies with the level of consumption relative to 

can help to explain the equity premium puzzle. This argument is based on 
habit, whereas risk aversion is constant in the ratio model. Second, in the 

two considerations. First, the average level of the riskless rate in (8.4.11 ) is 



330 8. IntertemporalE quilibrium Models 8.4. More General Utility Functions 331 

difference model consumption must always be above habit for utility to be proximately a traditional habit-formation model in which log habit responds 
well-defined, whereas this is not required in the ratio model. slowly to log consumption, 

To understand the first point, it is convenient to work with the surplus 
consumption ratio St, defined by 

The surplus consumption ratio gives the fraction of total consumption that 
where h = ln(1- 3) is the steady state value of x-c. The problem with the 

is surplus to subsistence or habit requirements. If habit Xt is held fixed 
traditional model (8.4.17) is that it allows consumption to fall below habit, 

as consumption Ct varies, the normalized curvature of the utility function, 
resulting in infinite or negative marginal utility. A process for st defined 

which would equal the coefficient of relative risk aversion and would be a 
over the real line implies that consumption can never fall below habit. 

constant y in the conventional power utility model, is 
Since habit is external, the marginal utility of consumption is ul(Ct) = 

(C, - Xt)-Y = SFYc tPYT. he stochastic discount factor is then 

This measure of risk aversion rises as the the surplus consumption ratio St 
declines, that is, as consumption declines toward habit." 

The requirement that consumption always be above habit is satisfied In the standard power utility model St = 1, so the stochastic discount factor 
automatically in microeconomic models with exogenous asset returns and isjust consumption growth raised to the power - y .  To get a volatile stochas- 
endogenous consumption, as in Constantinides (1990) and Sundaresan tic discount factor one needs a large value of y .  In the habit-formation 
(1989). It presents a more serious problem in models with exogenous model one can instead get a volatile stochastic discount factor from a volatile 
consumption processes. To handle this problem Campbell and Cochrane surplus consumption ratio St. 
(1995) specify a nonlinear process by which habit adjusts to consumption, The riskless real interest rate is related to the stochastic discount factor 
remaining below consumption at all times. Campbell and Cochrane write by (1 + R;~) = l/Et (Mt+l).T aking logs, and using (8.4.16) and (8.4.18), 
down a process for the log surplus consumption ratio st = log(St). They the log riskless real interest rate is 
assume that log consumption follows a random walk with drift g and inno- 
vation vt+l, Act+l = g +  vt+l. They propose anAR(1) model for st: 

The first two terms on the right-hand side of (8.4.19) are familiar from the 
Here 5 is the steady-state surplus consumption ratio. The parameter 4 gov- power utility model (8.2.6), while the last two terms are new. The third term 
erns the persistence of the log surplus consumption ratio, while the sensi- (linear in (st - S)) reflects intertemporal substitution, or mean-reversion 
tivity function A(st) controls the sensitivity of st+l and thus of log habit xt+l in marginal utility. If the surplus consumption ratio is low, the marginal 
to innovations in consumption growth vt+l. utility of consumption is high. However, the surplus consumption ratio is 

Equation (8.4.16) specifies that today's habit is a complex nonlinear expected to revert to its mean, so marginal utility is expected to fall in the 
function of current and past consumption. By taking a linear approxima- future. Therefore, the consumer would like to borrow and this drives up the 
tion around the steady state, however, it may be shown that (8.4.16) is ap- equilibrium risk free interest rate. The fourth term (linear in [k(st)+1I2) 

reflects precautionary savings. As uncertainty increases, consumers become 
more willing to save and this drives down the equilibrium riskless interest 

'kisk aversion may also be measured by the normalized curvature of the value function 
(maximized utility expressed as a function of wealth), or by the volatility of the stochastic rate. 
discount factor, or by the maximum Sharpe ratio available in asset markets. While these If this model is to generate stable real interest rates like those observed 
measures of risk aversion are different from each other in this model, they all move inversely in the data, the serial correlation parameter 4 must be near one. Also, the 
with St. Note that y ,  the curvature parameter in utility, is no longer a measure of risk aversion sensitivity function k(st) must decline with st SO that uncertainty is high when 
in this model. 



332 8. Intertemporal Equilibn'um Models 8.4. More General Utility Functions 333 

st is low and the precautionary saving term offsets the intertemporal substitu- Psychological models may best be understood by comparing them to the 
tion term. In fact, Campbell and Cochrane parametrize the h(st) function standard time-separable specification (8.1.1) in which an investor maximizes 
so that these two terms exactly offset each other everywhere, implying a 
constant riskless interest rate. 

Even with a constant riskless interest rate and random-walk consump- 
tion, the external-habit model can produce a large equity premium, volatile 
stock prices, and predictable excess stock returns. The basic mechanism is This specification has three main components: the period utility function 
time-variation in risk aversion. When consumption falls relative to habit, the U(Ct), the geometric discounting with discount factor 6, and the mathemat- 
resulting increase in risk aversion drives up the risk premium on risky assets ical expectations operator Et. Psychological models alter one or more of 
such as stocks. This also drives down the prices of stocks, helping to explain these components. 
why stock returns are so much more volatile than consumption growth or The best-known psychological model of decision-making is probably the 
riskless real interest rates. pospect themy of Kahneman and Tversky (1979) and Tversky and Kahneman 

Campbell and Cochrane (1995) calibrate their model to US data on con- (1992). Prospect theory was originally formulated in a static context, so it 
sumption and dividends, solving for equilibrium stock prices in the tradition does not emphasize discounting, but it does alter the other two elements of 
of Mehra and Prescott (1985). There is also some work on habit formation the standard framework. Instead of defining preferences over consumption, 
that uses actual stock return data in the tradition of Hansen and Single- preferences are defined over gains and losses relative to some benchmark 
ton (1982, 1983). Heaton (1995), for example, estimates an internal-habit outcome. A key feature of the theory is that losses are given greater weight 
model allowing for time-aggregation of the data and for some durability of than gains. Thus if x is a random variable that is positive for gains and 
those goods formally described as nondurable in the national income ac- negative for losses, utility might depend on 
counts. Durability can be thought of as the opposite of habit formation, in 
that consumption expenditure today lowers the marginal utility of consump- 
tion expenditure tomorrow. Heaton finds that durability predominates at 
high frequencies, and habit formation at lower frequencies. However his 
habit-formation model, like the simple power utility model, is rejected sta- Here yl and M are curvature parameters for gains and losses, which may 
tistically. differ from one another, and h > 1 measures the extent of loss aversion, the 

Both these approaches assume that aggregate consumption is the driv- greater weight given to losses than gains. 
ing process for marginal utility. An alternative view is that, for reasons dis- Prospect theory also changes the mathematical expectations operator 
cussed in Section 8.3.2, the consumption of stock market investors may not in (8.4.20). The expectations operator weights each possible outcome by its 
be adequately proxied by macroeconomic data on aggregate consumption. probability; prospect theory allows outcomes to be weighted by nonlinear 
Under this view the driving process for a habit-formation model should be functions of their probabilities (see Kahneman and Tversky (1979)) or by 
a process with a reasonable mean and standard deviation, but need not be nonlinear functions of the probabilities of a better or worse outcome. Other, 
highly correlated with aggregate consumption. more general models of investor psychology also replace the mathematical 

expectations operator with a model of subjective expectations. See for ex- 
ample Barberis, Shleifer, and Vishny (1996) DeLong, Shleifer, Summers, 

8.4.2 Psychological Models of Prejierences and Waldmann (1990b), and Froot (1989). 
Psychologists and experimental economists have found that in experimen- In applying prospect theory to asset pricing, a key question is how the 
tal settings, people make choices that differ in several respects from the benchmark outcome defining gains and losses evolves over time. Benartzi 
standard model of expected utility. In response to these findings unortho- and Thaler (1995) assume that investors have preferences defined over 
dox "psychological" models of preferences have been suggested, and some returns, where a zero return marks the boundary between a gain and a 
recent research has begun to apply these models to asset pricing.'8 loss. Returns may be measured over different horizons; a K-month return 

is relevant if investors update their benchmark outcomes every K months. 
Benartzi and Thaler consider values of K ranging from one to 18. They show 
that loss aversion combined with a short horizon can rationalize investors' 

'XUsefulg eneral references include Hogarth and Reder (1987) and Kreps (1988). 



334 8. Intertemporal Equilibrium Models Problems 335 

unwillingness to hold stocks even in the face of a large equity premium. if it is to explain the cross-sectional pattern of asset returns. We also know 
Bonomo and Garcia (1993) obtain similar results in a consumption-based that the conditional expectation of the stochastic discount factor must be 
model with loss aversion. comparatively stable in order to explain the stability of the riskless real 

In related work, Epstein and Zin (1990) have developed a parametric interest rate. These properties put severe restrictions on the kinds of asset 
version of the choice theory of Yaari (1987). Their specification for period pricing models that can be considered. 
utility displays jirst-order risk aversion-the risk premium required to induce There is increasing interest in the idea that risk aversion may vary over 
an investor to take a small gamble is proportional to the standard deviation time with the state of the economy. Time-varying risk aversion can explain 
of the gamble rather than the variance as in standard theory. This feature the large body of evidence that excess returns on stocks and other risky 
increases the risk premia predicted by the model, but in a calibration ex- assets are predictable. One mechanism that can produce time-varying risk 
ercise in the style of Mehra and Prescott (1985), Epstein and Zin find that aversion is habit formation in the utility function of a representative agent. 
they can fit only about one third of the historical equity premium. But it is also possible that investors appear to have time-varying risk aversion 

Another strand of the literature alters the specification of discounting because they trade on the basis of irrational expectations, or that time- 
in (8.4.20). Ainslie (1992) and Loewenstein and Prelec (1992) have argued varying risk aversion arises from the interactions of heterogeneous agents. 
that experimental evidence suggests not geometric discounting but hyperbolic Grossman and Zhou (1996), for example, present a model in which two 
discounting: The discount factor for horizon K  is not S K  but a function of agents with different risk-aversion coefficients trade with each other. One 
the form (1 + S1K ) - ~ Z /w' ~h,e re both S1 and S2 are positive as in the standard of the agents has an exogenous lower bound on wealth, and the resulting 
theory. This functional form implies that a lower discount rate is used equilibrium has a time-varying price of risk. This is likely to be an active 
for periods further in the future. Laibson (1996) argues that hyperbolic area for future research. 
discounting is well approximated by a utility specification 

8.1 Prove that the benchmark portfolio has the properties (PI) 
through (P5) stated on pages 298 and 300 of this chapter. 

where the additional parameter B < 1 implies greater discounting over 
the next period than between any periods further in the future. 8.2 Consider an economy with a representative agent who has power 

Hyperbolic discounting leads to time-inconsistent choices: Because the utility with coefficient of relative risk aversion y .  The agent receives 
discount rate between any two dates shifts as the dates draw nearer, the a nonstorable endowment. The process for the log endowment, or 
optimal plan for those dates changes over time even if no new information equivalently the log of consumption c,, is 
arrives. The implications for consumption and portfolio choice depend 
on the way in which this timeconsistency problem is resolved. Laibson 
(1996) derives the Euler equations for consumption choice assuming that where the coefficient 4 may be either positive or negative. If 4 is positive 
the individual chooses each period's consumption in that period without then endowment fluctuations are highly persistent; if it is negative then 
being able to constrain future consumption choices. Interestingly, he shows they have an important transitory component. 
that with hyperbolic discounting the elasticity of intertemporal substitution 8.2.1 Assume that consumption and asset returns are jointly log- 
is less than the reciprocal of the coefficient of relative risk aversion even normal, with constant variances and covariances. 
when the period utility function has the power form. i. Use the representative agent's Euler equations to show that 

the expected log return on any asset is a linear function of the 
expected growth rate of the endowment. What is the slope coef- 

8.5 Conclusion ficient in this relationship? 

Financial economists have not yet produced a generally accepted model ii. Use the representative agent's Euler equations to show that 
of the stochastic discount factor. Nonetheless substantial progress has been the difference between the expected log return on any asset and 
made. We know that the stochastic discount factor must be extremely volatile the log riskfree interest rate, plus one-half the own variance of 



336 8. Intertemporal Equilibrium Modek 

the log asset return (call this sum the "premium" on the asset), is 8.3.2 Now suppose that in the second period, with probability 112 
proportional to the conditional covariance of the log asset return all consumers receive m; with probability 112, a fraction (1-6) of 
with consumption growth. What is the slope coefficient in this consumers receive m and a fraction b receive (1- a/b)m. In the 
relationship? first period, all consumers face the same probability of being in the 

latter group, but no insurance markets exist through which they 
8.2.2 To a close approximation, the unexpected return on any can hedge this risk. Compute the expected return on the claim 
asset i can be written as defined above. Is it higher or lower than before? Is it bounded by 

a function of a and m? 

8.3.3 Relate your answer to the recent empirical literature on the 
determination of stock returns in representative-agent models. To 
what extent do your results in parts 8.3.1 and 8.3.2 depend on the 
details of the model, and to what extent might they hold more 
generally? 

where di,ti s the dividend paid on asset i at time t. This approxima- Note: This problem is based on Mankiw (1986). 
tion was developed as (7.1.25) in Chapter 7. 

i. Use this expression to calculate the unexpected return on an 
equity which pays aggregate consumption as its dividend. 

ii. Use this expression to calculate the unexpected return on a 
real consol bond which has a fixed real dividend each period. 

8.2.3 
i. Calculate the equity premium and the consol bond premium. 

ii. Show that the bond premium has the opposite sign to 4 and is 
proportional to the square of y .  Give an economic interpretation 
of this result. 
.. .  
111. Show that the equity premium is always larger than the bond 
premium, and the difference between them is proportional to y .  
Give an economic interpretation of this result. 

iv. Relate your discussion to the empirical literature on the "eq- 
uity premium puzzle." 

8.3 Consider a two-period world with a continuum of consumers. 
Each consumer has a random endowment in the second period and 
consumes only in the second period. In the first period, securities are 
traded but no money changes hands until the second period. All con- 
sumers have log utility over second-period consumption. 

8.3.1 Suppose that all consumers' endowments are the same. They 
are m with probability 112 and (I-a)m with probability 112, where 
0 < a < 1. Suppose that a claim to the second-period aggregate 
endowment is traded and that it costs p in either state, payable in the 
second period. Compute the equilibrium price p and the expected 
return on the claim. 



Derivative Pricing Models 

THE PRICING OF OPTIONS, warrants, and other denuatiue securities-financial 
securities whose payoffs depend on the prices of other securities-is one of 
the great successes of modern financial economics. Based on the well-known 
Law of One Price or no-arbitrage condition, the option pricing models of 
Black and Scholes (1973) and Merton (1973b) gained an almost immediate 
acceptance among academics and investment professionals that is unparal- 
leled in the history of economic science.' 

The fundamental insight of the BlackScholes and Merton models is 
that under certain conditions an option's payoff can be exactly replicated 
by a particular dynamic investment strategy involving only the underlying 
stock and riskless debt. This particular strategy may be constructed to be 
seIf-financing,i .e., requiring no cash infusions except at the start and allowing 
no cash withdrawals until the option expires; since the strategy replicates 
the option's payoff at expiration, the initial cost of this self-financing invest- 
ment strategy must be identical to the option's price, otherwise an arbitrage 
opportunity will arise. This no-arbitrage condition yields not only the o p  
tion's price but also the means to replicate the option synthetically-via the 
dynamic investment strategy of stocks and riskless debt-if it does not exist. 

This method of pricing options has since been used to price literally 
hundreds of other types of derivative securities, some considerably more 
complex than a simple option. In the majority of these cases, the pricing 
formula can only be expressed implicitly as the solution of a parabolic par- 
tial differential equation (PDE) with boundary conditions and initial values 
determined by the contractual terms of each security. To obtain actual 
prices, the PDE must be solved numerically, which might have been prob- 
lematic in 1973 when Black and Scholes and Merton first published their 
Papers but is now commonplace thanks to the breakthroughs in computer 

'see Bernstein (1992, Chapter 11) for a lively account of the intellectual history of the 
Black-~choles/~ertoonp tion pricing formula. 



340 9. Derivative Pricing Models 9.1. Brownian Motion 341 

technology over the past three decades. Although a detailed discussion of make the most economical use of computational resources. We turn to 
derivative pricing models is beyond the scope of this text-there are many these issues in Section 9.4. 

other excellent sources such as Cox and Rubinstein (1985), Hull (1993), 
and Merton (1990)-we do provide brief reviews of Brownian motion in 
Section 9.1 and Merton's derivation of the Black-Scholes formula in Section 9.1 Brownian Motion 
9.2 for convenience. 

Ironically, although pricing derivative securities is often highly computa- For certain financial applications it is often more convenient to model prices 
tion-intensive, in principle it leaves very little room for traditional statistical as evolving continuously through time rather than discretely at fixed dates. 
inference since, by the very nature of the no-arbitrage pricing paradigm, For example, Merton's derivation of the Black-Scholes option-pricing for- 
there exists no "error term" to be minimized and no corresponding statis- mula relies heavily on the ability to adjust portfolio holdings continuously in 
tical fluctuations to contend with. After all, if an option's price is deter- time so as to replicate an option's payoff exactly. Such a replicating portfo- 
mined exactly-without error-as some (possibly time-varying) combina- lio strategy is often impossible to construct in a discrete-time setting; hence 
tion of prices of other traded assets, where is the need for statistical infer- pricing formulas for options and other derivative securities are almost always 
ence? Methods such as regression analysis do not seem play a role even in derived in continuous time (see Section 9.2 for a more detailed discu~sion).~ 
the application of option pricing models to data. 

However, there are at least two aspects of the implementation of deriva- 9.1.I  Constructing Brownian Motion 
tive pricing models that do involve statistical inference and we shall focus 
on them in this chapter. The first aspect is the problem of estimating the pa- The first formal mathematical model of financial asset prices-developedby 
rameters of continuous-time price processes which are inputs for parametric Bachelier (1900) for the larger purpose of pricing warrants trading on the 
derivative pricing formulas. We use the qualifier parametric in describing the Paris Bourse-was the continuous-time random walk, or Brownian motion. 
derivative pricing formulas considered in this chapter because several non- Therefore, it should not be surprising that Brownian motion plays such a 
parametric approaches to pricing derivatives have recently been proposed central role in modern derivative pricing models. This continuous-time 
(see, for example, Ait-Sahalia [1992], Ait-Sahalia and Lo [1995], Hutchin- process is closely related to the discrete-time versions of the random walk 
son, Lo, and Poggio [1994], and Rubinstein [1994]).2 We shall consider described in Section 2.1 of Chapter 2, and we shall take the discrete-time 
nonparametric derivative pricing models in Chapter 12, and focus on issues random walk as a starting point in our heuristic construction of the Brownian 
surrounding parametric models in Section 9.3. m ~ t i o n .O~u r goal is to use the discrete-time random walk to define a 

The second aspect involves the pricing of pathdependent derivatives sequence of continuous-time processes which will converge to a continuous- 
by Monte Carlo simulation. A derivative security is said to be path-dependent time analog of the random walk in the limit. 
if its payoff depends in some way on the entire path of the underlying asset's 
price during the derivative's life. For example, a put option which gives The Discrete-Time Random Walk 
the holder the right to sell the underlying asset at its average price-where Denote by { p k }  a discrete-time random walk with increments that take on 
the average is calculated over the life of the option-is pathdependent only two values, A and -A: 
because the average price is a function of the underlying asset's entire price 
path. Although a few analytical approximations for pricing ~athdependent  A with probability IT 
derivatives do exist, by far the most effective method for pricing them is k - + i k  =  { (9.1.1) 

-A with probability n' 1-  n, 
by Monte Carlo simulation. This raises several issues such as measuring 
the accuracy of simulated prices, determining the number of simulations where i k i s IID (hence p k  follows the Random Walk 1 model of Section 2.1.1 of 
required for a desired level of accuracy, and designing the simulations to Chapter 2), and po is fixed. Consider the following continuous-time process 

pn(t), t E [0, TI, which can be constructed from the discrete-time process 
'1n contrast to the traditional parametric approach in which the price process of the 

underlying asset is fully specified up to a finite number of unknown parameters, e.g.,a  lognor- 3 ~ h e raer e two notable exceptions: the equilibrium approach of Rubinstein (1976), and 
mal diffusion with unknown drift and volatility parameters, a nonparametric approach does the binomial option-pricing model of Cox, Ross, and Rubinstein (1979). 
not specify the price process explicitly and attempts to infer it from the data under suitable 4 ~ oar m ore rigorous derivation, see Billingsley (1968, Section 37). 
regularity conditions. 



9. Da'vative Pricing Models 9. I .  Brownian Motion 343 

crease without bound if A,n ,a nd n' are held fixed. TOo btain a well-defined 
and nondegenerate limiting process, we must maintain finite moments for 
p,(T) as n approaches infinity. In particular, since we wish to obtain a 
continuous-time version of the random walk, we should expect the mean 
and variance of the limiting process p(T)  to be linear in T ,a s in (2.1.5) and 
(2.1.6) for the discrete-time random walk. Therefore, we must adjust A,n , 
and n' so that: 

and this is accomplished by setting: 

Figure 9.1. Sample Path of a Discrete-Time Random Walk 

(ph} ,k  = 1, . . . , n as follows: Partition the time interval [0, TI into n pieces, The adjustments (9.1.7) imply that the step size decreases as n increases, 
each of length h = T/  n, and define the process but at the slower rate of I/& or &. The probabilities converge to i,a lso 

at the rate of &,a nd hence we write: 

where [x] denotes the greatest integer less than or equal to x. p,(t) is a 
rather simple continuous-time process, constant except at times t = kh, 
k = 1, . . . , n (see Figure 9.1). 

where '(A)de notes terms that are of the same asymptotic order as 
Although p,(t) is indeed a well-defined continuous-time process, it is 

Therefore, as n increases, the random walk p,(T)  varies more frequently 
still essentially the same as the discrete-time process p h  since it only varies at on [0, TI,  but with smaller step size A and with upstep and down-step 
discrete points in time. However, if we allow n to increase without bound probabilities approaching i.  
while holding T fixed (hence forcing h to approach 0), then the number 
of points in [0, TI at which p,(t) varies will also increase without bound. 
If, at the same time, we adjust the step size A and the probabilities n and 5~ function f(h) is said to be of the same asymptotic order as g(h)-denoted by f (h) - 

O(g(h))-if the following relation is satisfied: 
n' appropriately as n increases, then p,(t) converges in distribution to a 
well-defined nondegenerate continuous-time process p(t)  with continuous 0 < lim-f (h) 

< oo. 
sample paths on [0, TI. h+O g(h) 

To see why adjustments to A,n ,a nd n' are necessary, consider the mean *function f (h) is said to be of smaller asymptotic order asg(h)-denoted by f ( h )-  o(g(h))-if 

and variance of p,(T): lim f- (h) = 0 
h-0 g(h) 

Some examples of asymptotic order relations are 

As n increases without bound, the mean and variance of p,(T) will also in- 



344 9. Derivative Pricing Models 9. I .  Brownian Motion 

The Continuous-Time Limit 
By calculating the moment-generating function of p,(T) and taking its limit 
as n approaches infinity, it may be shown that the distribution of p (T )  i' s nor- 
ma1 with mean p T and variance a 2T ;  thus p,(T) converges in distribution 
to a hl (pT , o 2T ) r andom variable. 

In fact, a much stronger notion of convergence relates p,(T) to p(T) ,  
which involves the Jinite-dimensional distributions (FDDs) of the two stochas- 
tic processes. An FDD of a stochastic process p(t) is the joint distribu- 
tion of any finite number of observations of the stochastic process, i.e., 
f (P(t l )p,  (&),. . . , p(tm))w,  here 0 5 tl < . . . < tm 5 T .  It can be shown that 
all the FDDs of the stochastic process p,(t) (not just the random variable 
p,(T)) converge to the FDDs of a well-defined stochastic process ~ ( t )Th. i~s  
implies, for example, that the distribution of p,(t) converges to the distri- 
bution of p(t) for all t E [0, TI (notjust at T ) , t hat the joint distribution of 
{p,(O), pn(3),p ,(7)) converges to the joint distribution of {P(O),p (3),p (7)),  
and so on. 

In addition to the normality of p(T) ,  the stochastic process p(t) possesses 
the following three properties: 

(Bl )  For any tl and t2 such that 0 5 tl < t2 5 T: 
Figure 9.2. Sample Path and Conditional Expectation of a Bmwnian Motion with Dn'ft 

(B2) For any tl, t2, t3, and t4 such that 0 5 tl < t2 5 t3 < t4 T ,  
the increment p(t2)-p(tl) is statistically independent of the increment 
P(h)-P(t3). 

(B3) The sample paths of p(t) are continuous. 

It is a remarkable fact that p(t), which is the celebrated arithmetic Brownian 
motion or Wienerprocess,i s completely characterized by these three properties. 
If we set p = 0 and o = 1, we obtain standard Brownian motion which we AS for the discrete-time random walk, the conditional mean and variance 
shall denote by B(t). Accordingly, we may re-express p(t) as of p( t )  are linear in t (see Figure 9.2). Properties (B2) and (B3) of Brow- 

nian motion imply that its sample paths are very erratic and jagged-if 
p(t) = ,ut + oB( t ) ,  t E [ O ,  T I .  (9.1.10) they were smooth, the infinitesimal increment B(t+h)-B(t) would be pre- 

dictable by B(t)-B(t-h), violating independence. In fact, observe that the 
To develop further intuition for p(t), consider the following conditional ratio ( ~ ( t + h-)~ ( t ) ) /dhoe s not converge to a well-defined random variable 
moments: as h approaches 0 ,s ince 

Var [ B ( t + h L - B ( t ) ]  - 0-h2  '  (9.1.16) 

 he Therefore, the derivative of Brownian motion, B1(t),d oes not exist in the 
convergence of FDDs, coupled with a technical conditional called tightness, is called 

weak convergence, a powerful tool in deriving asymptotic approximations for the statistical laws ordinary sense, and although the sample paths of Brownian motion are 
of many nonlinear estimators. See Billingsley (1968) for further details. everywhere continuous, they are nowhere differentiable. 



346 9. Derivative Pricing Mo&ls 9.1. Brownian Motion 347 

9.1.2 Stochastic Dqferential Equations lim E[(B(t + h) - B(t)14]-  h' 
h-d t  I 

Despite this fact, the infinitesimal increment of Brownian motion, i.e., the = o(dt) (9.1.22) 
limit of B(t+h)-B(t) as h approaches an infinitesimal of time (dt), has 
earned the notation dB(t) with its own unique interpretation because it has E[(dB)(dt)] = lim E[(B(t + h) - B(t))h] = 0 

h - + d l  (9.1.23) 
become a fundamental building block for constructing other continuous- Var[(dB)(dt)] = lim E[(B(t + h) - ~ ( t ) ) * h=~  ]  ~ ( d t )(.9 .1.24) 
time processes.7 Heuristically, B(t+ h) - B(t) can be viewed as Gaussian white h+ dl  
noise (see Chapter 2) and in the limit as h becomes infinitesimally small, From (Bl) and (9.1.19)-(9.1.20) and we see that dB(t) may be viewed as 
dB(t) is the "continuous-time version" of white noise. a normally distributed random variable with zero mean and infinitesimal 

It is understood that dB(t) is a special kind of differential, a stochastic variance dt. Although a variance of dt may seem like no variance at all, 
differential, not to be confused with the differentials dx and dy of the calcu- recall that we are in a world of infinitesimals-after all, according to (9.1.17) 
lus. Nevertheless, dB(t) does obey some of the mechanical relations that the expected value of dp(t) is pdt-so a variance of dt is not negligble in a 
ordinary differentials satisfy. For example, (9.1.10) is often expressed in relative sense. 
differential form as: However, a variance of (dt)* is negligible in a relative sense-relative to 

dp(t) = pdt + a dB(t). (9.1.17) dt-since the square of an infinitesimal is much smaller than the infinitesi- 
mal itself. If we treat terms of order o(dt) as essentially zero, then (9.1.21)- 

However, (9.1.17) cannot be treated as an ordinary differential equation, (9.1.24) shows that (dB)2 and (dB)(dt) are both non-stochastic (since the 
and is called a stochastic differential equation to emphasize this fact. For ex- variances of the right-hand sides are of order o(dt)) hence the relations 
ample, the natural transformation dp(t)/dt = p + dB(t)/dt does not make ( d ~ )=*  dt and (dB)(dt) = 0 are satisfied notjust in expectation but exactly. 
sense because dB(t)/dt is not a well-defined mathematical object (although This yields the well-known multiplication rules for stochastic differentials 
dB(t) is, by definition). summarized in Table 9.1. To see why these rules are useful, observe that we 

Indeed, thus far the symbols in (9.1. l7) have little formal content be- 
yond their relation to (9.1.10)--one set of symbols has been defined to be 
equivalent to another. To give (9.1.17) independent meaning, we must de- Table 9.1. Multiplication rules fm stochastic diffientials. 
velop a more complete understanding of the properties of the stochastic 
differential dB(t). For example, since dB is a random variable, what are its 
moments? How do ( d ~a)nd~ (d B)(dt) behave? To answer these questions, 
consider the definition of dB(t): 

dB(t) lim B(t + h) - B(t) (9.1.18) 
h+ d l  

can now calculate (dp)': 
and recall from (Bl) that increments of Brownian motion are normally dis- 
tributed with zero mean (since p = 0) and variance equal to the differencing ( d ~ )=~  (pdt +  ad^)* (9.1.25) 
interval h (since a = 1). Therefore, we have = p2(dt)*+  a 2 ( d ~ )+2 2 pa (dB)(dt) 

= a2dt. (9.1.26) 
E[dB] = lim E[B(t + h) - B(t)] = 0 (9.1.19) 

h+ dl  This simple calculation shows that although dp is a random variable, (dp)' 
Var[dB] = lim E[(B(t + h) - ~ ( t ) ) * l=  dl (9.1.20) is not. It also shows that dp does behave like a random walk increment in 

h+ dl  that the variance of dp is proportional to the differencing interval dt. 
E[(dB)(dB)] = lim E[(B(t 4- h) - B(~))*I=  dt (9.1.21) 

h+ dl Geometric Brownian Motion 
If the arithmetic Brownian motion p(t) is taken to be the price of some 

'A complete and rigorous exposition of Brownian motion and stochastic differential equa- asset, Property (Bl) shows that price changes over any interval will be nor- 
tions is beyond the scope of this text. Hoel, Port, and Stone (1972, Chapters 4-6). Merton mally distributed. But since the support of the normal distribution is the 
(1990, Chapter 3). and Schus (1980) provide excellent coverage of this material. 



348 9. Derivative Pricing Models 9.2. A Brief Reuiew of Derivative Pricing Methods 349 

entire real line, normally distributed price changes imply that prices can 
be negative with positive probability. Because virtually all financial assets 
enjoy limited liability-the maximum loss is capped at -100% of the total 
investment-negative prices are empirically implausible. 

As in Sections 1.4.2 of Chapter 1 and 2.1.1 of Chapter 2 ,  we may elimi- 
nate this problem by defining p(t)  to be the natural logarithm of price P ( t ) .  In contrast to arithmetic Brownian motion (9.1.17),w e see from (9.1.29) 
Under this definition, p(t)  can be an arithmetic Brownian motion without that the instantaneous mean and standard deviation of the geometric Brow- 
violating limited liability, since the price P( t )  = ep(t)i s always non-negative. nian motion are proportional to P. Alternatively (9.1.29) implies that the 
The price process P ( t )  = eP(I)i s said to be a geometric Brownian motion or lognor- instantaneous percentage price change d P / Pb ehaves like an arithmetic Brow- 
mal diffusion. We shall examine the statistical properties of both arithmetic nian motion or random walk, which of course is precisely the case given the 
and geometric Brownian motion in considerably more detail in Section 9.3. exponential transformation. 

We provide a considerably less trivial example of the power of It6's 
It6 S Lemma Lemma in Section 9.2: Merton's derivation of the Black-Scholes formula. 
Although the first complete mathematical theory of Brownian motion is due 
to Wiener ( 1 9 2 3 ) ,it~ i s the seminal contribution of It6 (1951)t hat is largely 
responsible for the enormous number of applications of Brownian motion to 9.2 A Brief Review of Derivative Pricing Methods 
problems in mathematics, statistics, physics, chemistry, biology, engineering, 
and of course, financial economics. In particular, It6 constructs a broad Although we assume that readers are already familiar with the theoretical 
class of continuous-time stochastic processes based on Brownian motion- aspects of pricing options and other derivative securities, we shall provide a 
now known as It6processes or It6 stochastic dijferential equationewhich is closed very brief review here, primarily to develop terminology and define notation. 
under general nonlinear transformations; that is, an arbitrary nonlinear Our derivation is deliberately terse and we urge readers unfamiliar with 
function f ( p ,  t )  of an It6 process p and time t is itself an It6 process. these models to spend some time with Merton's (1990,C hapter 8 ) d efinitive 

More importantly, It6 ( 195 1 ) provides a formula-It6S Lemma-for cal- treatment of the subject. Also, for expositional economy we shall confine 
culating explicitly the stochastic differential equation that governs the dy- our attention to plain vanilla options in this chapter, i.e., simple call and put 
namics off ( p ,  t ) :  options with no special features, and the underlying asset is assumed to be 

common stock.g 
Denote by G(P( t ) ,t )  the price at time t of a European call option with 

strike price X and expiration date T > t on a stock with price P( t )  at time 
t.1° Of course, G also depends on other quantities such as the maturity date 
T,  the strike price X, and other parameters but we shall usually suppress 

The modest term "lemma" hardly does justice to the wide-ranging impact these arguments except when we wish to focus on them specifically. 
(9.1.27) has had; this powerful tool allows us to quantify the evolution of However, expressing G as a function of the current stock price P( t ) ,a nd 
complex stochastic systems in a single step. For example, let p denote the not of past prices, is an important restriction that greatly simplifies the task 
log-price process of an asset and suppose it satisfies (9.1.17);w hat are the of finding G (in Section 9.4 we shall consider options that do not satisfy 
dynamics of the price process P ( t )=  eP(t)?I t6's Lemma provides us with an 
immediate answer: 

 o ow ever, the techniques reviewed in this section have been applied in similar fashion 
to literally hundreds of other types of derivative securities, hence they are considerably more 
general than they may appear to be. 

'O~ecallt hat a call option gives the holder the right to purchase the underlying asset for 
X and a put option gives the holder the right to sell the underlying asset for X. A Eumpean 
Option is one that can be exercised only on the maturity date. An Ama'can option is one that 
Can be exercised on or b e e t he maturity date. For simplicity we shall deal only with European 

?See Jerison, Singer, and Stroock (1996) for an excellent historical retrospective of Options in this chapter. See Cox and Rubinstein (1985), Hull (1993). and Merton (1990) for 
Wiener's research which includes several articles about Wiener's influence on modern financial institutional details and differences between the pricing of American and European options. 
economics. 



350 9. Derivative Pricing Models 9.2. A Brief Review of Derivative Pricing Methods 35 1 

this restriction). In addition, Black and Scholes (1973) make the following is to set pg equal to some "required" rate of return r,, one that comes from 
assumptions: equilibrium considerations of the corresponding risk of the option, much 

like the CAPM (see Chapter 5 ) .  If such an ro can be identified, the condition 
(Al )  There are no market imperfections, e.g., taxes, transactions costs, shortsales pg = r, reduces to a PDE which, under some regularity and boundary condi- 

constraints, and trading is continuous and frictionless. tions, possesses a unique solution. This is the approach taken by Black and 
Scholes (1973). I1 However, this approach requires more structure than we 

(A2) There is unlimited riskless borrowing and lmding at the continuously com- have imposed in (A1)-(A4)-a fully articulated dynamic model of economic 
pounded rate of return r; hence a $1 inuestment in such an asset over the time equilibrium in which ro can be determined explicitly must be specified. 
interval t grows to $1 . err. Alternatively, i fD(t )i s the date t price of a discount bond Merton (1973b) presents an alternative to the equilibrium approach 
maturing at date T with face value $1, then for t E [ O ,  TI the bond price dynamics of Black and Scholes (1973) in which the same option-pricing formula is 
are given by obtained but without any additional assumptions beyond (A1)-(A4) .  He 

dD(t) = rD(t) dt. (9.2.1) does this by constructing a portfolio of stocks, options and riskless bonds 
that requires no initial net investment and no additional funds between 0 

(A3) The stockprice dynamics aregiven by ageometric Brownian motion, the solution and T ,a  seIf;financing portfolio where long positions are completely financed 
to the following It6 stochastic differential equation on t E [O, TI : by short positions. Specifically, denote by Ip(t)t he dollar amount invested in 

the stock at date t ,  Id( t )t he dollar amount invested in riskless bonds at date 
t which mature at date T ,  and Ig(t)  the dollar amount invested in the call 

where B(t)  is a standard Brownian motion, and at least one investor obserues a option at date t. Then the zero net investment condition may be expressed 
without error as: 

Ip(t)+  Id(t) + Ig(t)  = 0 ,  V t  E 10, T I .  (9.2.6) 
(A4) There is no arbitrage. Portfolios satisfymg (9.2.6) are called arbitrage portfolios. Merton (1969) 

shows that the instantaneous dollar return d l  to this arbitrage portfolio is: 
9.2.1 The Black-Scholes and Merton Approach 

The goal is to narrow down the possible expressions for G, with the hope of 
obtaining a specific formula for it. Black and Scholes (1973) and Merton where the stochastic differentials dP and dD are given in (9.2.2) and (9.2.1) 
(197313) accomplish this by considering relations among the dynamics of respectively, and dG follows from It6's Lemma: 
the option price G, the stock price P, and the riskless bond D. To do this, 
we first derive the dynamics of the option price by assuming that G is only a 
function of the current stock price P and t itself and applying It6's Lemma 
(see Section 9.1.2 and Merton [1990,C hapter 31) to the function G(P( t ) ,t ) ,  
which yields the dynamics of the option price: 

Substituting the dynamics of P ( t ) ,B ( t ) ,a nd G(t)i nto (9.2.7) and imposing 
where (9.2.6) yields 

"1n particular, Black and Scholes (1973) assume that the CAPM holds, and obtain r,, by 
appealing to the security-market-line relation which links expected returns to beta. However, 
the CAPM is not a dynamic model cf equilibrium returns and there are some subtle but signif- 
icant inconsistencies between the CAPM and continuous-time option-pricing models (see, for 

Unfortunately, this expression does not seem to provide any obvious restric- example, Dybvig and Ingersoll [1982]). Nevertheless, Rubinstein (1976) provides a dynamic 
equilibrium model in which the Black-Scholes formula holds. 

tions that might allow us to narrow down the choices for G. One possibility 



352 9. Derivative Pricing Models 9.2. A Brief Review of Derivative Pricing Methods 353 

Now let us further restrict this arbitrage portfolio to be completely riskless in where @(.) is the standard normal cumulative distribution function, and 
the sense that its return is nonstochastic on [ 0 ,T I .  This may be guaranteed T- t  is the time-to-maturity of the option. 
by choosing Ig = I; and Ip = I ,  so that The importance of assumptions ( A l ) a nd (A3)  should now be appar- 

ent: it is the combination of Brownian motion (with its continuous sample 
paths) and the ability to trade continuously that enables us to construct 
a perfectly hedged portfolio. If either of these assumptions failed, there 

implying that would be occasions when the return to the arbitrage portfolio is nonzero 
and stochastic, i.e., risky. If so, the arbitrage argument no longer applies. 

Merton's derivation of the Black-Scholes formula also showcases the 
power of Id's Lemma, which gives us the dynamics (9.2.8)o f the option 
price G that led to the PDE (9.2.15).  In a discrete-time setting, obtaining 

for every t E [ 0 ,T I ,  where n;(t) and nl ( t )  are the number of shares of the dynamics of a nonlinear function of even the simplest linear stochastic 
the stock and the option, respectively, held in the self-financing zero-risk process is generally hopeless, and yet this is precisely what is required to 
portfolio. Note that unless aG( t ) /aPi s constant through time, both n;(t) construct a perfectly hedged portfolio. 
and ni( t )  must time-varying to ensure that this portfolio is riskless at all More importantly, the existence of a self-financing perfectly hedged 
times. Such a portfolio is said to be perfectly hedged and -3 G( t ) /aPi s known portfolio of options, stocks, and bonds implies that the option may be syn- 
as the hedge ratio. thetically replicated by a self-financing dynamic trading strategy consisting 

Imposing (9.2.6)a nd (9.2.13)f or all t E [0 ,  TI yields a dynamic portfolio of only stocks and bonds. The initial cost of setting up this replicating port- 
strategy that is riskless and requires no net investment. But then surely its folio of stocks and bonds must then equal the option's price to rule out 
nonstochastic return d l  must be zero, otherwise there would exist a costless, arbitrage because the replicating portfolio is self-financing and duplicates 
riskless dynamic portfolio strategy that yields a positive return. Therefore, the option's payoff at maturity. The hedge ratio (9.2.13)p rovides the recipe 
to avoid arbitrage, it must be the case that for implementing the replicating strategy. 

Option Sensitivities 

where a time argument has been added to p g ( . )t o emphasize the fact that The sensitivities of G to its other arguments also play crucial roles in trading 

it varies through time as well. and managing portfolios of options, so much so that the partial derivatives1' 

Surprisingly, this simple no-arbitrage condition reduces the possible of G with respect to its arguments have been assigned specific Greek letters 

choices of G to just one expression which is a second-order linear parabolic in the parlance of investment professionals and are now known collectively 

PDE for G: as option sensitivities or, less formally, as the "~reeks" : '~  

aG 
Delta A -= -a  (9.2.19) 

P 
subject to the following two boundary conditions: G ( P ( T ) ,T )  = 
Max[P(T)-X,  01, and G(0,t )  = 0.  The unique solution is, of course, the a2G 
Black-Scholes formula: Gamma r = -

ap2 

aG 
Theta O = - 

at 

 he term "partial derivatives" in this context refers, of course, to instantaneous iates of 
change. This is unfortunate coincidence of terminology is usually not a source of confusion, 
but readers should beware. 

19 Of course, "vega" is not a Greek letter and V is simply a script V. 



9. Derivative Pricing Models 9.3. ImpkntzngP arametric Option Pricing Models 355 

aG 
Rho R - -a  (9.2.22) same expected rate of return which, under assumption (A2), must equal 

r the riskless rate r. This fundamental insight, due to Cox and Ross (1976), 
simplifies the computation of option-pricing formulas enormously because 

aG 
Vega V - -. in a risk-neutral economy the option's price is simply the expected value of 

aa its payoff discounted at the riskless rate: 

For the Black-Scholes formula (9.2.16), these option sensitivities can be 
evaluated explicitly: 

However, the conditional expectation in (9.2.29) must be evaluated with 
respect to the random variable P*(T), not P(T), where P*(T) is the terminal 
stock price adjusted for risk-neutrality. 

Specifically, under assumption (A3), the conditional distribution of 
P(T) given P(t) is simply a lognormal distribution with E [log P(T) I P(t)] = 
logP(t) + ( p-  $)(T-t) and Var[logP(T) / P(t)] = 0 2 ( ~ - t ) .U nder risk- 
neutrality, the expected rate of return for all assets must be r, and hence the 
conditional distribution of the ~sk-neutralizedte rminal stock price P*(T) is 
also lognormal, but with E[logP(T) ( P(t)] = log P(t) + ( r  - $)(T-t) and 
Var[logP(T) I P(t)] = a 2 ( ~ - t ) .  

For obvious reasons, this procedure is called the risk-neutral pricing 
method and under assumptions (Al) through (A4), the expectation in 
(9.2.29) may be evaluated explicitly as a function of the standard normal 

where @(.)i s the standard normal probability density function. We shall CDF and yields the Black-Scholes formula (9.2.16). 
have occasion to consider these measures again once we have developed To emphasize the generality of the risk-neutral pricing method in valu- 
methods for estimating option prices in Section 9.3.3. ing arbitrary payoff streams, (9.2.29) is often rewritten as 

9.2.2 The Martingale Approach 

Once Black and Scholes (1973) and Merton (1973b) presented their option- where the asterisk in ET indicates that the expectation is to be taken with re- 
pricing models, it quickly became apparent that their approach could be spect to an adjusted probability distribution, adjusted to be consistent with 
used to price a variety of other securities whose payoffs depend on the prices risk-neutrality. In a more formal setting, Harrison and Kreps (1979) have 
of other securities: Find some dynamic, costless self-financing portfolio shown that the adjusted probability distribution is precisely the distribution 
strategy that can replicate the payoff of the derivative, and impose the no- under which the stock price follows a martingale; thus they call the adjusted 
arbitrage condition. This condition usually reduces to a PDE like (9.2.15), distribution the equivalent martingale measure. Accordingly, the risk-neutral 
subject to boundary conditions that are determined by the specific terms of pricing method is also known as the martingalepricingt echnique. We shall ex- 
the derivative security. ploit this procedure extensively in Section 9.4 where we propose to evaluate 

It is an interesting fact that pricing derivative securities along these expectations like (9.2.30) by Monte Carlo simulation. 
lines does not require any restrictions on agents' preferences other than 
nonsatiation, i.e., agents prefer more to less, which rules out arbitrage op- 
portunities. Therefore, the pricing formula for any derivative security that 9.3 Implementing Parametric Option Pricing Models 
can be priced in this fashion must be identical for all preferences that do 
not admit arbitrage. In particular, the pricing formula must be the same Because there are so many different types of options and other derivative 
regardless of agents' risk tolerances, so that an economy composed of risk- securities, it is virtually impossible to describe a completely general method 
neutral investors must yield the same option price as an economy composed for implementing all derivative-pricing formulas. The particular features 
of risk-averse investors. But under risk-neutrality, all assets must earn the of each derivative security will often play a central role in how its pricing 



356 9. Derivative Pricing Models 9.3. Implementing Parametric Option Pricing Models 357 

formula is to be applied most effectively. But there are several common In this case, the parameter vector 0 consists of only two elements, the con- 
aspects to every implementation of a parametric option-pricing model-a stants cu and j3.I4 
model in which the price dynamics of the underlying security, called the In the more general case the functions a(P,  t; a)a nd b(P, t; P)  must be 
fundamental asset, is specified up to a finite number of parameters-and we restricted in some fashion so as to ensure the existence of a solution to the 
shall focus on these common aspects in this section. stochastic differential equation (9.3.1) (see Arnold [1974], for example). 

To simplify terminology, unless otherwise stated we shall use the term Also, for tractability we assume that the coefficient functions only depend 
option to mean any general derivative security, and the term stock to mean the on the most recent price P ( t ) ;  hence the solution to (9.3.1) is a Markov 
derivative security's underlying fundamental asset. Although there are cer- process. This assumption is not as restrictive as it seems, since non-Markov 
tainly aspects of some derivative securities that differ dramatically from those processes can often be re-expressed as a vector Markov process by expansion 
of standard equity options and cannot be described in analogous terms, they of the states, i.e., by increasing the number of state variables so that the col- 
need not concern us at the current level of generality. After developing a lection of prices and state variables is a vector-Markov process. In practice, 
coherent framework for implementing general pricing formulas, we shall however, expanding the states can also create intractabilities that may be 
turn to modifications tailored to particular derivative securities. more difficult to overcome than the non-Markovian nature of the original 

price process. 
For option-pricing purposes, what concerns us is estimating 0 ,  since 

9.3.1 Parameter Estimation of Asset Price Dynamics pricing formulas for options on P ( t )  will invariably be functions of some or 
The term "parametric" in this section's title is meant to emphasize the re- all of the parameters in 0 .  In particular, suppose that an explicit expression 
liance of a class of option-pricing formulas on the particular assumptions for the option-pricing function exists and is given by G(P( t ) ,0 )w here other 
concerning the fundamental asset's price dynamics. Although these rather dependencies on observable constants such as strike price, time-to-maturity, 
strong assumptions often yield elegant and tractable expressions for the and the interest rate have been su.p. p ressed for notational convenience.I5 An 
option's price, they are typically contradicted by the data, which does not estimator of the option price 6 may then be written as 6 = G ( P ( t ) ,e ),w here 
bode well for the pricing formula's success. In fact, perhaps the most impor- 9 is some estimator of the parameter vector 0 .  Naturally, the properties of 
tant aspect of a successful empirical implementation of any option-pricing 6 are closely related to the properties of e, so that imprecise estimators of 
model is correctly identifying the dynamics of the stock price, and uncer- the parameter vector will yield imprecise option prices and vice versa. To 
tainty regarding these price dynamics will lead us to consider nonparametric quantify this relation between the precision of 6 and of e, we must first 
alternatives in Chapter 12. consider the procedure for obtaining e. 

But for the moment, let us assert that the specific form of the stock 
price process P ( t ) i s known up to a vector of unknown parameters 0 which Maximum Likelihood Estimation 
lies in some parameter space 0,an d that it satisfies the following stochastic The most direct method for obtaining 9 is to estimate it from historical data. 
differential equation: Suppose we have a sequence of n+l historical observations of P ( t )s ampled 

at non-stochastic dates to < tl < . . . < t,, which are not necessarily equally 
spaced. This is an important feature of financial time series since markets 

dP( t )  = a(P,  t;  a)d t + b(P, t;  0 )d B(t ) ,  t E [0 ,T I, (9.3.1) 
are generally closed on weekends and holidays, ~ieldingir regular sampling 
intervals. Since P ( t )  is a continuous-time Markov process by assumption, 

where B(t)  is a standard Wiener process and 0 = [ a '  P' 1' is a ( k x  1) vector irregular sampling poses no conceptual problems; the joint density function 
of unknown parameters. The functions a(P,  t;  a)a nd b(P, t;  0)a re called f of the sample is given by the following product: 
the drift and diffusionfunctions,r espectively, or the coefJicient functions collec- 
tively. For example, the lognormal diffusion assumed by Black and Scholes 
(1973) is given by the following coefficient functions: 

1 4 ~ o tteh at although the drift and diffusion functions depend on distinct parameter vectors 
and /3, these two vectors may contain some parameters in common. 

'"ven if G cannot be obtained in closed-form,0  is a necessary input for numerical solutions 
of G and must still be estimated. 



358 9. Derivative Pricing Models 9.3. Implementing Parametric Option Pricing Models 359 

where Pk = P(tk) ,  fo(Po) is the marginal density function of Po, and a function of Pk and tk; to emphasize this, we drop the subscript k from the 
f (Pk,tk  I Pk-1, tk-1; 8 )  is the conditional density function of Pk given PkPl ,  arguments and write fk(P, t I Pk-1, tk-1). Then it follows from the Fokker- 
also called the transition density function. For notational simplicity, we will Planck or fonvard equation that fk must satisfy the following (see Lo [I9881 
write f (Pk,tk  I Pk-1, tk-1; 8)s imply as f k .  for a derivation):  

Given (9.3.4) and the observations Po, . . . , Pn, tbe parameter vector 
may be estimated by the method of maximum lihlihood (see Section A.4 
of the Appendix and Silvey [1975, Chapter 41). To define the maximum 
likelihood estimator e, let L ( 8 )d enote the log-likelihood function, the natural with initial condition 
logarithm of the joint density function of Po, . . . , P, viewed as a function 
of 8 :  n 

where li(P-Pk-1) is the Diracdelta function centered at Pk-1. Maximum 
likelihood estimation is feasible whenever (9.3.10) can be solved explicitly 

The maximum likelihood estimator is then given by 

8 - and this depends on how complicated the coefficient functions a and b are. 
argmax L ( 8 ) .  Once f k  is obtained, 9 can be computed numerically given the data 

8 6 0  Po, . . . , P,. To interpret this estimator, we must check that the regularity 
conditions for the consistency and asymptotic normality of 8 are satisfied. 

Under suitable regularity conditions, e is consistent and has the following In some cases of interest they are not. For example, a lognormal diffusion 
normal limiting distribution: dP = pP dt + U P  dBviolates the stationarity requirement. But in this case, a 

simple log-transformation of the data does satisfy the regularity conditions: 
11, r2, . . . , r,, where Q = l~gPk/Pk- l ,is  a stationary sequence. Maximum 
likelihood estimation of 8 may then be performed with (rkJW e shall return 

where Z(8)  is called the information matrix. When n is large, the asymptotic to this example in Section 9.3.2. 

distribution in (9.3.7) allows us to approximate the variance of e as 
GMM Estimation 
For many combinations of coefficient functions a and b, (9.3.10) cannot 
be solved explicitly, hence for these cases maximum likelihood estimation is 
infeasible. An alternative proposed by Hansen and Scheinkman (1995) is to 

and the information matrix Z ( 8 )  may also be estimated in the natural way: apply Hansen's (1982) generalized method of moments (GMM) estimator, 
which they extend to the case of strictly stationary continuous-time Markov 
processes (see the Appendix for an exposition of GMM). 

The focus of any GMM procedure is, of course, the moment conditions 
Moreover, e has been shown to be asymptotically efficient in the class of all in which the parameter vector 8 is implicitly defined. The GMM estimator 

consistent and uniformly asymptotically normal (CUAN) estimators; that is that parameter vector e that minimizes the "distance" between the sample 
is, it has the smallest asymptotic variance of all estimators that are CUAN, moment conditions and their population counterparts. 
hence it is the preferred method of estimation whenever feasible. Of course, The properties of a GMM estimator depend critically on the choice of 
maximum likelihood estimation is only feasible when the likelihood func- moment conditions and the distance metric, and for standard discrete-time 
tion can be obtained in closed form which, in our case, requires obtaining GMM applications, these two issues have been studied quite thoroughly. 
the transition density functions f k  in closed form. Unfortunately, a closed- Moment conditions are typically suggested by the interaction of condition- 
form expression for f k  for arbitrary drift and diffusion functions cannot be ing information and optimality or equilibrium conditions such as Euler 
obtained in general. equations, e.g., the orthogonality conditions of linear instrumental variables 

However, it is possible to characterize f k  implicitly as the solution to a estimation (see the Appendix, Section A.l). In some cases, the optimal 
PDE. In particular, fix the conditioning variables Pk-1 and tk-1 and let f k  be (in an asymptotic sense) distance metric can be deduced explicitly (see, 



360 9. Deriuatiue Pricing Models 9.3. Implementing Parametric Option Pricing Models 361 

for example, Hamilton [1994, Chapter 14]), and efficiency bounds can be Before considering the importance of (9.3.18),o bserve that (9.3.17) is a 
obtained (see Hansen (1985) and Hansen, Heaton, and Ogaki (1988)). first-order linear ordinary differential equation in E, [PI which can easily be 

But for continuous-time applications in finance, especially those involv- to yield a closed-form expression for E, [PI (note the initial condition 
ing derivative securities, much less is known about the properties of GMM Eo [p(O>I = Po) : 
estimators. Indeed, one of Hansen and Scheinkman's (1995) main contri- E,[pl = ~ , e C ' ~ + f i .  
butions is to show how to generate moment conditions for continuous-time BYa pplying similar arguments to the stochastic differential equations ofp*, 
Markov processes with discretely sampled data. Although a complete expo- p3, and so on-which may be obtained explicitlyvia It6's Lemma-all higher 
sition of GMM estimation for continuous-time processes is beyond the scope moments of p may be computed in the same way. 
of this text, the central thrust of their approach can be illustrated through Now consider the unconditional version of the infinitesimal generator 
a simple example. D[.]--  dE[.]/dt. Aseries of calculations similar to (9.3.14)-(9.3.18)f ollows, 

Suppose we wish to estimate the parameters of the following stationary but with one important difference: the time derivative of the unconditional 
diffusion process: expectation is zero, since pis a strictly stationary process, hence we have the 

restriction: 
v[Pl = ?@[PI - p )  = 0 (9.3.19) 

This is a continuous-time version of a stationary AR(1) process with uncon- which implies 
ditional mean p (see Section 9.3.4 for further discussion), and hence it u p 1  = p 
satisfies the hypotheses of Hansen and Scheinkman (1995). 

To generate moment conditions for {p(t)], Hansen and Scheinkman and this yields a first-moment condition: The unconditional expectation of 
(1995) use the infinitesimal generator V,, also known as the Dynkin operato?; p must equal the steady-state mean p.  
which is the time-derivative of a conditional expectation. Specifically, More importantly, we can apply the infinitesimal generator to any well- 

behaved transformation f (.) of p and by It6's Lemma we have: 

where the expectations operator E, [.] is a conditional expectation, condi- 
tioned on p(0) = pa. which yields an infinite number of moment conditions--one for each f- 

This operator has several important applications for deriving moment related to the marginal distribution off (p). From these moment conditions, 
conditions of diffusions. For example, consider the following heuristic cal- and under the regularity conditions specified by Hansen and Scheinkman 
culation of the expectation of dp: (1995), GMM estimation may be performed in the usual way. 

Hansen and Scheinkman (1995) also generate multipoint moment con- 
Eo[dpl = E,[-y (p - p) dtl + E,[a dB1 (9.3.14) ditions-conditions which exploit information contained in the conditional 

and joint distributions of f @)-making creative use of the properties of 
timereversed diffusions along the way, and they provide asymptotic approx- 
imations for statistical inference. Although it is too early to say how their 
approach will perform in practice, it seems quite promising and, for many 
It6 processes of practical interest, the GMM estimator is currently the only 
One that is both computationally feasible and consistent. 

where (9.3.14) and (9.3.15) follow from the fact the expectation of a linear 9.3.2 Estimating a in the Black-Schob Model 
function is the linear functions of the expectation, (9.3.16) follows from the To illustrate the techniques described in Section 9.3.1, we consider the 
same property for the differential operator and from the fact that increments implementation of't he Black-Scholesm odel (9.2.16) in which the parameter 
of Brownian motion have zero expectation, and (9.3.17) is another way of must be estimated. 
expressing (9.3.16). 



362 9. Derivative Pricing Models 9.3. Implementing Parametric Option Pricing Models 363 

A common misunderstanding about a is that it is the standard deviation and in this case the maximum likelihood estimators for a! and a* can be 
of simple returns & of the stock. If, for example, the annual standard obtained in closed form: 
deviation of IBM's stock return is 3076, it is often assumed that a = 0.30. TO 

see why this is incorrect, let prices P(t) follow a lognormal diffusion (9.2.2) 
as required by the Black-Scholes model (see Section 9.2.1) and assume, for 
expositional simplicity, that prices are sampled at equally spaced intervals of 
length h in the interval [O, TI, hence Pk - P(kh), k = 0,1,.  . . , n and T = 
nh. Then simple returns &(h) - (Pk/Pk-l)-  1 are lognormally distributed 
with mean and variance: Moreover, because the rk(h)'s are IID normal random variables under the 

dynamics (9.2.2), the regularity conditions for the consistency and asymp 
totic normality of the estimator s2a re satisfied (see the Appendix, Section 
A.4). Therefore, we are assured that ? c' and s2a re asymptotically efficient 
in the class of CUAN estimators, with asymptotic covariance matrix given by 

Therefore, the magnitude of IBM's a cannot be gauged solely by the 30% (9.3.8). 
estimate since this is an estimate of aar[&(h)]  and not of a. In particular, 
solving (9.3.22) and (9.3.23) for a yields the following: Iwegularly Sampled Data 

To see why irregularly sampled data poses no problems for continuous-time 
processes, observe that the sampling interval h is arbitrary and can change 
mid-sample without affecting the functional form of the likelihood function 

Therefore, a mean and standard deviation of 10% and 30%, respectively, (9.3.26). Suppose, for example, we measure returns annually for the first nl 
for IBM's annual simple returns implies a value of 26.8% for a .  While observations and then monthly for the next TQ observations. If h is measured 
30% and 26.8% may seem almost identical for most practical purposes, in units of one year, so that h = 1 indicates a one-year holding period, the 
the former value implies a Black-Scholes price of $8.48 for a one-year call maximum likelihood estimator of a* for the nl+n2 observations is given by 
option with a $35 strike price on a $40 stock, whereas the latter value implies a 
Black-Scholes price of $8.10 on the same option, an economically significant 
difference. 

Since most published statistics for equity returns are based on simple 
returns, (9.3.24) is a useful formula to obtain a quick "ballpark estimate of a 
when historical data is not readily available. If historical data are available, it 
is a simple matter to compute the maximum likelihood estimator of a using 
continuously compounded returns, as discussed in Sections 9.3.1 and 9.3.3. where 
In particular, applying 16's Lemma to log P(t) and substituting (9.2.2) for 
dP yields 

dlogP = (,u- i a 2 ) d t + a d ~=  adt  +*dB (9.3.25) 
Observe that the second term of (9.3.29) may be rewritten as 

where a! - p - +a2.T herefore, continuously compounded returns rk(h) 
10g(P~/Pk-l)a re IID normal random variables with mean a h  and variance 
a 2h  hence the sample variance of rk(h)/& be a good estimator of 
a*; in fact, the sample variance of rk(h)/& is the maximum likelihood 
estimator of a .  More formally, under (9.3.25) the likelihood function of a which is simply the variance estimator of monthly continuously compounded 
sample of continuously compounded returns rl(h),.  . . , rn(h)i s returns, rescaled to an annual frequency. 

The ease with which irregularly sampled data can be accommodated 
is one of the greatest advantages of continuoustime stochastic processes. 



364 9. Derivative Pricing Models 9.3. Implementing Parametric Option Pricing Models 

However, this advantage comes at some cost: this great flexibility is the Table 9 . 2 ~ . A symptotic standard m sfm 
result of strong parametric restrictions that each continuous-time process 
imposes on all its finitedimensional distributions (see Section 9.1.1 for the 
definition of a finitedimensional distribution). In particular, the stochas- 
tic differential equation (9.3.25) imposes independence and normality on 
all increments of log P(t) and linearity of the mean and variance in the in- 
crement interval, hence the continuously compounded return between a 
Friday and a Monday must have three times the mean and three times the 
variance of the continuously compounded return between a Tuesday and a 
Wednesday, the Tuesday/Thursday return must have the same distribution 
as the Saturday/Monday return, and so on. In fact, the specification of a 
continuous-time stochastic process such as (9.3.25) includes an infinity of 
parametric assumptions. Therefore, the convenience that continuous-time 
stochastic processes affords in dealing with irregular sampling should be 
weighed carefully against the many implicit assumptions that come with it. 

Asymptotic standard error of & for various values of n and h, assuming a base interval of h = 1 
year and a = 0.20. Recall that T = nh; hence the values n = 64 and h = 1/16 imply a sample 

Continuous-Reand Asymptotics of 64 observations equally spaced over 4 years. 
Since we can estimate a and a from arbitrarily sampled data, a natural 
question to consider is how to sample returns so as to obtain the "best" 
estimator? Are 10 annual observations preferable to 100 daily observations, 
or should we match the sampling interval to the horizon we are ultimately Tables 9.2a and 9.2b illustrate the sharp differences between & and by 
interested in, e.g., the time-to-maturity of the option we wish to value? reporting asymptotic standard errors for the two estimators for various values 

To address these issues, consider again a sample of n+l prices Po, PI,.  . . , of n and h, assuming a base interval of h=l year and a=0.20. Recall that 
P, equally spaced at intervals of length h over the fixed time span [0, TI so T=nh, hence the values n=64 and h=& imply a sample of 64 observations 
that Pk I P(kh), k = 0, . . . , n, and T = nh. The asymptotic variance of the equally spaced over 4 years. 
maximum likelihood estimator of [aa 21 ' is then given by (9.3.7), which In Table 9.2a the standard error of & declines as we move down the table 
may be evaluated explicitly in this case as: (corresponding to increasing T),  increases as we move left (corresponding 

to decreasing T),  and remains the same along the diagonals (corresponding 
to a fixed T). For purposes of estimating a,h aving 2 observations measured 
at &month intervals yields as accurate an estimator as having 1,024 observa- 
tions measured four times per day. 

In Table 9.2b the pattern is quite different. The entries are identical 
across the columns--only the number of observations n matters for deter- 
mining the standard error of &*. In this case, a sample of 1,024 observations 

Observe that (9.3.31) does not depend on the sampling interval h. As 
n increases without bound while T is fixed (hence h decreases to O), &* measured four times a day is an order of magnitude more accurate than a 

sample of 2 observations measured at six-month intervals. 
becomes more precise. This suggests that the "best" estimator for a2,  the one 

The consistency of s2a nd the inconsistency of & under contiriuous- 
with smallest asymptotic variance, is the one based on as many observations 

record asyrnptotics, first observed by Merton (1980) for the case of geomet- 
as possible, regardless of what their sampling interval is. ric Brownian motion, is true for general diffusion processes and is an artifact 

Interestingly, this result does not hold for the estimator of a, whose ofthe nondifferentiability of diffusion sample paths (see Bertsimas, Kogan, 
asymptotic variance depends on T and not n. More frequent sampling and Lo [I9961 for further discussion). In fact, if we observe a continuous 
within a fixed time span--often called continuous-record asymptotics-~ll record of P(t) over any finite interval, we can recover without m r t he dif- 
not increase the precision of 2,  and the "best" estimator for a is one based fusion coefficient a (.), even in cases where it is time-varying. Of course, in 
on as long a time span as possible. 



9. Derivative Pricing Models 9.3. Impkmnting Parametric Option Pricing Models 367 

9.3.3Q uantifyingt he Precision of Option Price Estimators 
Table 9.26. Asymptotic standard errors for 3'. 

Once the maximum likelihood estimator 6 of the underlying asset's param- 
eters is obtained, the maximum likelihood estimator of the option price 

A 

G may be constructed by inserting 8 into the option-pricing formula (or 
into the numerical algorithm that generates the price).17 Since e contains 
estimation error, G - G(P(t), 6) will also contain estimation error, and for 
trading and hedging applications it is imperative that we have some mea- 
sure of its precision. This can be constructed by applying a first-order Taylor 

,. 
expansion to G(6) to calculate its asymptotic distribution (see Section A.4 
of the Appendix) 

Asymptotic standard error of G 2  for various values of n and h, assuming a base interval of h = 1 
year and a = 0.20. Recall that T = nh; hence the values n = 64 and h = 1/16 imply a sample 
of 64 observations equally spaced over 4 years. where G - G(P(t),8 )  and 2(6) is the information matrix defined in (9.3.7). 

Therefore, for large n the variance of may be approximated by: 

practice we never observe continuous sample paths-the notion of contin- 
uous time is an approximation, and the magnitude of the approximation 
error varies from one application to the next. 

As the sampling interval becomes finer, other problems may arise such and Vf may be estimated in the natural way: 
as the effects of the bid-ask spread, nonsynchronous trading, and related 
market microstructure issues (see Chapter 3). For example, suppose we 
decide to use daily data to estimate a-how should weekends and holidays 
be treated? Some choose to ignore them altogether, which is tantamount 
to assuming that prices exhibit no volatility when markets are closed, with In much the same way, the precision of the estimators of an option's sensi- 
the counterfactual implication that Friday's closing price is always equal to t i ~ t tyo  its arguments-the option's delta, gamma, theta, rho, and vega (see 
Monday's opening price. Alternatively, we may use (9.3.29) in accounting Section 9.2.1)-may also be readily quantified. 
for weekends, but such a procedure implicitly assumes that the price process 
exhibits the samevolatilitywhen markets are closed, implying that the Friday- The Black-Schoks Case 
to-Monday return is three times more volatile than the Monday-to-Tuesday As an illustration of these results, consider the case of Black and Scholes 
return. This is also counterfactual, as French and Roll (1986) have shown. (1973) in which P(t) follows a geometric Brownian motion, 

The benefits of more frequent sampling must be weighed against the 
costs, as measured by the types of biases associated with more finely sampled 
data. Unfortunately, there are no general guidelines as to how to make 
such a tradeoff-it must be made on an individual basis with the t articular and in which the only parameter of interest is a .  Since the maximum 
application and data at hand.16 likelihood estimator & 2  of a2 has an asymptotic distribution given by 

 his follows from the principle of invariance: The maximum likelihood estimator of a 
'%ee Bertsimas, Kogan, and Lo (1996), Lo and MacKinlay (1989), Perron (1991), and nonlinear function of a parameter vector is the nonlinear function of the parameter vector's 

Shiller and Perron (1985) for a more detailed analysis of the interactions between sampling maximum likelihood estimator. See, for example, Zehna (1966). 
interval and sample size. 



368 9. Derivative Pricing ModeIs 9.3. Implementing Parametric Option Pricing Models 369 

- a 2 )  N(0,  2a4) (see Section 9.3.2), the asymptotic distribution 
Table 9.3. Cutoff valuesfm comparative statics of 5. 

of the Black-Scholes call-option price estimator 6 is 

where 4(.) is the standard normal probability density function and dl is 
given in (9.2.17). 

From the asymptotic variance Vf given in (9.3.37), some simple com- 
parative static results may be derived: 

6 begins to increase. Inequality (9.3.41) shows a similar pattern for Vf with 
respect to the strike price. 

Interestingly, inequality (9.3.43) does not depend on either the stock or 
strike prices, and hence for shorter maturity options the accuracy of 6 will 
increase with the time-to-maturity T-t. But even if (9.3.43) is not satisfied, 
the accuracy of 6 may still decline with T-t if (9.3.42) holds. 

Table 9.3 reports values of cl through c3 for various values of T- t assum- 

The following inequalities may then be established: ing an annual interest rate of 5% and an annual standard deviation of 5O%, 
corresponding to weekly values of r = log(1.05)/52 and a = 0 . 5 0 / m .  
Given the numerical values of c2 and 1/c2, (9.3.42) will be satisfied by op- 
tions that are far enough in- or out-of-the-money. For example, if the stock 
price is $40, then options maturing in 24 weeks with strike prices greater 
than $42.09 or less than $38.02 will be more precisely estimated as the time- 
to-maturity declines. This is consistent with the finding of MacBeth and 
Merville (1979, 1980) that biases of in- and out-of-the-money options de- 
crease as the time-to-maturity declines, and also supports the observation 
by Gultekin, Rogalski, and T i n i ~(1 982) that the Black-Scholes formula is 
more accurate for short-lived options.18 

Through first-order Taylor expansions (see Section A.4 of the Appendix), 
the accuracy of the option's sensitivities (9.2.24)-(9.2.28)c an also be read- 

where ily derived, and thus the accuracy of dynamic hedging strategies can be 
measured. For convenience, we report the asymptotic variances of these 
quantities in Table 9.4. 

9.3.4 The Effects of Asset Return Predictability 

The martingale pricing method described in Section 9.2.2 exploits the fact 
that the pricing equation (9.2.15) is independent of the drift of P(t). Since 

Inequality (9.3.40) shows that the accuracy of 6 decreases with the level of 
the stock price as long as the ratio of the stock price to the strike price is less  here are, of course, other possible expianations for such empirical regularities, such as 

than cl . However, as the stock price increases beyond Xcl , the accuracy of the presence of stochastic volatility or a misspecification of the stock price dynamics. 



370 9. Derivative Pricing Models 9.3. Implementing Parametric Option Pricing Models 371 

constant a, then the Black-Scholes formula yields the correct option price 
Table 9.4. Asymptotic variances of Black-Scholes call p c e  sensitivity estimators. 

regardless of the specification and arguments of the drift. This holds more 
venerally for any derivative asset which can be priced purely by arbitrage, 
a 

Estimator Asymptotic Variance and where the underlying asset's log-price dynamics is described by an It8 
diffusion with constant diffusion coefficient: the derivative-pricingf ormula 
is functionally independent of the drift and is determined purely by the 
diffusion coefficient and the contract specifications of the derivative asset. 

This may seem paradoxical since two stocks with the same a but different 
drifts will yield the same Black-Scholes price, yet the stock with the larger 
drift has a larger expected return, implying that a call option on that stock 
is more likely to be in-the-money at maturity than a call option on the stock 
with the smaller drift. The resolution of this paradox lies in the observation 
that although the expected payoff of the call option on the largerdrift stock 
is indeed larger, it must be discounted at a higher rate of return-one that 
is commensurate with the risk and expected return of its underlying stock- 

These asymptotic variances are based on the assumption that the variance estimator d2 is the and this higher discount rate exactly offsets the larger expected payoff so 
maximum likelihood estimator which has asymptotic distribution ,h (d2  - a2)  A N(0 ,  2a4), that the present value is the same as the price of the call on the stock with 
hence ,h(F(d2) - F(a2)) N(0 ,  2a4(aF(u2)/aa2)') where ~ ( ais~ th)e o ption sensitivity. the smaller drift. Therefore, the fact that the drift plays no direct role in 
Following standard conventions, the expressions reported in the table are the asymptotic vari- 
ances of ,h(F(d2) - F(u2))a nd must be divided by the sample size n to obtain the asymptotic the Black-Scholes formula belies its importance. The same economic forces 
variances of the (unnormalized) sensitivities F(d2). that determine the expected returns of stocks, bonds, and other financial 

assets are also at work in pricing options. 
These considerations are less important when the drift is assumed to 

be constant through time, but if expected returns are time-varying so that 
the drift does not enter into the PDE (9.2.15),f or purposes of pricing options stock returns are predictable to some degree, this predictability must be 
it may be set to any arbitrary function or constant without loss of generality taken into account in estimating a. 
(subject to some regularity conditions). In particular, under the equivalent 
martingale measure in which all asset prices follow martingales, the option's The Trending Omstein-UhlenbeckP rocess 
price is simply the present discountedvalue of its expected payoff at maturity, To see how a time-varying drift can influence option prices, we follow LO 

where the expectation is computed with respect to the risk-neutralizedprocess and Wang's (1995) analysis by replacing the geometric Brownian motion 
P*(t): assumption (A3) of the Black-Scholes model with the following stochastic 

differential equation for the log-price process p(t): 

where 
Y > 0, p(0) = Po, t E [O, 00). 

Although the risk-neutralized process is not empirically observable,lg it is 
nevertheless an extremely convenient tool for evaluating the price of an Unlike the geometric Brownian motion dynamics of the original Black- 

option on the stock with a data-generating process given by P(t). Scholes model, which implies that log-prices follow an arithmetic random 

Moreover, the risk-neutral pricing approach yields the following impli- walk with IID normal increments, this log-price process is the sum of a zero- 
mean stationary autoregressive Gaussian process-an Ornstein-Uhlenbeck 

cation: as long as the diffusion coefficientf or the log-price process is a fixed 
process--and a deterministic linear trend, so we call this the trending 0-U 
process. Rewriting (9.3.46) as 

Ig~oweveru, nder certain conditions it can be estimated: see, for example, Ait-Sahalia and 
LO (l996),J ackwerth and Rubinstein (1995), Rubinstein (1994),S himko (1993),a nd Section d(p(t) - gt)  = --y(p(t) - ~ t ) d+t   u dB (9.3.47) 
12.3.4 of Chapter 12. 



372 9. Derivative Pricing Models 9.3. Implementing Parametric Option Pricing Models 373 

shows that when p(t) deviates from its trend pt, it is pulled back at a rate Note that the first-order autocorrelation (9.3.52) of the trending 0- 
proportional to its deviation, where y is the speed of adjustment. This reversion U increments is always less than or equal to zero, bounded below by -$, 
to the trend induces predictability in the returns of this asset. and approaches -$  as r increases without bound. These prove to be se- 

To develop further intuition for the properties of (9.3.46), consider its rious restrictions for many empirical applications, and they motivate the 
explicit solution: alternative processes introduced in Lo and Wang (1995) which have con- 

siderably more flexible autocorrelation functions. But as an illustration of 
the impact of serial correlation on option prices, the trending O-U process 
is ideal: despite the differences between the trending O-U process and an 
arithmetic Brownian motion, both data-generating processes yield the same 

from which we can obtain the unconditional moments and co-moments of risk-neutralized price process (9.3.44), hence the Black-Scholes formula still 
continuously compounded r-period returns rt(r) = p(t)--p(t-~):~O applies to options on stocks with log-price dynamics given by (9.3.46). 

However, although the Black-Scholes formula is the same for (9.3.46), 
the a in (9.3.46) is not necessarily the same as the a in the geometric 
Brownian motion specification (9.2.2). Specifically, the two data-generating 
processes (9.2.2) and (9.3.46) must fit the same price data-they are, after 
all, two competing specifications of a single price process, the "true" DGP. 
Therefore, in the presence of serial correlation, e.g., specification (9.3.46), 
the numerical value for the Black-Scholes input o will be different than in 
the case of geometric Brownian motion (9.2.2). 

To be concrete, denote by rt(s), s2 [rt(r)], and pl (r) the unconditional 
mean, variance, and first-order autocorrelation of {rL(r)],r espectively, 
which may be defined without reference to any particular data-generating 
process.22 The numerical values of these quantities may also be fixed without 

Since (9.3.46) is a Gaussian process, the moments (9.3.49)-(9.3.51) com- reference to any particular data-generating process. All competing specifi- 
pletely characterize the finitedimensional distributions of r, (t) (see Section cations for the true data-generating process must come close to matching 
9.1.1 for the definition of a finitedimensional distribution). Unlike the these moments to be plausible descriptions of that data (of course, the 
arithmetic Brownian motion or random walk which is nonstationary and best specification is one that matches all the moments, in which case the 
often said to be d#imnce-stationary or a stochastic trend, the trending O-U true data-generating process will have been discovered). For the arithmetic 
process is said to be trend-stationary since its deviations from trend follow a Brownian motion, this implies that the parameters (p, 02)m ust satisfy the 
stationary process.21 following relations: 

''since we have conditioned on p(0) = p, in defining the detrended log-price process, 
it is a slight abuse of terminology to call these moments "unconditional". However, in this 
case the distinction is primarily semantic since the conditioning variable is more of an initial 
condition than an information variable-if we define the beginning of time as t = 0 and the 
fully observable starting value of p(0) asp,, then (9.3.49)-(9.3.52)a re unconditional moments 
relative to these initial conditions. We shall adopt this definition of an unconditional moment 
throughout the remainder of this chapter. From (9.3.54), we obtain the well-known result that the Black-Scholes input 

" ~ inmp lication of trend-stationarity is that the variance of 5-period returns has a finite 
0' may be estimated by the sample variance of continuously compounded 

limit as s increases without bound, in this case a21y,w hereas this variance increases linearly 
with r under a random walk. While trend-stationary processes are often simpler to estimate, returns {rt(r)]. 
they have been criticized as unrealistic models of financial asset prices since they do not accord 
well with the common intuition that longer-horizon asset returns exhibit more risk or that price Nevertheless, Lo and Wang (1995) provide a generalization of the trending O-U process that 
forecasts exhibit more uncertainty as the forecast horizon grows. However, if the source of such contains stochastic trends, in which case the variance of returns will increase with the holding 
intuition is empirical observation, it may well be consistent with trend-stationarity since it is period 7 .  
now well-known that for any finite set of data, trend-stationarity and difference-stationam are "of course, it must be assumed that the moments exist. However, even if they do  not, a sim- 
virtually indistinguishable (see, for example, Section 2.7 in Chapter 2, Campbell and Perron ilar but more involved argument may be based on location, scale, and association parametem 
[1991], Hamilton [1994, Chapters 17-181, and the many other "unit root" papers they cite). 



374 9. Derivative Pricing Models 9.3. Implementing Parametric Option Pricing Models 375 

In the case of the trending 0-U process, the parameters (F, y ,  a 2)  must As the return interval t decreases, it can be shown that the adjustment 

satisfy factor to s2[rt(r)]/r in (9.3.61) approaches unity (use L'HBpital's rule). 
- In the continuous-time limit, the standard deviation of continuously com- 

= CLr (9.3.56) pounded returns is a consistent estimator for a and the effects of predictabil- 
ity on a vanish. The intuition comes from the fact that a is a measure of 
local volatilitpthe volatility of infinitesimal price changes-and there is no 
predictability over any infinitesimal time interval by construction (see Sec- 
tion 9.1.1). Therefore, the influence of predictability on estimators for a is 
an empirical issue which depends on the degree of predictability relative to 
how finely the data is sampled, and must be addressed on a case-by-case ba- 

Observe that these relations must hold for the population values of the pa- 
sis. However, we provide a numerical example in the next section in which 

rameters if the trending 0-U process is to be a plausible description of the 
the magnitude of these effects is quantified for the Black-Scholes case. 

DGP. Moreover, while (9.3.56)-(9.3.58) involve population values of the 
parameters, they also have implications for estimation. In particular, un- 

Adjusting the Black-Scholes Fornula for Predictability 
der the trending 0-U specification, the sample variance of continuously 

Expression (9.3.61) provides the necessary input to the Black-Scholes for- 
compounded returns is clearly not an appropriate estimator for a2 .  

mula for pricing options on an asset with the trending 0-U dynamics. If 
Holding the unconditional variance of returns fixed, the particular 

the unconditional variance of daily returns is s2[rt(l)],a nd if the first-order 
value of a2n ow depends on y .  Solving (9.3.57) and (9.3.58) for y and 
a2 autocorrelation of t-period returns is pl (t),  then the price of a call option 

yields: 
is given by: 

where 

which shows the dependence of a 2 on y explicitly. 
In the second equation of (9.3.60), a 2 has been re-expressed as the 

product of two terms: the first is the standard Black-Scholes input under the and dl and 4 are defined in (9.2.17) and (9.2.18), respectively. 
assumption that arithmetic Brownian motion is the data-generating process, Expression (9.3.62) is simply the Black-Scholes formula with an adjusted 
and the second term is an adjustment factor required by the trending 0-U volatility input. The adjustment factor multiplying s2 [rt(l)]/r in (9.3.63) is 
specification. Since this adjustment factor is an increasing function of y ,  easily tabulated (see Lo and Wang [1995]); hence in practice it is a simple 
as returns become more highly (negatively) autocorrelated, options on the matter to adjust the Black-Scholes formula for negative autocorrelation of 
stock will become more valuable ceteris paribus. More specifically, (9.3.60) the form (9.3.58):  Multiply the usual variance estimator s2 [r,( 1)]/ t by the 
may be rewritten as the following explicit function of pl(t): appropriate factor from Table 3 of Lo and Wang (1995), and use this as a2 

in the Black-Scholes formula. 
Note that for all values of p l ( t )  in (- i ,  01, the factor multiplying 

s2[rt(l)]/ti n (9.3.63) is greater than or equal to one and increases in the ab- 
Holding fixed the unconditional variance of returns s2 [ rt ( r) I,a s the absolute solute value of the first-order autocorrelation coefficient. This implies that 
value of the autocorrelation increases from 0 to i , t he value of a2 increases option prices under the trending 0-U specification are always greater than 
without bound.23 This implies that a specification error in the dynamics of or equal to prices under the standard Black-Scholes specification, and that 
s(t) can have dramatic consequences for pricing options. option prices are an increasing function of the absolute value of the first- 

order autocorrelation coefficient. These are purely features of the trending 
' 3 ~ efo cus on the absolute value of the autocorrelation to avoid confusion in making 0-U process and do not generalize to other specifications of the drift (see 

comparisons between results for negatively autocorrelated and positively autocorrelated asset Lo and Wang [I9951 for examples of other patterns). 
returns. See Lo and Wang (1995) for further details. 



9. Derivative Pricing Models 9.3. Implementing Parametric Option Pricing Models 377 

The first panel of Table 9.5 shows that even extreme autocorrelation in 
Table 9.5. Option prices on assets with negatively autocorrelated returns. daily returns does not affect short-maturity in-the-money call option prices 

1 very much. For example, a daily autocorrelation of -45% has no impact 
Strike Black-Scholes Trending 0 - U  Price, with Daily p, (1) = on the $30 7-day call; the price under the trending 0-U process is identical 
price 1 Price -.05 - . lo -.20 -.30 -.40 -.45 to the standard Black-Scholes price of $10.028. But even for such a short 

maturity, differences become more pronounced as the strike price increases; 
Time-to-Maturity T- t = 7 Days the at-the-money call is worth $0.863 in the absence of autocorrelation, but 

10.028 increases to $1.368 with an autocorrelation of -45%. 
5.036 However, as the time to maturity increases, the remaining panels of 
0.863 Table 9.5 show that the impact of autocorrelation also increases. With a 
0.011  -10% daily autocorrelation, an at-the-money 1-year call is $7.234 and rises 
0.000 to $10.343 with a daily autocorrelation of -45%, compared to the standard 

Time-to-Maturity T- t = 182 Days Black-Scholes price of $6.908. This is not surprising since the sensitivity of 
11.285 the Black-Scholes formula to a-the option's vega-is an increasing func- 
7.558 tion of the time-to-maturity (see Section 9.2.1). From (9.2.28), we see that 
4.740 for shorter-maturity options, changes in a have very little impact on the call 
2.810 price, but longer-maturity options will be more sensitive. 

50 1.592 In general, the effects of asset return predictability on the price of deriva- 
Time-to-Maturity T- t  = 364 Days tives depends intimately on the precise nature of the predictability. For ex- 

12.753 ample, the importance of autocorrelation for option prices hinges critically 
9.493 on the degree of autocorrelation for a given return horizon r and, of course, 
6.908 on the data-generating process which determines how rapidly this autocorre- 
4.941 lation decays with t. For this reason, Lo and Wang (1995) introduce several 
3.489 new stochastic processes that are capable of matching more complex pat- 

terns of autocorrelation and predictability than the trending 0-U process. 
Comparison of Black-Scholes call option prices on a hypothetical $40 stock under an arithmetic 
Brownian motion versus a trending Ornstein-Uhlenbeck process for log-prices, assuming a 
standard deviation of 2% for daily continuously compounded returns, and a daily continuously 
compounded riskfree rate of log(1.05)/364. As autocorrelation becomes larger in absolute 9.3.5 Implied Volatility Estimators 
value, option prices increase. Suppose the current market price of a one-year European call option on a 

nondividend-paying stock is $7.382. Suppose further that its strike price is 
$35, the current stock price is $40, and the annual simple riskfree interest 

An Empirical Illustration rate is 5%. If the Black-Scholes model holds, then the volatility a implied 
To gauge the empirical relevance of this adjustment for autocorrelation, by the values given above can only take on one value-0.200-because the 
Table 9.5 reports a comparison of Black-Scholes prices under arithmetic Black-Scholes formula (9.2.16) yields a one-to-one relation between the op- 
Brownian motion and under the trending Ornstein-Uhlenbeck process for tion's price and a ,  holding all other parameters fixed. Therefore, the op- 
various holding periods, strike prices, and daily autocorrelations from -5 tion described above is said to have an implied volatility of 0.200 or 20%. SO 
to -45% for a hypothetical $40 stock. The unconditional standard devia- common is this notion of implied volatility that options traders often quote 
tion of daily returns is held fixed at 2% per day. The Black-Scholes price is prices not in dollars but in units of volatility, e.g., "The one-year European 
calculated according to (9.2.16), setting a equal to the unconditional stan- call with $35.000 strike is trading at 20%." 
dard deviation. The trending 0-U prices are calculated by solving (9.3.57) Because implied volatilities are linked directly to current market prices 
and (9.3.58) for a given r and the return autocorrelations (t) of -0.05, (via the Black-Scholesf ormula), some investment professionals have argued 
-0.10, -0.20, -0.30, -0.40, and -0.45, and using these values of a in the that they are better estimators of volatility than estimators based on historical 
Black-Scholes formula (9.2.16),w here r = 1. data such as 82. Implied volatilities are often said to be "forward looking" 



378 9. Derivative Pricing Models 9.3. Implementing Parametric Option Pricing Models 379 

since they are based on current prices which presumably have expectations pirically because stock prices do not follow a lognormal diffusion, we may 

of the future impounded in them. be able to specify an alternate price process that fits the data better, in 

However, such an argument overlooks the fact that an implied volatility which case the "implied" parameter(s) of options on the same stock may 

is intimately related to a specific parametric option-pricing model-typically indeed be numerically identical. Alternatively, if the Black-Scholes model 

the Black-Scholes model-which, in turn, is intimately related to a particular fails empirically because in practice it is impossible to trade continuous~y 

set of dynamics for the underlying stock price (geometric Brownian motion due to transactions costs and other institutional constraints, then markets 

in the Black-Scholes case). Herein lies the problem with implied volatilities: are never dynamically complete, options are never redundant securities, 

If the Black-Scholes formula holds, then the parameter a can be recovered and we should never expect "implied" parameters of options on the same 

without error by inverting the Black-Scholes formula for any one option's price stock to be numerically identical for any option-pricing formula. In this 

(each of which yields the same numerical value for a ) ;i f the Black-Scholes case, the degree to which implied volatilities disagree may be an indication 

formula does not hold, then the implied volatility is difficult to interpret of how "redundant" options really are. 
since it is obtained by inverting the Black-Scholesf ormula. Therefore, using The fact that options traders quote prices in terms of Black-Scholes 
the implied volatility of one option to obtain a more accurate forecast of implied volatilities has no direct bearing on their usefulness from a pric- 
volatility to be used in pricing other options is either unnecessary or logically ing point of view, but is a remarkable testament to the popularity of the 
inconsistent. Black-Scholes formula as a convenient heuristic. Quoting prices in terms of 

To see this more clearly, consider the argument that implied volatilities "ticks" rather than dollars has no far-reaching economic implications sim- 

are better forecasts of future volatility because changing market conditions ply because there is a well-known one-to-one mapping between ticks and 
cause volatilities vary through time stochastically, and historical volatilities dollars. Moreover, just because options traders quote prices in terms of 
cannot adjust to changing market conditions as rapidly. The folly of this Black-Scholes implied volatilities, this does not imply that they are using 

argument lies in the fact that stochastic volatility contradicts the assumptions the Black-Scholes model to set their prices. Implied volatilities do convey 

required by the Black-Scholes model-if volatilities do change stochastically information, but this information is identical to the information contained 

through time, the BlackScholes formula is no longer the correct pricing in the market prices on which the implied volatilities are based. 

formula and an implied volatility derived from the Black-Scholes formula 
provides no new information. 9.3.6 Stochastic-Volatility Models 

Of course, in this case the historical volatility estimator is equally useless, 
since it need not enter into the correct stochastic volatility option-pricing Several empirical studies have shown that the geometric Brownian motion 
formula (in fact it does not, as shown by Hull and White [1987], Wiggins (9.2.2) is not an appropriate model for certain security prices. For exam- 
[I9871 and others-see Section 9.3.6). The correct approach is to use a ple, Beckers (1983), Black (1976), Blattberg and Gonedes (1974), Christie 
historical estimator of the unknown parameters entering into the pricing (1982), Fama (1965), Lo and MacKinlay (1988, 1990c), and Mandelbrot 
formula-in the Black-Scholes case, the parameter a is related to the his- (1963, 1971) have documented important departures from (9.2.2) for US 
torical volatility estimator of continuously compounded returns, but under stock returns: skewness, excess kurtosis, serial correlation, and time-varying 
other assumptions for the stock price dynamics, historical volatility need not volatilities. Although each of these empirical regularities has implications 
play such a central role. for option pricing, it is the last one that has received the most attention in the 

This raises an interesting issue regarding the validity of the Black-Scholes recent derivatives l i t e r a t ~ r ep,a~r~tly  because volatility plays such a central 
formula. If the Black-Scholes formula is indeed correct, then the implied role in the Black-Scholes/Merton formulation and in industry practice. 
volatilities of any set of options on the same stock must be numerically identical. If, in the geometric Brownian motion model (9.2.2) the a is a known 
Of course, in practice they never are; thus the assumptions of the Black- deterministic function of time o ( t ) ,  then the Black-Scholes formula still 
Scholes model cannot literally be true. This should not come as a complete applies but with a replaced by the integral T a ( s )d s over the option's 
surprise; after all the assumptions of the Black-Scholes model imply that life. However, if a is stochastic, the situation becomes more complex. For 

options are redundant securities, which eliminates the need for organized 
options markets altogether. 24 See, for example, Amin and Ng (l993), Ball and Roma (l994), Beckers (l98O), Cox 

The difficulty lies in determining which of the many Black-Scholes as- (1975), Goldenberg (1991), Heston (1993), Hofmann, Platen, and Schweizer (1992). Hull 
sumptions are violated. If, for example, the Black-Scholes model fails em- and White (1987),J ohnson and Shanno (1987),S cott (1987),a nd Wiggins (1987). 



9.3. Implementing Parametric Option Pricing Models 
380 9. Derivative Pricing Models 

Parameter Estimation 
example, suppose that the fundamental asset's dynamics are given by: 

One of the most challenging aspects of stochastic-volatility models, is the fact 
that realizations of the volatility process are unobservable yet option-pricing 
formulas are invariably functions of the parameters of the process driving 0. 

To date, there has been relatively little attention devoted to this important 
issue for continuous-time processes like (9.3.64)-(9.3.65) primarily because 

where a(.)a nd B(.) are arbitrary functions of volatility a, and Bp and B, are of the difficulties inherent in estimating continuous-time models with diS- 
standard Brownian motions with instantaneous correlation dBp dB, = p dt. cretely sampled data. However, a great deal of attention has been devoted to 
In this case, it may not be possible to determine the price of an option by a related discrete-time model: the autoregressive conditional heteroskedas- 
arbitrage arguments alone, for the simple reason that there may not exist a ticity (ARCH) process of Engle (1982) and its many variants (see Chapter 12 
dynamic self-financing portfolio strategy involving stocks and riskless bonds and Bollerslev, Chou, and Kroner [I9921) . 
that can perfectly replicate the option's payoff. Although originally motivated by issues other than option pricing, 

Heuristically, stochastic volatility introduces a second source of uncer- ARCH models does capture the spirit of some of the corresponding contin- 
tainty into the replicating portfolio and if this uncertainty (B,) is not per- uous-time models. Recent studies by Nelson and Foster (1994), Nelson and 
fectly correlated with the uncertainty inherent in the stock price process Rarnaswamy (1990), and Nelson (1991,1992,1996) provide some important 
(Bp), the replicating portfolio will not be able to "span" the possible out- links between the two. In particular, Nelson (1996) and Nelson and Foster 
comes that an option may realize at maturity (see Harrison and Kreps [1979] (1994) derive the continuous-record asymptotics for several discrete-time 
and Duffie and Huang [I9851 for a more rigorous discussion). Of course, ARCH processes, some of which converge to the continuous-time processes 
if o were the price of a traded asset, then under relatively weak regularity of Hull and White (1987) and Wiggins (1987). The empirical properties of 
conditions there would exist a dynamic self-financing portfolio strategy con- these estimators have yet to be explored but will no doubt be the subject of 
sisting of stocks, bonds, and the volatility asset that could perfectly replicate future research. 
the option. 

In the absence of this additional hedging security, the only available 
method for pricing options in the presence of stochastic volatility of the DiscrehTime Models 
form (9.3.65) is to appeal to a dynamic equilibrium model. Perhaps the Another approach is to begin with a discrete-time dynamic equilibrium 
simplest approach is to assert that the risk associated with stochastic volatility model for option prices in which the fundamental asset's price dynamics 
is not priced in equilibrium. This is the approach taken by Hull and White are governed by an ARCH model. Although it is typically impossible to 
(1987) for the case where volatility follows a geometric Brownian motion price securities by arbitrage in discrete time, continuous-time versions must 

appeal to equilibrium arguments as well in the case of stochastic volatility; 
hence there is little loss of generality in leaving the continuous-time frame- 
work altogether in this case. This is the approach taken by Amin and Ng 
(1993),w ho derive option-pricing formulas for avariety of price dynamics- 
stochastic volatility, stochastic consumption growth variance, stochastic in- 

By assuming that volatility is uncorrelated with aggregate consumption, they terest rates, and systematic jumps-by applying the discrete-time dynamic 
show that equilibrium option prices are given by the expectation of the equilibrium models of Brennan (1979) and Rubinstein (1976). 
Black-Scholes formula, where the expectation is taken with respect to the Discrete-time models are also generally easier to implement empirically 
average volatility over the option's life. since virtually all historical data are sampled discretely, financial transac- 

Using the dynamic equilibrium models of Garman (1976b) and Cox, tions are typically recorded at discrete intervals, parameter estimation and 
Ingersoll, and Ross (1985b), Wiggins (1987) derives the equilibrium price hypothesis testing involve discrete data records, and forecasts are produced 
of volatility risk in an economy where agents possess logarithmic utility func- at discrete horizons. For these reasons, there may be an advantage in mod- 
tions, yielding an equilibrium condition-in the form of a PDE with certain eling stochastic volatility via ARCH and pricing derivatives within a discrete- 
boundary conditionsfor the instantaneous expected return of the option time equilibrium model. 
price. Other derivative-pricing models with stochastic volatility take similar However, continuous-time models do offer other insights that are harder 
approaches, the differences coming from the type of equilibrium model to come by within a discrete-time framework. For example, the dynamics 

- - - l - - , -A fir the choice of preferences that agents exhibit. 



382 9. Derivative Pricing Models 9.4. Pricing Path-Dependent Derivatives Via Monte Carlo Simulation 383 

of nonlinear functions of the data-generating process are almost impossible - e-rT E* [ Max ~ ( t )-] C TE*  [ P ( T ) ]  
to obtain in discrete time, but in continuous time It6's differentiation rule O515T (9.4.2) 

gives an explicit expression for such dynamics. Theoretical insights into 
the equilibrium structure of derivatives prices-for example, which state - p r T E *  1M ax ~ ( t )-] P (0) .  
variables affect derivatives prices and which do not-are also more readily O5t5T 

obtained in a continuous-time framework such as Cox, Ingersoll, and Ross where E* is the expectations operator with respect to the risk-neutral prob- 
(1985b). Therefore, each set of models offers some valuable insights that ability distribution or equivalent martingale measure. 
are not contained in the other. Observe that in going from (9.4.2) to (9.4.3) we have used the fact that 

the expected present value of P ( T ) d iscounted at the riskless rate r is P(0) .  
This holds because we have used the risk-neutral expectations operator E*, 

9.4 Pricing Path-Dependent Derivatives Via Monte Carlo Simulation and under the risk-neutral probabilities implicit in E* all assets must earn 
an expected return of r; hence ePTE * [ P ( T ) ]=  P(0) .  

Consider a contract at date 0 that gives the holder the right but not the To evaluate (9.4.3) via Mome Carlo simulation, we simulate many sam- 
obligation to sell one share of stock at date T for a price equal to the max- ple paths of { P ( t ) )f,i nd the maximum value for each sample path, or repli- 
imum of that stock's price over the period from 0 to T .  Such a contract, cation, and average the present discounted value of the maxima over all the 
often called a lookback option, is clearly a put option since it gives the holder replications to yield an expected value over all replications, i.e., an estimate 
the option to sell at a particular price at maturity. However, in this case the of H(0) .  Two issues arise immediately: How do we simulate a continuous 
strike price is stochastic and determined only at the maturity date. Because sample path, and how many replications do we need for a reasonably precise 
the strike price depends on the path that the stock price takes from 0 to T ,  estimate of H(O)? 
and not just on the terminal stock price P ( T ) ,s uch a contract is called a 
path-dependent option. 

Path-dependent options have become increasingly popular as the hedg- 9.4.1 Discrete Versus Continuous Time 
ing needs of investors become ever more complex. For example, many By their very nature, digital computers are incapable of simulating truly 
multinational corporations now expend great efforts to hedge against ex- continuous phenomena; but as a practical matter they are often capable 
change-rate fluctuations since large portions of their accounts receivable of providing excellent approximations. In particular, if we divide our time 
and accounts payable are denominated in foreign currencies. One of the interval [0, TI into n discrete intervals each of length h, and simulate prices 
most popular path-dependent options are foreign currency average rate or at each discrete date kh, k = 0 ,.  . . , n, the result will be an approximation 
Asian options which gives the holder the right to buy foreign currency at a to a continuous sample path which can be made successively more precise 
rate equal to the average of the exchange rates over the life of the contract.25 by allowing n to grow and h to shrink so as to keep T fixed. 

Path-dependent options may be priced by the dynamic-hedging a p  For example, consider the case of geometric Brownian motion (9.2.2) 
proach of Section 9.2.1, but the resulting PDE is often intractable. The risk- for which the risk-neutral dynamics are given by 
neutral pricing method offers a considerably simpler alternative in which 
the power of high-speed digital computers may be exploited. For example, 
consider the pricing of the option to sell at the maximum. If P( t )  denotes and consider simulating the following approximate sample path P,*: 
the date t stock price and H ( 0 ) i s the initial value of this put, we have 

H(0)  = e - r T ~1*M  ax P( t )-  P ( T )  
O5 t5T  Despite the fact that the simulated path P, varies only at multiples of h, 

the approximation may be made arbitrarily precise by increasing n and 
"The term "Asian" comes from the fact that such options were first actively written on stocks 

trading on Asian exchanges. Because these exchanges are usually smaller than their European therefore decreasing h-as n increases without bound, P,* converges weakly 
and American counterparts, with relatively thin trading and low daily volume, prices on such to (9.4.4) (see Section 9.1.1 for further discussion). Unfortunately, there 
exchanges are somewhat easier to manipulate. To minimize an option's exposure to the risk are no general rules for how large n must be to yield an adequate approxi- 
of stock-price manipulation, a new option was created with the average of the stock prices over mation-hoosing n must be done on a case-bycase basis. 
the option's life playing the role of the terminal stock price. 



384 9. Derivative Pricing Models 9.4. Pricing Path-Dependent Derivatives Via Monte Carlo Simulation 385 

9.4.2 How Many Simulations to P e r f m  is given by Goldman, Sosin, and Gatto (1979): 

We can, however, provide some clear guidelines for choosing the number 
of replications m to simulate. Recall that our Monte Carlo estimate of H(0) 
involves a simple average across replications: 

where a = r - a2/2. 
Therefore, in this case we may compare the accuracy of the Monte Carlo 

where {q]b,i s the jth replication or sample path of the stock price process estimator ~ ( 0w)ith  the theoretical value H(0). Table 9.6 provides such a 

under the risk-neutral distribution which, in the case of (9.4.4), implies that comparison under the following assumptions: 

p = r - f . But since by construction the qn's are IID random variables with Annual (Simple) Riskfree Interest Rate = 5% 
finite positive variance, the Central Limit Theorem implies that for large m: 

Annual (Simple) Expected Stock Return = 15% 

Annual Standard Deviation of Stock Return = 20% 

Therefore, for large m an approximate 95% confidence interval for H(0) Initial Stock Price P(0) = $40 
may be readily constructed from (9.4.7): 

Time to Maturity T = 1 Year. 

From the entries in Table 9.6, we see that large differences between the 
continuous-time price H(0) = $3.0981 and the crude Monte Carlo estimator 
~ ( 0ca)n  arise, even when m and n are relatively large (the antithetic estima- 

The choice of m thus depends directly on the desired accuracy of ~ ( 0 ) .  
tor is defined and discussed in the next section). For example, H(0) and 

If, for example, we require a H(O) that is within $0.001 of H(0) with 95% 
~ ( 0d)iff er by 18 cents when n = 250, a nontrivial discrepancy given the 

confidence, m must be chosen so that: 
typical size of options portfolios. 

The difference between H(O) and H(0) arises from two sources: sam- 
pling variation in H(O) and the discreteness of the simulated sample paths 
of prices. The former source of discrepancy is controlled by the number 

Typically Var[qn] is not known, but it can be readily estimated from the of replications m, while the latter source is controlled by the number of 
simulations in the obvious way: observations n in each simulated sample path. Increasing m will allow us to 

estimate E* [H(o)] with arbitrary accuracy, but if n is fixed then E* [H (O)] 
need not converge to the continuous-time price H(0). Does this discrep 
ancy imply that Monte Carlo estimators are inferior to closed-form solutions 
when such solutions are available? Not necessarily. 

Since the replications are IID by construction, estimators such as (9.4.10) This difference highlights the importance of discretization in the pric- 
will generally be very well-behaved, converging in probability to their expec- ing of pathdependent securities. Since we are selecting the maximum over 
tations rapidly and, when properly normalized, converging in distribution k exponentials of the (discrete) partial sum ELl r;, where k ranges from 0 
just as rapidly to their limiting distributions. to n,a s n increases the maximum is likely to increase as Heuristically, 

26~~thougith is  probable that the maximum of the partial sum will increase with n, it 
9.4.3 Comparisons with a Closed-Form Solution is not guaranteed. As we increase n in Table 9.6, we generate a new independent random 

wquence {r:):=, , and there is always some chance that this new sequence with more terms will 
In the special case of the option to sell at the maximum with a geometric nevertheless yield smaller partial sums. 
Brownian motion price process, a closed-form solution for the option price 



9. Derivative Pricing Models 9.4. Pricing Path-Dependent Derivatives Via Monte Carlo Simulation 387 

and Handscomb [I9641 for example), for obvious reasons. Therefore, a 
Table 9.6. Monte Carlo estimation of lookback option pice. 

number of variance-reduction techniques have been developed to improve 
the efficiency of simulation estimators. Although a thorough discussion of 

Crude 
these techniques is beyond the scope of this text, we shall briefly review a 
few of them here.*' 

A simple technique for improving the performance of Monte Carlo es- 
timators is to replace estimates by their population counterparts whenever 
possible, for this reduces sampling variation in the estimator. For example, 
when simulating risk-neutralized asset returns, the sample mean of each 
replication will almost never be equal to its population mean (the riskless 
rate), but we can correct this sampling variation easily by adding the differ- 
ence between the riskless rate and the sample mean to each observation of 
the replication. If this is done for each replication, the result will be a set 

Monte Carlo estimator of the price of a one-year look-back put option with continuous-time of replications with no sampling error for the mean. The efficiency gain 
Goldman-Sosin-Gatto price H(0)=$3.0981. Each row corresponds to an independent set of depends on the extent to which sampling errors for the mean contributes 
simulations of 100,000 replications of sample paths of length n. For the antithetic-variates to the overall sampling variation of the simulation, but in many cases the 
simulations, each sequence of IID random variates is used twice-the original sequence and 
its negative-yieldinga  total of 200,000 sample paths, or 100,000 negatively correlated pairs of improvement can be dramatic. 
paths. SE[H(O)Ia nd SE[H(O)Ia re the standard errors of ~ ( 0an) d  H(o), respectively. A related technique is to exploit other forms of population informa- 

tion. For example, suppose we wish to estimate E*[ f  (X)] and we find a 
random variable g(Y) such that E* [g(Y)] is close to E*[ f  (X)] and E* [g(Y)] 

the maximum of the daily closing prices of P over the year (n  = 250 trading is known (this is the population information to be exploited). E* [f  (X)] 
days) must be lower than the maximum of the daily highs over that same might be the price of newly created pathdependent derivative which must 

be estimated, and E* [g(Y)] the market price of an existing derivative with 
year (n-m). Therefore, the continuous-time price H(O), which is closer 

similar characteristics, hence a similar expectation. By expressing E* [f  (X)] 
to the maximum of the daily highs, will almost always exceed the simulation 

as the sum of E* [g(Y)] and E* [f  (X) - g(Y)],  the expectation to be esti- 
price H(O) which is discretized. mated is decomposed into two terms where the first term is known and the 

Which price is more relevant depends of course on the terms of the second term can be simulated with much smaller sampling variation. This 
particular contract. For example, average rate options on foreign exchange technique is known as the control variate method-g(Y) is the control variate 
usually specify particular dates on which the exchange rate is measured, and for f (X)-and its success depends on how close E* [g(Y)] is to E* [f  (X)]. 
it is almost always either a market closing rate (such as the corresponding Another form of population information that can be exploited is symme- 
spot rate of the IMM futures closing) or a central bank fixing rate. In both try. If, for example, the population distribution is symmetric about its mean 
cases, the more relevant price would be the simulation price, since the path and this mean is known (as in the case of risk-neutralized asset returns), 
dependence is with respect to the discrete set of measured rates, and not an then m/2 replications can yield m sample paths since each replication can 
idealized continuous process. 

27~everatle xts provide excellent coverage of this material. Hammersley and Handscomb 
9.4.4 Computational EfJiciency (1964) is a classic, concise but complete. Kalos and Whitlock (1986) ~rovidea  more detailed 

and updated exposition of similar material. Fishman (1996) is considerably more comprehen- 
The two main concerns of any Monte Carlo simulation are accuracy and sive and covers several advanced topics not found in other Monte Carlo texts such as Markov 
computational cost, and in most cases there will be tradeoffs between the chain sampling, Gibbs sampling, random tours, and simulated annealing. Fishman (1996) 
two. As we saw in Section 9.4.2, the standard error of the Monte Carlo also contains many applications, explicit algorithms for many of the techniques covered, and 

FORTRAN software (from an ftp site) for random number generation. Finally, Fang and Wang 
estimator H(O) is inversely proportional to the square root of the number (1994) present a compact introduction to a new approach to Monte Carlo simulation based on 
of replications m, hence a 50% reduction in the standard error requires Purely deterministic sampling. Although it is still too early to tell how this approach compares 
four times the number of replications, and so on. This type of Monte to the more traditional methods, Fang and Wang (1994) provide some intriguing examples 

Carlo procedure is often described as crude Monte Carlo (see Hammersley that look quite promising. 



388 9. Derivative Pricing Models 9.4. Pricing Path-Dependent Derivatives Via Monte Carlo Simulation 389 

be "reflected" through its mean to produce a mirror-image which has the antithetic-variates estimator ~ ( 0is) si mply the average across all 2m paths 
same statistical properties. This approach yields an added benefit: nega- 
tive correlation among pairs of replications. If the summands of the Monte 
Carlo estimator are monotone functions of the replications they will also be 
negatively correlated, implying a smaller variance for the estimator. 

This is a simple example of a more general technique known as the where 
antithetic uariates method in which correlation is induced across replications 
to reduce the variance of the sum. A more formal motivation for this ap- 
proach comes from the following theorem: for any estimator which can be The relation between antithetic-variates and crude Monte Carlo can be more 
expressed as the sum of random variables, it is always possible to create a easily seen by rewriting (9.4.12) as 
strict functional dependence between the summands which leaves the es- 
timator unbiased but yields a variance that comes arbitrarily close to the 
minimum variance possible with these random variables (see Hammersley 
and Mauldon [1956]). Of course, the challenge is to construct such a func- 
tional dependence, but even if the optimal transformation is not apparent, 
substantial efficiency gains can be achieved by simpler kinds of dependence. 

Variance reduction can also be accomplished by more sophisticated 
sampling methods. In stratzjied sampling, the support of the basic random Equation (9.4.13) shows that ~ ( 0is) b ased on a simple average of two av- 
variable X being simulated is partitioned into a finite number of intervals erages, one based on the sample paths {T~},a"nd= t,h e other based on 
and crude Monte Carlo simulations are performed in each interval. If there {qk};T=heo f.ac t that these two averages are negatively correlated leads to 
is less variation in f (X) within intervals than across the intervals, the sampling a reduction in variance. 
variation of the estimator of E* [f  (X)] will be reduced. Equation (9.4.14) combines the two sums of (9.4.13) into one, with the 

Importance sampling is a more sophisticated version, sampling more fre- averages of the antithetic pairs as the summands. This sum is particularly 
quently in regions of the support where there is more variation in f ( X ) -  easy to analyze because the summands are IID-the correlation is confined 
where sampling is more "importantn-instead of sampling at regular inter- within each summand, not across the summands-hence the variance of the 
vals. An even more sophisticated version of this method has recently been sum is simply the sum of the variances. An expression for the variance of 
proposed by Fang and Wang (1994) in which replications are generated de- ~ ( 0th) e n follows readily 
terministically, not randomly, accordingly to an algorithm that is designed to 
minimize the sampling variation of the estimator directly. It is still too early 
to say how this approach-called the number-theoretic method-compares to 
the more traditional Monte Carlo estimators, but it has already found its way 
into the financial community (see, for example, Paskov and Traub [1995]) 
and the preliminary findings seem encouraging. 

An Illustration of Variance Reduction 
TO illustrate the potential power of variance-reduction techniques, we con- where a:(n)=~ar[e-'~~ , , ] = ~ a r [ e -E' n~]  and p= ~ o r r [ e - 'Y~,,, e-'T E n ] .  

struct an antithetic-variates estimator of the price of the one-year lookback Equation (9.4.15) shows that the variance of ~ ( 0c)an  be estimated by the 
put option of Section 9.4.3. For each simulated price path {?:}:=,,, another product of e-'T/ mand the sample variance of the IID sequence {( I;.,+ 5,)/2). 
can be obtained without further simulation by reversing the sign of each of There is no need to account for the correlation between antithetic pairs be- 
the randomly generated IID standard normal variates on which the price 
path is based, yielding a second path {ek}w% cause this is implicitly accounted for in the sample variance of {(I;.,+ t , ) / 2 ] .  

hicho is  negatively correlated Equation (9.4.16) provides additional insight into the variance reduc- 

with the first. If m sample paths of { ~ i * , } : = ~ar e generated, the resulting tion that antithetic variates affords. The reduction in variance comes from 



390 9. Derivative Pricing Models 9.5. Conclusion 391  

options that was riskless. In effect, this implies that the option is ''spannedW 

two sources: a doubling of the number of replications from m to 2m, and 
the factor l+p which should be less than one if the correlation between the by stocks and bonds or, more precisely, the option's payoff at date T can 
antithetic variates is negative. Note that even if the correlation is positive, be perfectly replicated by a particular dynamic trading strategy involving 
the variance of ~ ( 0w)ill  still be lower than the crude Monte Carlo estima- only stocks and bonds. The no-arbitrage condition translates into the re- 
tor H(O) unless there is perfect correlation, i.e., p=l.  Also, while we have +rement that the option price must equal the cost of the dynamic trading 
doubled the number of replications, we have done so in a computationally strategy that replicates the option's payoff. 
trivial way: changing signs. Since the computations involved in pseudo- But there are situations where the derivative security cannot be repli- 
random number generation are typically more demanding than mere sign cated by any dynamic strategy involving existing securities. For example, if 
changes, this is another advantage of antithetic-variates simulations. we assume that the diffusion parameter a in (9.2.2) is stochastic, then it may 

Acomparison of the crude Monte Carlo estimator ~ ( 0to) t he antithetic- be shown that without further restrictions on a there exists no nondegen- 
variates estimator ~ ( 0is) p rovided in Table 9.6. For most of the simula- erate dynamic trading strategy involving stocks, bonds, and options that is 
tions, the ratio of the standard error of H(O) to the standard error of ~ ( 0 )  riskless. Heuristically, because there are now two sources of uncertainty, the 
is 0.0043/0.0116=0.371, a reduction of about 63%. In comparison, a dou- option is no longer "spanned" by a dynamic portfolio of stocks and bonds 
bling of the number of replications from rn to 2m for the crude Monte Carlo (see Section 9.4.6 and Huang [I9921 for further discussion). 
estimator would yield a ratio of 1/fi=0.707, only a 29% reduction. More Therefore, before we can apply the risk-neutral pricing method to a 
formally, observe from (9.4.7) and (9.4.16) that the ratio of the standard er- particular derivative security, we must first check that it is spanned by other 

ror of H(O) to the standard error of H(O)i s an  estimator of d m , traded assets. Since Goldman, Sosin, and Gatto (1979) demonstrate that 
hen ce 

the option to sell at the maximum is indeed spanned, we can apply the Cox- 
the ratio O.OO43/O.Ol l6=0.371 implies a correlation of -72% between the 

Ross method to that case with the assurance that the resulting price is in fact 
antithetic pairs of the simulations in Table 9.6, a substantial value which is 

the no-arbitrage price and that deviations from this price necessarily imply 
responsible for the dramatic reduction in variance of ~ ( 0 ) .  

riskless profit opportunities. But it may be more difficult to verify spanning 
for more complex path-dependent derivatives. In those cases, we may have 

9.4.5 Extensions and Limitations to embed the security in a model of economic equilibrium, with specific 
assumptions about agents' preferences and their investment opportunity 

The Monte Carlo approach to pricing pathdependent options is quite gen- sets as, for example, in the stochastic-volatility model of Section 9.3.6. 
eral and may be applied to virtually any European derivative security. For 
example, to price average-rate foreign currency options we would simulate 
price paths as above (perhaps using a different stochastic process more ap- 
propriate for exchange rates), compute the average for each replication, 9.5 Conclusion 
repeat this many times, and compute the average across the replications. 
Thus the power of the Cox-Ross risk-neutral pricing method is consider- The pricing of derivative securities is one of the unqualified successes of 
able. However, there are several important limitations to this approach that modern economics. It has changed the way economists view dynamic mod- 
should be emphasized. els of securities prices, and it has had an enormous impact on the investment 

First, the Monte Carlo approach may only be applied to European op- community. The creation of ever more complex financial instruments has 
tions, options that cannot be exercised early. The early exercise feature of been an important stimulus for academic research and for the establish- 
American options introduces the added complication of determining an ment of a bona fide "financial engineering" discipline. Recent innovations 
optimal exercise policy, which must be done recursively using a dynamic- in derivative securities include: average rate options, more general "look- 
programming-like analysis. In such cases, numerical solution of the corre- backn options, barrier options (also known as "down and out" or "birth and 
sponding PDE is currently the only available method for obtaining prices. death" options), compound options, dual-currency or dual-equity options, 

Second, to apply the Cox-Ross technique to a given derivative security, Synthetic convertible bonds, spread-lock interest rate swaps, rainbow o p  
tions, and many other exotic securities. In each of these cases, closed-form 

we must first prove that the security can be priced by arbitrage considera- 
tions alone. Recall that in the Black-Scholes framework, the no-arbitrage pricing formulas are available anly for a very small set of processes for the 

underlying asset's  rice, and a great deal of further research is needed to 
condition was sufficient to completely determine the option price only be- check whether such processes actually fit the data. Moreover, in many of 
cause we were able to construct a dynamic portfolio of stocks, bonds, and 



392 9. Derivative Pricing Models 

these cases, analytical expressions for hedging positions in these securities that volatilities do shift over time in random fashion, it is clear that issues 
do not exist and must also be determined empirically. regarding market incompleteness are central to the pricing of derivative 

There are many unsettled issues in the statistical inference of continuous- securities. 
time processes with discretely sampled data. Currently, the most pressing In this chapter we have only touched upon a small set of issues that 
issue is the difficulty in obtaining consistent estimates of the parameters of surround derivatives research, those that have received the least attention 
It6 processes with nonlinear drift and/or diffusion coefficients. For many in the extant literature, with the hope that a wider group of academics and 
It6 processes of interest, we do not have closed-form expressions for their investment professionals will be encouraged to join in the fray and quicken 
transition densities and hence maximum likelihood estimation is not feasi- the progress in this exciting area. 
ble. The GMM approach of Hansen and Scheinkman (1995) may be the 
most promising alternative, and empirical applications and Monte Carlo 
studies are sure to follow. Problems-Chapter 9 

Another area of active current research involves developing better mod- 
els of fundamental asset price dynamics. For example, casual empirical ob- 9.1 Show that the continuous-time process p,(t) of Section 9.1.1 converges 
servation suggests the presence ofjump components in asset prices that are in distribution to a normally distributed continuous-time process p(t)  by 
responsible for relatively large and sudden movements, but occur relatively calculating the the moment-generating function of p,(t) and taking limits. 
infrequently and are therefore considerably more challenging to estimate 9.2 Derive (9.3.30)a nd (9.3.31)e xplicitly by evaluating and inverting the 
precisely.28 Indeed, there is even some doubt as to whether such jump Fisher information matrix in (9.3.7)f or the maximum likelihood estimators 
processes can ever be identified from discretely sampled price data since j.2 and c?* of the parameters of a geometric Brownian motion based on 
the very act of discrete-sampling destroys the one clear distinction between regularly sampled data. 
diffusion processes and jump processes-the continuity of sample paths. 

The difficulties in estimating parametric models of asset price dynamics 9.3 Derive the maximum likelhood estimators /Is,2 a,n d f of the param- 
have led to several attempts to capture the dynamics nonparametrically. For eters of the trending Ornstein-Uhlenbeck process (9.3.46),a nd calculate 
example, by placing restrictions on the drift coefficient of a diffusion pro- their asymptotic distribution explicitly using (9.3.7).  How do these three 
cess, Kt-Sahalia (1993) proposes a nonparametric estimator of its diffusion estimators differ in their asymptotic properties under standard asymptotics 
coefficient and applies this estimator to the pricing of interest rate options. and under continuous-record asymptotics? 
Longstaff (1995) proposes a test of option-pricing models by focusing on 9.4 You are currently managing a large pension fund and have invested 
the risk-neutral distribution implicit in option prices. And Hutchinson, Lo, most of it in IBM stock. Exactly one year from now, you will have to liquidate 
and Poggio (1994) attempt to price derivative securities via neural network your entire IBM holdings, and you are concerned that it may be an inauspi- 
models. Although it is still too early to tell if these nonparametric and highly cious time to sell your position. CLM Financial Products Corporation has 
data-intensive methods will offer improvements over their parametric coun- come to you with the following proposal: For a fee to be negotiated, they 
terparts, the preliminary evidence is quite promising. In Chapter 12, we will agree to buy your entire IBM holdings exactly one year from now, but 
review some of these techniques and present an application to the pricing at a price per share equal to the maximum of the daily closing prices over 
and hedging of derivative securities. the one-year period. What fee should you expect in your negotiations with 

Closely related to the issue of stock price dynamics are several open CLM? Specifically: 
questions regarding the pricing of options in incomplete markets, markets 9.4.1 Estimate the current (time 0 )f air market price H(0)o f the option 
in which the sources of uncertainty affecting the fundamental asset are not to sell at the maximum using Monte Carlo simulation. For simplicity, 
spanned by traded securities. For, example, if the volatility of the funda- assume that IBM's stock price P( t )  follows a geometric Brownian motion 
mental asset's price is stochastic, it is only under the most restrictive set (9.2.2)s o that 
of assumptions that the price of an option on such an asset may be deter- 
mined by arbitrage arguments. Since there is almost universal agreement 

"See, for example, Ball and Torous (1983, 198.5). Merton (1976b) develops an option- and use daily returns of IBM stock over the most recent five-year period to 
pricing formula for combined diffusion/jump processes. See also Merton (1976a) for more estimate the parameters p and a2 to calibrate your simulations. Assume 
general discussion of the impact of misspecifylngstock price dynamics on the pricingof options. 



394 9. Derivative Pricing Models 

that there are 253 trading days in a year and that market prices have no 
volatility when markets are closed, i.e., weekends, holidays. 

9.4.2 Provide a 95%c onfidence interval for H(O)a nd an estimate of the Fixed-Income Securities 
number of simulations needed to yield a price estimate that is within $.05 
of the true price. 

9.4.3 How does this price compare with the price given by the Goldman- 
Sosin-Gatto formula? Can you explain the discrepancy? Which price 
would you use to decide whether to accept or reject CLM's proposal? 

IN THIS CHAPTERand the nextwe turn our attention to the bond markets. We 
study bonds that have no call provisions or default risk, so that their payments 
are fully specified in advance. Such bonds deserve the name $xed-income 
securities that is often used more loosely to describe bonds whose future 
payments are in fact uncertain. In the US markets, almost all true fixed- 
income securities are issued by the US Treasury. Conventional Treasury 
securities make fixed payments in nominal terms, but in early 1996 the 
Treasury announced plans to issue indexed bonds whose nominal payments 
are indexed to inflation so that their payments are fixed in real terms.' 

Many of the ideas discussed in earlier chapters can be applied to fixed- 
income securities as well as to any other asset. But there are several reasons to 
devote special attention to fixed-income securities. First, the fixed-income 
markets have developed separately from the equity markets. They have 
their own institutional structure and their own terminology. Likewise the 
academic study of fixed-income securities has its own traditions. Second, 
the markets for Treasury securities are extremely large regardless of whether 
size is measured by quantities outstanding or quantities traded. Third, fixed- 
income securities have a special place in finance theory because they have 
no cash-flow uncertainty, so their prices vary only as discount rates vary. By 
studying fixed-income securities we can explore the effects of changing dis- 
count rates without having to face the complications introduced by changing 
expectations of future cash flows. The prices of conventional Treasury se- 
curities carry information about nominal discount rates, while the prices 
of indexed securities carry information about real discount rates. Finally, 
many other assets can be seen as combinations of fixed-income securities 
and derivative securities; a callable bond, for example, is a fixed-income 
security less a put option. 

'such bonds have already been issued by the UK, Canadian, and several othergovemments. 
See Campbell and Shiller (1996) for a review. 



396 10. Fixed-Income Securities 10.1. Basic Concepts 397 

The literature on fixed-income securities is vast.' We break it into two 10.1.1 Discount Bondr 
main parts. First, in this chapter we introduce basic concepts and discuss We first define and illustrate basic bond market concepts for discount bonds. 
empirical work on linear time-series models of bond yields. This work is The yield to maturity on a bond is that discount rate which equates the present 
only loosely motivated by theory and has the practical aim of exploring the value of the bond's payments to its price. Thus if P,, is the time t price of 
forecasting power of the term structure of interest rates. In Chapter 11 we a discount bond that makes a single payment of $1 at time t + n, and Yn, is 
turn to more ambitious, fully specified term-structure models that can be the bond's yield to maturity, we have 
used to price interest-rate derivative securities. 

10.1 Basic Concepts so the yield can be found from the price as 

In principle a fixed-income security can promise a stream of future payments 
of any form, but there are two classic cases. It is common in the empirical finance literature to work with log or continu- 

Zermoupon bonds, also called discount bonds, make a single payment at a ously compounded variables. This has the usual advantage that it transforms 
date in the future known as the maturity date. The size of this payment is the nonlinear equation (10.1.2) into a linear one. Using lowercase letters 
the face value of the bond. The length of time to the maturity date is the for logs the relationship between log yield and log price is 
maturity of the bond. US Treasury bills (Treasury obligations with maturity 
at issue of up to 12 months) take this form. 

Coupon bonds make coupon payments of a given fraction of face value at 
equally spaced dates up to and including the maturity date, when the face 
value is also paid. US Treasury notes and bonds (Treasury obligations with The term structure of interest rates is the set of yields to maturity, at a given 
maturity at issue above 12 months) take this form. Coupon payments on time, on bonds of different maturities. The yield spread Snt - Ynt-  f i t ,o r in 
Treasury notes and bonds are made every six months, but the coupon rates log terms snt = ynt - yl,, is the difference between the yield on an n-period 
for these instruments are normally quoted at an annual rate; thus a 7% bond and the yield on a one-period bond, a measure of the shape of the 
Treasury bond actually pays 3.5% of face value every six months up to and term structure. The yield curve is a plot of the term structure, that is, a plot 
including maturity3 of Ynt or ynt against n on some particular date t .  The solid line in Figure 

Coupon bonds can be thought of as packages of discount bonds, one 10.1.1 shows the log zerocoupon yield curve for US Treasury securities at 
corresponding to each coupon payment and one corresponding to the final the end of January 1987 .~T his particular yield curve rises at first, then 
coupon payment together with the repayment of principal. This is not falls at longer maturities so that it has a hump shape. This is not unusual, 
merely an academic concept, as the principal and interest components of although the yield curve is most commonly upward-sloping over the whole 
US Treasury bonds have been traded separately under the Treasury's STRIPS range of maturities. Sometimes the yield curve is inverted, sloping down over 
(Separate Trading of Registered Interest and Principal Securities) program the whole range of maturities. 
since 1985, and the prices of such Treasury strips at all maturities have been 
reported daily in the Wall Street Journal since 1989. Holding-Pen'od Returns 

The holding+eriod return on a bond is the return over some holding period 
less than the bond's maturity. In order to economize on notation, we spe- 
cialize at once to the case where the holding period is a single period.5 We 

 ortu tun at el^ it has increased in quality since Ed Kane's judgement: "It is generally agreed 4This curve is not based on quoted strip prices, which are readily available only for recent 
that, ceteris paribus, the fertilityof a field is roughly proportional to the quantity of manure that Years, but is estimated from the prices of coupon-bearing Treasury bonds. Figure 10.1.1 is 
has been dumped upon it in the recent past. By this standard, the term structure of interest rates due to McCulloch and Kwon (1993) and uses McCulloch's (1971, 1975) estimation method as 
has become . . . an extraordinarily fertile field indeed" (Kane [1970]). See Melino (1988) or $mussed in section 10.1.3 below. 
Shiller (1990) for excellent recentsurveys, and Sundaresan (1996) for a book-length treatment. 5~hiller(1 990) gives a much more comprehensive treatment, which requires more com- 

%ee a textbook such as Fabozzi and Fabozzi (1995) or ~hbozzi( 1996) for further details plicated notation. 
on the markets for US Treasury securities. 



398 10. Fixed-Income Securities 10.1. Basic Concepts 399 

Equation (10.1.5) can be rearranged so that it relates the log bond price 
today to the log price tomorrow and the return over the next period: p,, = 
r,,,+l +p,-lvt+l. One can solve this difference equation forward, substituting 
out future log bond prices until the maturity date is reached (and noting 

Forward Rate 

.2
that the log price at maturity equals zero) to obtain p,t = - rn-i,t+l+i, 

c  z -  
a . or in terms of the yield 

,,  
.. 

, \ 
, 

,,  \ 

8 ,,  \ 
\ 

\ 
\ 
\ 

z \ 
Zero-Coupon Yield This equation shows that the?l og yield to maturity on a zero-coupon bond 

\ 

3 \ 
- \ equals the average log return per period if the bond is held to maturity. 

Forward Rates 
\ Bonds of different maturities can be combined to guarantee an interest rate 
\ on a fixed-income investment to be made in the future; the interest rate on 

~ ' ' ' " " ' ~ ' " ' ~ ' ' ' ' 1 ' ' ' ' 1 1 ' ~  

0 5 10 15 20 25 this investment is called a forward rate.6 
Maturity in Years 

To guarantee at time tan interest rate on a one-period investment to be 
made at time t + n, an investor can proceed as follows. The desired future 

Figurn 10.1. ZemCoupon Yield and Forward-Rate Curves in January 1987 investment will pay $1 at time t + n + 1 so she first buys one (n + 1)-period 
bond; this costs Pn+l,att  time t and pays $1 at time t + n + 1. The investor 
wants to transfer the cost of this investment from time t to time t + n; to do 
this she sells Pn+l,t/Pn-tp eriod bonds. This produces a positive cash flow of 

define R.,,,t+l as the one-period holding-period return on an n-period bond Pnt (Pn+l,t/Pnt=)  P,+lvat t time t, exactly enough to offset the negative time 
purchased at time t and sold at time t + 1. Since the bond will be an (n - 1)- t cash flow from the first transaction. The sale of n-period bonds implies a 
period bond when it is sold, the sale price is Pn-l,t+aln d the holding-period negative cash flow of Pn+l,t/Pantt t ime t + n. This can be thought of as the 
return is cost of the one-period investment to be made at time t + n. The cash flows 

resulting from these transactions are illustrated in Figure 10.2. 
The forward rate is defined to be the return on the time t+ n investment 

of P,+1,t/Pnt: 

The holding-period return in (10.1.4) is high if the bond has a high yield 
when it is purchased at time t, and if it has a low yield when it is sold at time 
t + 1 (since a low yield corresponds to a high price). 

Moving to logs for simplicity, the log holding-period return, rn,,+l In the notation Fntt he first subscript refers to the number of periods ahead 
log(1 + %,t+l) ,  is that the one-period investment is to be made, and the second subscript 

refers to the date at which the forward rate is set. At the cost of additional 
complexity in notation we could also define forward rates for multiperiod 
investments, but we do not pursue this further here. 

The last equality in (10.1.5) shows how the holding-period return is deter- 6 ~ exnam ple of forward trading is the when-issued market in US Treasury securities. After 
mined by the beginning-of-period yield (positively) and the change in the an auction of new securities is announced but before the securities are issued, the securities 

yield over the holding period (negatively). are traded in the when-issued market, with settlement to occur when the securities are issued. 



400 10. Fixed-Income Securities 10.1. Basic Concqbts 40 1 

time t t + n  t + n + l  marginal unit, so the average cost rises when the marginal cost is above the 
Transactions average cost. Conversely, the average cost falls when the marginal cost is 

below the average cost. 
Buy 1 
(n + 1)-period - 
bond 10.1.2 Coupon Bonds 

As we have already emphasized, a coupon bond can be viewed as a package 
of discount bonds, one with face value equal to the coupon for each date at 

sell pn+l,t/pnt (2p n)t   
n-period bonds which a coupon is paid, and one with the same face value and maturity as 

the coupon bond itself. Figure 10.3 gives a time line to illustrate the time 
pattern of payments on a coupon bond. 

Net 0 - The price of a coupon bond depends not only on its maturity nand the 
date t, but also on its coupon rate. To keep notation as simple as possible, 
we define a period as the time interval between coupon payments and C as 

Figurn 10.2. Cash Flows in a Forward Transaction the coupon rate perperiod. In the case of US Treasury bonds a period is six 
months, and C is one half the conventionally quoted annual coupon rate. 

Moving to logs for simplicity, the n-period-ahead log forward rate is We write the price of a coupon bond as Pento  show its dependence on the 
coupon rate. 

fnt = pnt  - pn+l,t The per-period yield to maturity on a coupon bond, Yenti,s  defined as 
that discount rate which equates the present value of the bond's payments 
to its price, so we have 

Equation (10.1.8) shows that the forward rate is positive whenever discount In the case of US Treasury bonds, where a period is six months, Yent is the 
bond prices fall with maturity. Also, the forward rate is above both the n- six-month yield and the annual yield is conventionally quoted as twice Yent. 
period and the (n+  1)-period discount bond yields when the (n + 1)-period Equation (10.1.9) cannot be inverted to get an analytical solution for 
yield is above the n-period yield, that is, when the yield curve is upward- Ye,,. Instead it must be solved numerically, but the procedure is straightfor- 
sloping.' This relation between a yield to maturity and the instantaneous ward since all future payments are positive so there is a unique positive real 

forward rate at that maturity is analogous to the relation between marginal solution for yCnt.*U nlike the yield to maturity on a discount bond, the yield 

and average cost. The yield to maturity is the average cost of borrowing for to maturity on a coupon bond does not necessarily equal the per-period 

n periods, while the forward rate is the marginal cost of extending the time return if the bond is held to maturity. That return is not even defined until 

period of the loan. one specifies the reinvestment strategy for coupons received prior to matu- 
rity. The yield to maturity equals the per-period return on the coupon bond 

Figure 10.1 illustrates the relation between the forward-rate curve (shown 
held to maturity only if coupons are reinvested at a rate equal to the yield 

as a dashed line) and the yield curve (a solid line). The forward-rate curve 
to maturity. 

lies above the yield curve when the yield curve is upwardsloping, and below 
The implicit yield formula (10.1.9) simplifies in two important special 

it when the yield curve is downwardsloping. The two curves cross when 
cases. First, when Pen=t  1, the bond is said to be selling at par. In this case 

the yield curve is flat. These are the standard properties of marginal and 
the yield just equals the coupon rate: Yent = C. Second, when maturity n 

average cost curves. When the cost of a marginal unit exceeds the cost of 
an average unit then the average cost increases with the addition of the 'with negative future payments, there can be multiple positive real solutions to (10.1.9). 

In the analysis of investment projects, the discount rate that equates the present value of a 
7 ~ thse  time unit shrinks relative to the bond maturity n, the formula (10.1.8)a pproaches project to its cost is known as the internal rate of return. When projects have some negative cash 

fnl = ynl + naynl /an ,t he n-period yield plus n times the slope of the yield curve at maturity n. flows in the future, there can be multiple solutions for the internal rate of return. 



10. I .  Basic Concepts 403 

is infinite, the bond is called a consol or perpetuity. In this case the yield just 
equals the ratio of the bond price to the coupon rate: YCm=t  C/P,,,. 

Duration and Immunization 
For discount bonds, maturity measures the length of time that a bondholder 
has invested money. But for coupon bonds, maturity is an imperfect mea- 
sure of this length of time because much of a coupon bond's value comes 
from payments that are made before maturity. Macaulay's duration, due to 
Macaulay (1938) ,i s intended to be a better measure; like maturity, its units 
are time periods. To understand Macaulay's duration, think of a coupon 
bond as a package of discount bonds. Macaulay's duration is a weighted av- 
erage of the maturities of the underlying discount bonds, where the weight 
on each maturity is the present value of the corresponding discount bond 
calculated using the coupon bond's yield as the discount rate:g 

The maturity of the first component discount bond is one period and this 
receives a weight of C / ( 1 +  Ycnt)t,h e present value of this bond when YCnits  
the discount rate; the maturity of the second discount bond is two and this 
receives a weight of C / ( 1  + yCnt)*a; nd so on until the last discount bond 
of maturity n gets a weight of (1  + C ) / ( 1+  YCnt)"T. o convert this into an 
average, we divide by the sum of the weights Cl(1  + Ycnt)+  C / ( 1+  yCnt)*+  

+ (1 + C ) / ( 1+  YCnt)"w, hich from (10.1.9) is just the bond price Pent. 
These calculations are illustrated graphically in Figure 10.3. 

When C = 0 ,  the bond is a discount bond and Macaulay's duration 
equals maturity. When C > 0 , M acaulay's duration is less than maturity and 
it declines with the coupon rate. For a given coupon rate, duration declines 
with the bond yield because a higher yield reduces the weight on more 
distant payments in the average (10.1.10).T he duration formula simplifies 
when a coupon bond is selling at par or has an infinite maturity. A par 
bond has price Pent = 1 and yield Ycnt = C,  so duration becomes Dent = 
(1 - (1 + Y,n t ) -n ) / ( l-  (1  + yCnt)-l) .A  consol bond with infinite maturity 
has yield YCwt=  C/Pc,, so duration becomes DCmt=  (1 + Y,,t)/ Yc,t. 

Numerical examples that illustrate these properties are given in Ta- 
ble 10.1. The table shows Macaulay's duration (and modified duration, de- 
fined in (10.1.12) below, in parentheses) for bonds with yields and coupon 

g~acaulaayl so suggests that one could use yields on discount bonds rather than the yield 
on the coupon bond to calculate the present value of each coupon payment. However this 
approach requires that one measure a complete zerocoupon term structure. 



10. Fixed-Income Secun'ties 10.1. Basic Concepts 

important property. It is the negative of the elasticity of a coupon bond's 
Table 10.1. Macaulay 's and modified duration for selected bonds. price with respect to its gross yield (1 + yCnt):lo 

Maturity (years) 

1 2 5 10 30 
In industry applications, Macaulay's duration is often divided by the 

Coupon rate 0% gross yield (1 + Y,,,) to get what is called modijied duration: 
Yield 0% 1.000 

(1. OOO) 

5% 1.000 
Modified duration measures the proportional sensitivity of a bond's pric

(0.976) e 
to a small absolute change in its yield. Thus if modified duration is 10, an 

10% 1.000 increase in the yield of 1 basis point (say from 3.00% to 3.01%) will cause a 
(0.952) 10 basis point or 0.10% drop in the bond price.11 

Macaulay's duration and modified duration are sometimes used to an- 
Coupon rate 5% swer the following question: What single coupon bond best approximates 
Yield 0% 0.988 1.932 4.550 8.417 21.150 - the return on a zerocoupon bond with a given maturity? This question is 

(0.988) (1.932) (4.550) (8.417) (21.150) of practical interest because many financial intermediaries have long-term 
zerocoupon liabilities, such as pension obligations, and they may wish to 

5% 0.988 1.928 4.485 7.989 15.841 20.500 
match or immunize these liabilities with coupon-bearing Treasury bonds.12 

(0.964) (1.881) (4.376) (7.795) (15.454) (20.000) 
Although today stripped zerocoupon Treasury bonds are available, they 

10% 0.988 1.924 4.414 7.489 10.957 10.500 may be unattractive because of tax clientele and liquidity effects, so the im- 
(0.940) (1.832) (4.204) (7.132) (10.436) (10.000) munization problem remains relevant. If there is a parallel shift in the yield 

curve so that bond yields of all maturities move by the same amount, then 
Coupon rate 10% a change in the zerocoupon yield is accompanied by an equal change in 
Yield 0% 0.977 1.875 4.250 7.625 18.938 - the coupon bond yield. In this case equation (10.1.11) shows that a coupon 

(0.977) (1.875) (4.250) (7.625) (18.938) bond whose Macaulay duration equals the maturity of the zero-coupon li- 
ability (equivalently, a coupon bond whose modified duration equals the 
modified duration of the zerocoupon liability) has, to a first-order approx- 
imation, the same return as the zero-coupon liability. This bond--or any 
portfolio of bonds with the same duration-solves the immunization prob- 
lem for small, parallel shifts in the term structure. 

Although this approach is attractively simple, there are several reasons 
why it must be used with caution. First, it assumes that yields of all maturi- 

The table reports Macaulay's duration and, in parentheses, modified duration for bonds with 
selected yields and maturities. Duration, yield, and maturity are stated in annual units but the ties move by the same amount, in a parallel shift of the term structure. We 
underlying calculations assume that bond payments are made at six-month intervals. 

l0'I'he elasticity of a variable B with respect to a variable A is defined to be the derivative of 
Bwith respect to A, times A/B: (dB/dA)(A/B). Equivalently, it is the derivative of log(B) with 
respect to I O ~ ( A ) .  

1 1N ote that if duration is measured in six-month time units, then yields should be measured 
rates of 0%, 5%, and lo%,a nd maturities ranging from one year to infinity. on a six-month basis. One can convert to an annual basis by halving duration and doubling 
Duration is given in years but is calculated using six-month periods as would pelds. The numbers in Table 10.1 have been annualized in this way. 
be appropriate for US Treasury bonds. '2~mmunizationw as originally defined by Reddington (1952) as "the investment of the 

If we take the derivative of (10.1.9) with respect to Ycnto, r equivalently assets in such a way that the existing business is immune to a general change in the rate of 
Interest". Fabozzi and Fabozzi (1995), Chapter 42, gives a comprehensive discussion. 

with respect to (1 + Y,,,), we find that Macaulay's duration has another very 



406 IO. Fixed-Income Securities 10.1. Basic Concepts 

show in Section 10.2.1 that historically, movements in short-term interest 
rates have tended to be larger than movements in longer-term bond yields. 
Some modified approaches have been developed to handle the more real- 
istic case where short yields move more than long yields, so that there are 
nonparallel shifts in the term structure (see Bierwag, Kaufman, and Toevs 
[1983], Granito [1984], Ingersoll, Skelton, and Weil [I9781)  . 

Second, (10.1.11) and (10.1.12) give first-order derivatives so they apply 
only to infinitesimally small changes in yields. Figure 10.4 illustrates the 
fact that the relationship between the log price and the yield on a bond is 
convex rather than linear. The slope of this relationship, modified duration, 
increases as yields fall (a fact shown also in Table 10.1). This may be taken 
into account by using a second-rder derivative. The conuexity of a bond is 
defined as 

i(i+l) n(n+l) 
a2pCn1t  --  C CI1=1 (1+~,.~)'+: '( 1+Y,dnt2 

Convexity r -a -  , (10.1.13) 
 ,Y: pCnt Pent 

and convexity can be used in a second-order Taylor series approximation of 
the price impact of a change in yield: 

Figure 10.4. The Price-Yield Relationship 

= (- modified duration) d Ye, loglinear approximate return formula (7.1.19) derived in Chapter 7, and 

+ 1 apply it to the one-period return re.,,,+l on an n-period coupon bond: 
-2 convexity 

Finally, both Macaulay's duration and modified duration assume that Here the log nominal coupon c plays the role of the dividend on stock, 
cash flows are fixed and do not change when interest rates change. This but of course it is fixed rather than random. The parameters p and k are 
assumption is appropriate for Treasury securities but not for callable securi- given by p - 1/(1 + exp(-)) and k -- - lo@) - (1 - p) log(l/p - 1). 
ties such as corporate bonds or mortgage-backed securities, or for securities When the bond is selling at par, then its price is $1 so its log price is zero 
with default risk if the probability of default varies with the level of interest and p = 1/(1 + C) = (1 + Yen,)-'. It is standard to use this value for p, 
rates. By modelling the way in which cash flows vary with interest rates, i t  is which gives a good approximation for returns on bonds selling close to 
possible to calculate the sensitivity of prices to interest rates for these more par. 
complicated securities; this sensitivity is known as effectived uration.13 One can treat (10.1.15), like the analogous zero-coupon expression 

(10.1.5), as a difference equation in the log bond price. Solving forward to 
A Loglinear Model for Coupon Bonds the maturity date one obtains 
The idea of duration has also been used in the academic literature to find 
approximate linear relationships between log coupon bond yields, holding- 
period returns, and forward rates that are analogous to the exact relation- 
ships for zero-coupon bonds. To understand this approach, start from the 

This equation relates the price of a coupon bond to its stream of coupon pay- 
'%ee Fabozzi and Fabozzi (1995), Chapters 28-30. and Fabozzi (1996) for a discussion of ments and the future returns on the bond. A similar approximation of the 

various methods used by fixed-income analysts to calculate effective duration. 



408 10. Fixed-Income Securities 10.1. Basic Concepts 409 

log yield to maturity yenl shows that it satisfies an equation of the same form: 10. I .3 Estimating the Zero-Coupon Term Structure 

The classic immunization problem is that of finding a coupon bond or 
portfolio of coupon bonds whose return has the same sensitivity to small 
interest-rate movements as the return on a given zerocoupon bond. Alter- 
natively, one can try to find a portfolio of coupon bonds whose cash flows 
exactly match those of a given zerocoupon bond. In general, this portfolio 
will involve shortselling some bonds. This procedure has academic interest 

Equations (10.1.16)a nd (10.1. l 7 )  as well; one can extract an implied zero-coupon term structure from the 
together imply that the n-period coupon 

bond yield satisfies ycnt % ( ( 1  - p ) / ( l  - ,on))  ,oire,n-i,t+l+i.T coupon term structure. 
hus al- 

If the complete zerocoupon term structure-that is, the prices of dis- 
though there is no exact relationship there is an approximate equality be- 

count bonds Pl . . . Pn maturing at each coupon date-is known, then it is 
tween the log yield to maturity on a coupon bond and a weighted average 

easy to find the price of a coupon bond as 
of the returns on the bond when it is held to maturity. 

Equation (10.1.11)t ells us that Macaulay's duration for a coupon bond is 
the derivative of its log price with respect to its log yield. Equation (10.1.17) Pcn = f ' l C + P 2 C + . . . + P n ( 1 + C )  . (10.1.21) 

gives this derivative as Time subscripts are omitted here and throughout this section to economize 
on notation. 

Similarly, if a complete coupon term structure-the prices of coupon 
bonds Pcl . . . Penm aturing at each coupon date-is available, then (10.1.21) 

where the second equality uses ,o = ( 1+  As noted above, this relation call be used to back out the implied zero-coupon term structure. Starting 
with a one-period coupon bond, Pcl = P l ( l  + C)  so Pl = Pcl / ( l+  C ) .  We 

between duration and yield holds exactly for a bond selling at par. 
can then proceed iteratively. Given discount bond prices f i ,.  . . , Pn-1, we 

Substituting (10.1.17)a nd (10.1.18) into (10.1.15),w e obtain a loglin- 
can find Pn as 

ear relation between holding-period returns and yields for coupon bonds: 

Sometimes the coupon term structure may be more-thancomplete in 
This equation was first derived by Shiller, Campbell, and Schoenholtz the sense that at least one coupon bond matures on each coupon date 
(1983).14 It is analogous to (10.1.5) for zerocoupon bonds; maturity in and several coupon bonds mature on some coupon dates. In this case 
that equation is replaced by duration here, and of course the two equations (10.1.21) restricts the prices of some coupon bonds to be exact functions 
are consistent with one another for a zerocoupon bond whose duration of the prices of other coupon bonds. Such restrictions are unlikely to hold 
equals its maturity. in practice because of tax effects and other market frictions. To handle this 

A similar analysis for forward rates shows that the n-period-ahead 1- Carleton and Cooper (1976) suggest adding a bond-specific error term to 
period forward rate implicit in the coupon-bearing term structure is (10.1.21)a nd estimating it as a cross-sectional regression with all the bonds 

outstanding at a particular date. If these bonds are indexed i = 1 . . . I ,  then 
Dc,n+l yc,n+l,t - Den Ycnt 

fnt the regression is 
Dc, n+l - Den 

This formula, which is also due to Shiller, Campbell, and Schoenholtz 
(1983),r educes to the discount bond formula (10.1.8)w hen duration equals where Ci is the coupon on the ith bond and ni is the maturity of the ith bond. 
maturity. The regressors are coupor, payments at different dates, and the coefficients 

are the discount bond prices 4,j  = 1 . . . N, where N = maxi ni is the longest 
I4Shiller,C ampbell, and Schoenholtz use Ycnti nstead of yCnt,b ut these are equivalent to the coupon bond maturity. The system can be estimated by OLS provided that 

same first-order approximation used to derive (10.1.19). They also derive formulas relating 
multiperiod holding returns to yields. the coupon term structure is complete and that I 2 N .  



10.1. Basic Concepts 
410 10. Fixed-Income Securities 

Spline Estimation points, there are K - 1 subintervals in each of which the spline is a polyno- 
mial. The spline has K - 2 + r free parameters, r for the first subinterval 

In practice the term structure of coupon bonds is usually incomplete, and 
this means that the coefficients in (10.1.23) are not identified without im- and 1 (that determines the unrestricted rth derivative) for each of the K - 2 

following subintervals. McCulloch suggests that the knot points should be 
posing further restrictions. It seems natural to impose that the prices of dis- 

chosen so that each subinterval contains an equal number of bond maturiq 
count bonds should vary smoothly with maturity. McCulloch (1971, 1975) 
suggests that a convenientway to do this is to write Pn,r egarded as a function dates. 

If forward rates are to be continuous, the discount function must have at 
of maturity P(n), as a linear combination of certain prespecified functions: 

least one continuous derivative. Hence a quadratic spline, estimated by Mc- 
Culloch (1971),i s the lowest-order spline that can fit the discount function. 
If we require that the forward-rate curve should also be continuously differ- 
entiable, then we need to use a cubic spline, estimated by McCulloch (1975) 
and others. McCulloch's papers give the rather complicated formulas for 

McCulloch calls P(n) the discount function. The J(n) in (10.1.24) are known the functions J(n) that make P(n) a quadratic or cubic spline." 
functions of maturity n, and the aj are coefficients to be estimated. Since 
P(0) = 1, we must have J(0) = 0 for all j. Tax Effects 

Substituting (10.1.24) into (10.1.23) and rearranging, we obtain a re- OLS estimation of (10.1.25) chooses the parameters 9 so that the bond pric- 
gression equation ing errors ui are uncorrelated with the variables XV that define the discount 

function. If a sufficiently flexible spline is used, then the pricing errors will 
be uncorrelated with maturity or any nonlinear function of maturity. Pricing 
errors may, however, be correlated with the coupon rate which is the other 
defining characteristic of a bond. Indeed McCulloch (1971) found that his 
model tended to underprice bonds that were selling below par because of 

where n i  - PC,,-,  1 - Cini, the difference between the coupon bond price 
their low coupon rates. 

and the undiscounted value of its future payments, and Xq = J(ni) + 
C, xy~ McCulloch (1975) attributes this to a tax effect. US Treasury bond 

J ( I )~.   Like equation (10.1.23), this equation can be estimated by 
coupons are taxed as ordinary income while price appreciation on a coupon- 

OLS, but there are now only J coefficients rather than N . ' ~  
bearing bond purchased at a discount is taxed as capital gains. If the capital 

A key question is how to specify the functions J(n) in (10.1.24). One 
gains tax rate tg is less than the ordinary income tax rate t (as has often 

simple possibility is to make P(n), the discount function, a polynomial. To do 
been the case historically), then this can explain a price premium on bonds 

this one sets J(n) = nJ. Although a sufficiently high-order polynomial can 
selling below par. For an investor who holds a bond to maturity the pricing 

approximate any function, in practice one may want to use more parameters 
to fit the discount function at some maturities rather than others. For formula (10.1.21) should be modified to 

example one may want a more flexible approximation in maturity ranges n 
where many bonds are traded. PC, = [ l  - r,(l - Pcn)]P(n)+  (1 - t ) C  x P(i). (10.1.26) 

To meet this need McCulloch suggests that P(n) should be a spline i= 1 

functicn.16 An rth-order spline, defined over some finite interval, is a piece- The spline approach can be modified to handle tax effects like that in 
wise rth-order polynomial with r-1  continuous derivatives; its rth derivative (10.1.26), at the cost of some additional complexity in estimation. Once 
is a step function. The points where the rth derivative changes discontin- fax effects are included, coupon bond prices must be used to construct the 
uously (including the points at the beginning and end of the interval over variables Xq on the right-hand side of (10.1.25). This means that the bond 
which the spline is defined) are known as knot points. If there are K knot pricing errors are correlated with the regressors so the equation must be 

I 5 ~ hbeo nd pricing errors are unlikely to be homoskedastic. McCulloch argues that the 17 Adams and van Deventer (1994) argue for the use of a fourth-order spline, with the 
standard deviation of u, is proportional to the bid-ask spread for bond i, and thus weights cubic term omitted, in order to maximize the 'smoothness" of the forward-rate curve, where 
each observation by the reciprocal of its spread. This is not required for consistency, but may Smoothness is defined to be minus the average squared second derivative of the forward-rate 
improve the efficiency of the estimates. curve with respect to maturity. 

'%uits, Mason, and Chan (1978) give an accessible introduction to spline methodolog. 



412 10. Fixed-Income Securities 10.2. Interpreting the T m S tructure $Interest Rates 413 

estimated by instrumental variables rather than simple OLS. Litzenberger more difficult to use than the standard spline and this cost may outweigh the 
and Rolfo (1984) apply a tax-adjusted spline model of this sort to bond exponential spline's desirable long-horizon properties. In any case, forward 
market data from several different countries. rate and yield curves should be treated with caution if they are extrapolated 

The tax-adjusted spline model assumes that the same tax rates are rel- beyond the maturity of the longest traded bond. 
evant for all bonds. The model cannot handle "clientele" effects, in which Some other authors have solved the problem of negative forward rates 
differently taxed investors specialize in different bonds. Schaefer (1981, by restricting the shape of the zerocoupon yield curve. Nelson and Siegel 
1982) suggests that clientele effects can be handled by first finding a set of (1987), for example, model the instantaneous forward rate at maturity n 
taxefficient bonds for an investor in a particular tax bracket, then estimating as the solution to 9 second-order differential equation with equal roots: 
an implied zerocoupon yield curve from those bonds alone. f (n) = Bo + B1 exp(-an) + q 9 B 2  exp(-an). This implies that the discount 

function is doubleexponential: 

Nonlinear Models 
Despite the flexibility of the spline approach, spline functions have some 
unappealing properties. First, since splines are polynomials they imply a This specification generates forward-rate and yield curves with a desirable 
discount function which diverges as maturity increases rather than going range of shapes, iqcluding upward-sloping, inverted, and humpshaped. 
to zero as required by theory. Implied forward rates also diverge rather Svensson (1994) has developed this specification further. Other recent work 
than converging to any fixed limit. Second, there is no simple way to ensure has generated bond-price formulas from fully specified generalequilibrium 
that the discount function always declines with maturity (i.e., that all forward models of the term structure, which we discuss in Chapter 11. 
rates are positive). The forward curve illustrated in Figure 10.1 goes negative 
at a maturity of 27 years, and this behavior is not uncommon (see Shea 
[1984]). These problems are related to the fact that a flat zerocoupon 10.2 Interpreting the Term Structure of Interest Rates 
yield curve implies an exponentially declining discount function, which is 
not easily approximated by a polynomial function. Since any plausible yield There is a large empirical literature which tests statements about expected- 
curve flattens out at the long end, splines are likely to have difficulties with return relationships among bonds without deriving these statements from 
longer-maturity bonds. a fully specified equilibrium model. For simplicity we discuss this literature 

These difficulties have led some authors to suggest nonlinear alterna- assuming that zerocoupon bond prices are observed or can be estimated 
tives to the linear specification (10.1.24). One alternative, suggested by from coupon bond prices. 
Vasicek and Fong (1982), is to use an exponential spline, a spline applied to 
a negative exponential transformation of maturity. The exponential spline 

10.2.1 The Expectations Hypothesis 
has the desirable property that forward rates and zero-coupon yields con- 
verge to a fixed limit as maturity increases. More generally, a flat yield curve The most popular simple model of the term structure is known as the expec- 
is easy to fit with an exponential spline. tations hypothesis. We distinguish the pure expectations hypothesis (PEH) (PEH),  

Although the exponential spline is appealing in theory, it is not clear that which says that expected excess returns on long-term over short-term bonds 
it performs better than the standard spline in practice (see Shea [19851). are zero, from the expectations hypothesis (EH), which says that expected ex- 
The exponential spline does not make it easier to restrict forward rates to cess returns are constant over time. This terminology is due to Lutz (1940). 
be positive. As for its long-maturity behavior, it is important to remember 
that forward rates cannot be directly estimated beyond the maturity of the a 2 - t  F m o fthe Pure Expectations Hypothesis 
longest coupon bond; they can only be identified by restricting the relation We also distinguish different forms of the PEH, according to the time hori- 
between long-horizon forward rates and shorter-horizon forward rates. The zon over which expected excess returns are zero. A first form of the PEH 
exponential spline, like the standard spline, fits the observed maturity range equates the one-period expected returns on one-period and n-period bonds. 
flexibly, leaving the limiting forward rate and speed of convergence to this The oneperiod return on a one-period bond is known in advance to be 
rate to be determined more by the restrictions of the spline than by any (1 + Ylt), so this form of the PEH implies 
characteristics of the long-horizon data. Since the exponential spline in- 
volves nonlinear estimation of a parameter used to transform maturity, it is 



414 10. Fixed-Income Securities 10.2. Interpreting the T mS tructure of Interest Rates 

where the second equality follows from the definition of holding-period Table 10.2. Means and standard deviations oj term-struc 
return and the fact that (1 + Ynt) is known at time t. 

A second form of the PEH equates the n-period expected returns on Long bond maturity (n) 
one-period and n-period bonds: Variable 

2 3 6 12 24 48 120 

Excessreturn 0.385 0.564 0.848 0.917 0.709 0.614 -0.048 
, + I  (0.644) (1.222) (2.954) (6.218) (11.33) (19.40) (37.08) 

Here (1 + Ynt)"i s the n-period return on an n-period bond, which equals 
Change in yield 0.010 0.010 0.010 0.010 0.011 0.01 1 0.012 

the expected return from rolling over one-period bonds for n periods. It is yn.t+l - ynt (0.592) (0.576) (0.570) (0.547) (0.488) (0.410) (0.310) 
straightforward to show that if (10.2.2) holds for all n, it implies 

Change in yield -0.188 -0.119 -0.056 -0.014 0.011 0.011 0.012 
Y ~ - I . I-+ Iyn t (0.608) (0.586) (0.573) (0.555) (0.488) (0.410) (0.310) 

Yield spread 0.197 0.326 0.570 0.765 0.958 1.153 1.367 
y,,-ylt (0.212) (0.303) (0.438) (0.594) (0.797) (1.012) (1.237) 

Under this form of the PEH, the (n - 1)-period-ahead one-period forward 
rate equals the expected (n - 1)-period-ahead spot rate. Long bond maturities are measured in months. For each variable the table reports the sample 

It is also straightforward to show that if (10.2.2) holds for all n, it implies mean and sample standard deviation (in parentheses) using monthly data over the period 
1952:l-1991:2. The units are annualized percentage points. The underlying data are zero- 
coupon bond yields from McCulloch and Kwon (1993). 

But (10.2.4) is inconsistentwith (10.2.1) whenever interest rates are random. 
The problem is that by Jensen's Inequality, the expectation of the reciprocal 
of a random variable is not the reciprocal of the expectation of that random Var[rn,,+l-  yll],w hich measures the quantitative importance of the Jensen's 
variable. Thus the pure expectations hypothesis cannot hold in both its Inequality effect in a lognormal homoskedastic model. 
one-period form and its n-period form.'' Table 10.2 reports unconditional sample means and standard deviations 

One can understand this problem more clearly by assuming that interest for several term-structure variables over the period 1952:l to 1991:2.19 All 
rates are lognormal and homoskedastic and taking logs of the one-period data are monthly, but are measured in annualized percentage points; that 
PEH equation (10.2.1) and the n-period PEH equation (10.2.4). Noting is, the raw variables are multiplied by 1200. The first row shows the mean 
that from equation (10.1.5) the excess one-period log return on an n-period and standard deviation of excess returns on n-month zerocoupon bonds 
bond is over one-month bills. The mean excess return is positive and rising with 

maturity at first, but it starts to fall at a maturity of one year and is even 
slightly negative for ten-year zerocoupon bonds. 

This pattern can be understood by breaking excess returns into the two 
equation (10.2.1) implies that components on the right-hand side of equation (10.2.5): the yield spread 

( ~ n-t  ylt) between n-period and one-period bonds, and -(n - 1) times the 
change in yield (yn-l,t+l-  ynt)o n the n-period bond. Interest rates of all 
fixed maturities rise during the sample period, as shown in the second row 

while (10.2.4) implies that 
of Table 10.2 and illustrated for one-month and ten-year rates in Figure 10.5. 
At the short end of the term structure this effect is offset by the decline in 
maturity from n to n - 1 as the bond is held for one month; thus the change 

The difference between the right-hand sides of (10.2.6) and (10.2.7) is 

 able 10.2 is an expanded version of a table shown in Campbell (1995). The numbers 
%ox, Ingersoll, and Ross (1981a) make this point very clearly. They also argue that in given here are slightly different from the numbers in that paper because the sample period 

continuous time, only expected equality of instantaneous returns (a model corresponding to used in that paper was 1951:l to 1990:2,a lthough it was erroneously reported to be 1952:l to 
(10.2.1)) is consistent with the absence of arbitrage. But McCulloch (1993) has shown that 1991:2. 
this result depends on restrictive assumptions and does not hold in general. 



10. Fixed-Income Securities 10.2. Interpreting the Term Structure of Interest Rates 417 

4 n  = 24. But it rises to 1.15% for n = 120. This calculation shows that 
the differences between different forms of the PEH are small except for 
very long-maturity zerocoupon bonds. Since these bonds have the most 
imprecisely estimated mean returns, the data reject all forms of the PEH 
at the short end of the term structure, but reject no forms of the PEH at 
the long end of the term structure. In this sense the distinction between 
different forms of the PEH is not critical for evaluating this hypothesis. 

Most empirical research uses neither the one-period form of the PEH 
(10.2.6), nor the n-period form (lO.2.7), but a log form of the PEH that 
equates the expected log returns on bonds of all maturities: 

This model is halfway between equations (10.2.6) and (10.2.7) and can be 
justified as an approximation to either of them when variance terms are 
small. Alternatively, it can be derived directly as in McCulloch (1993). 

Implications of the Log Pure Expectations Hypothesis 
Once the PEH is formulated in logs, it is comparatively easy to state its 
implications for longer-term bonds. The log PEH implies, first, that the 

Figure 10.5. Short- and Long-Ten Interest Rates 1952 to 1991 one-period log yield (which is the same as the one-period return on a one- 
period bond) should equal the expected log holding return on a longer 
n-period bond held for one period: 

in yield (yn-i,t+i - ynt),s hown in the third row of Table 10.2, is negative for 
short bonds, contributing positively to their return.20 At the long end of the Second, a long-term n-period log yield should equal the expected sum of 
term structure, however, the decline in maturity from n to n-1  is negligible, n successive log yields on one-period bonds which are rolled over for n 
and so the change in yield (yn-l,t+l-  ynt)i s positive, causing capital losses on periods: 
long zerocoupon bonds which outweigh the higher yields offered by these 
bonds, shown in the fourth row of Table 10.2. 

The standard deviation of excess returns rises rapidly with maturity. If 
excess bond returns are white noise, then the standard error of the sample Finally, the ( n-  1)-period-ahead one-period log forward rate should equal 
mean is the standard deviation divided by the square root of the sample size the expected one-period spot rate (n - 1) periods ahead: 
(469 months). The standard error for n = 2 is only 0.03%, whereas the 
standard error for n = 120 is 1.71%. Thus the pattern of mean returns is 
imprecisely estimated at long maturities. 

The standard deviation of excess returns also determines the size of the This implies that the log forward rate for a one-period investment to be 
wedge between the one-period and n-period forms of the pure expectations made at a particular date in the future should follow a martingale: 
hypothesis. The difference between mean annualized excess returns under 
(10.2.6) and (10.2.7) is only 0.0003% for n = 2. It is still only 0.11% for 

any of equations (10.2.9), (10.2.10), and (10.2. hold for all n and t, 
'O~nvestorsw ho seek to profit from this tendency of bond yields to fall as maturity shrinks len the other equations also hold for all 

are said to be "riding the yield curve." n and Also, if any of these 



418 10. Fixed-Income Securities 10.2. Interpreting the Term Structure of Interest Rates 419 

equations hold for n = 2 at some date t, then the other equations also Recall that the yield spread between the n-period yield and the one- 
hold for n = 2 and the same date t. Note however that (10.2.9)-(10.2.11) period yield is snt = ynl - yll. Equation (10.1.6) implies that 
are not generally equivalent for particular n and t. 

Alternatives to the Pure Expectations Hypothesis 
The expectations hypothesis (EH) is more general than the PEH in that 
it allows the expected returns on bonds of different maturities to differ by 
constants, which can depend on maturity but not on time. The differences 
between expected returns on bonds of different maturities are sometimes 
called term prmia. The PEH says that term premia are zero, while the EH The second equality in equation (10.2.13) replaces multiperiod interest rate 
says that they are constant through time.*l Like the PEH, the EH can be changes by sums of single-period interest rate changes. The equation says 
formulated for one-period simple returns, for n-period simple returns, or that the yield spread equals a weighted average of expected future interest 
for log returns. If bond returns are lognormal and homoskedastic, as in rate changes, plus an unweighted average of expected future excess returns 
Singleton (1990), then these formulations are consistent with one another on long bonds. If changes in interest rates are stationary (that is, if interest 
because the Jensen's Inequality effects are constant over time. Recent em- rates themselves have one unit root but not two), and if excess returns 
pirical research typically concentrates on the log form of the EH. are stationary (as would be implied by any model in which risk aversion 

Early discussions of the term structure tended to ignore the possibility and bonds' risk characteristics are stationary), then the yield spread is also 
that term premia might vary over time, concentrating instead on their sign. stationary. This means that yields of different maturities are ~ointe~rated.~~ 
Hicks (1946) and Lutz (1940) argued that lenders prefer short maturities The expectations hypothesis says that the second term on the right- 
while borrowers prefer long maturities, so that long bonds should have hand side of (10.2.13) is constant. This has important implications for the 
higher average returns than short bonds. Modigliani and Sutch (1966) relation between the yield spread and future interest rates. It means that the 
argued that different lenders and borrowers might have different prejimed yield spread is (up to a constant) the optimal forecaster of the change in the 
habitats, so that term premia might be negative as well as positive. All these long-bond yield over the life of the short bond, and the optimal forecaster 
authors disputed the PEH but did not explicitly question the EH. More of changes in short rates over the life of the long bond. Recalling that we 
recent work has used intertemporal asset pricing theory to derive both the have dropped all constant terms, the relations are 
average sign and the time-variation of term premia; we discuss this work in 
Chapter 11. 

10.2.2 Yield Spreads and Interest Rate Forecasts and 
We now consider empirical evidence on the expectations hypothesis (EH). 
Since the EH allows constant differences in the expected returns on short- 
and long-term bonds, it does not restrict constant terms so for convenience 
we drop constants from all equations in this section. Equation (10.2.14) can be obtained by substituting the definition of rn,t+l, 

So far we have stated the implications of the expectations hypothesis (10.1.5), into (10.2.9) and rearranging. It shows that when the yield spread 
for the levels of nominal interest rates. In post-World War I1 US data, nom- is high, the long rate is expected to rise. This is because a high yield spread 
inal interest rates seem to follow a highly persistent process with a root very gives the long bond a yield advantage which must be offset by an anticipated 

close to unity, so much empirical work uses yield spreads instead of yield capital loss. Such a capital loss can only come about through an increase 

levels.22 in the long-bond yield. Equation (10.2.15) follows directly from (10.2.13) 
with constant expected excess returns. It shows that when the yield spread is 

 his usage is the most common one in the literature. Fama (1984), Fama (1990), and 
Fama and Bliss (1987), however, use "term premia" to refer to realized, rather than expected, process is discussed further in Chapter 11. 
exce.s s returns on long-term bonds. 2 3 ~ eCea mpbell and Shiller (1987) for a discussion of cointegration in the term structure 

' 
".See Chapters 2 and 7 for a discussion of unit roots. The persistence of the short-rate of interest rates. 



10. Fixed-Income Securities 10.2. Interpreting the T m S tructure of Interest Rates 42 1 

to the expectations hypothesis, we should find Pn = 1. In fact all the 
Table 10.3. Regression coe f~ ic i en tda nd f,.  estimates in Table 10.3 are negative; all are significantly less than one, and 

some are significantly less than zero. When the long-short yield spread is 
Dependent Long bond matunty (n )  high the long yield tends to fall, amplifying the yield differential between 

variable 2 3 6 12 24 48 120 long and short bonds, rather than rising to offset the yield differential as 
Long-yield required by the expectations hypothesis. 
changes 0.003 -0.145 -0.835 -1.435 -1.448 -2.262 -4.226 The regression equation (10.2.16) contains the same information as a 
(10.2.16) (0.191) (0.282) (0.442) (0.599) (1.004) (1.458) (2.076) regression of the excess one-period return on an n-period bond onto the 

Short-rate yield spread snl. Equation (10.2.5) relating excess returns to yields implies 
changes 0.502 0.467 0.320 0.272 0.363 0.442 1.402 that the excess-return regression would have a coefficient of (1 - p,).  Thus 
(10.2.18) (0.096) (0.148) (0.146) (0.208) (0.223) (0.384) (0.147) the negative estimates of Bn in Table 10.3 correspond to a strong positive 

relationship between yield spreads and excess returns on long bonds. This is 
Long bond maturities are measured in months. The first row reports the estimated regression similar to the positive relationship between dividend yields and stock returns 
coefficient fin from (lO.2.16),w ith an asymptotic standard error (in parentheses) calculated to discussed in Chapter 7.25 
allow for heteroskedasticity in the manner described in the Appendix. The second row reports One difficulty with the regression (10.2.16) is that it is particularly sen- 
the the estimated regression coefficient );, from (10.2.18),w ith an asymptotic standard error 
calculated in the same manner, allowing also for residual autocorrelation. The expectations sitive to measurement error in the long-term interest rate (see Stambaugh 
hypothesis of the term structure implies that both b, and );, should equal one. The underlying [1988]). Since the long rate appears both in the regressor with a positive 
data are monthly zero-coupon bond yields over the period 1952:l to 1991:2,f rom McCulloch sign and in the dependent variable with a negative sign, measurement error 
and Kwon (1993). would tend to produce the negative signs found in Table 10.3. Campbell 

and Shiller (1991) point out that this can be handled by using instrumen- 
tal variables regression where the instruments are correlated with the yield 

high, short rates are expected to rise so that the average short rate over the spread but not with the bond yield measurement error. They try a variety 
life of the long bond equals the initial long-bond yield. Near-term increases of instruments and find that the negative regression coefficients are quite 
in short rates are given greater weight than further-off increases, because robust. 
they affect the level of short rates during a greater part of the life of the long 
bond. Yield Spreads and Future Short Rates 

There is much more truth in proposition (10.2.15),t hat high yield spreads 
Yield Spreads and Future Long Rates should forecast long-term increases in short rates. This can be tested either 
Equation (10.2.14),w hich says that high yield spreads should forecast in- directly or indirectly. The direct approach is to form the expost value of the 
creases in long rates, fares poorly in the data. Macaulay (1938) first noted short-rate changes that appear on the right-hand side of (10.2.15) and to 
the fact that high yield spreads actually tend to precede decreases in long regress this on the yield spread. We define 
rates. He wrote: "The yields of bonds of the highest grade should fall during 
a period in which short-term rates are higher than the yields of the bonds 
and rise during a period in which short-term rates are lower. Now experience 
is more nearly the opposite" (Macaulay [1938, p. 331 ). 

Table 10.3 reports estimates of the coefficient Bn and its standard error 
in the regression 

that this is not the same as approximating p,-l , l+l  by p , , l+ l .  The numbers given differ slightly 
from those in Campbell (1995) because that paper uses the sample period 1951:l to 1990:2, 
erroneously reported as 1952:l to 1991:2. 

25~ampbealln d Arnmer (1993),F ama and French (l989),a nd Keim and Stambaugh (1986) 
The maturity n varies from 3 months to 120 months (10 .24 According show that yield spreads help to forecast excess returns on bonds as well as on other long-term 

assets. Campbell and Shiller (1991) and S h i k ,C ampbell, and Schoenholtz (1983) show that 
yield spreads tend to forecast declines in long-bond yields. 

' 4 ~ o rm aturities above one year the table uses the approximation yn-l , l+l  X yn,l+l .  Note 



422 10. Fixed-Income Securities 10.3. Conclusion 
423 

and run the regression To interpret Table 10.3, it is helpful to return to equation (10.2.13)a nd 
rewrite it as 

The expectations hypothesis implies that y, = 1 for all n.26 
Table 10.3 reports estimated qn coefficients with standard errors, cor- 

recting for heteroskedasticity and overlap in the equation errors in the man- 
ner discussed in the Appendix. The estimated coefficients have a U shape: 
For small n they are smaller than one but significantly positive; up to a and 
year or so they decline with n, becoming insignificantly different from zero; 
beyond one year the coefficients increase and at ten years the coefficient 
is even significantly greater than one. Thus Table 10.3 shows that yield 
spreads have forecasting power for short-rate movements over a horizon of In general the yield spread is the sum of two components, one that forecasts 
two or three months, and again over horizons of several years. Around one interest rate changes (synt)a nd one that forecasts excess returns on long 
year, however, yield-spread variation seems almost unrelated to subsequent bonds (sr,,). This means that the regression coefficient yn in equation 
movements in short rates. (10.2.18)i s 

The regression equation (10.2.18) contains the same information as a 
regression of (1/  n) times the excess n-period return on an n-period bond 
onto the yield spread snt. The relation between excess returns and yields 
implies that the excess-return regression would have a coefficient of ( 1  - y,). 
Table 10.3 implies that yield spreads forecast excess returns out to horizons 
of several years, but the forecasting power diminishes towards ten years. 

There are several econometric difficulties with the direct approach just For any given variance of excess-return forecasts sr,,, as the variance 
described. First, one loses n periods of data at the end of the sample period. of interest rate forecasts sy,, goes to zero the coefficient yn goes to zero, 
This can be quite serious: For example, the ten-year regression in Table but as the variance of sy,, increases the coefficient y, goes to one. The U- 
10.3 ends in 1981, whereas the three-month regression ends in 1991. This shaped pattern of regression coefficients in Table 10.3 may be explained by 
makes a substantial difference to the results, as discussed by Campbell and reduced forecastability of interest rate movements at horizons around one 
Shiller (1991).  Second, the error term cnt is a moving average of order year. There may be some short-run forecastability arising from Federal Re- 
(n - I ) ,  so standard errors must be corrected in the manner described in " ~ ~ 

serve operating procedures, and some long-run forecastability arising from 
the Appendix. This can lead to finite-sample problems when ( n-  1 )  is not business-cycle effects on interest rates, but at a one-year horizon the Federal 
small relative to the sample size. Third, the regressor is serially correlated Reserve may smooth interest rates so that the variability of synti s small. Bal- 
and correlated with lags of the dependent variable, and this too can cause duai, Bertola, and Foresi (ISCIS),  Rudebusch ( l 9 9 5 ) ,a nd Roberds, Runkle, 
finite-sample problems (see Mankiw and Shapiro [1986] ,R ichardson and and Whiteman (1996) argue for this interpretation of the evidence. Con- 
Stock [1990],a nd Stambaugh [1986] ) .  skent with this explanation, Mankiw and Miron (1986) show that the pre- 

Although these econometric problems are important, they do not seem dictions of the expectations hypothesis fit the data better in periods when 
to account for the U-shaped pattern of coefficients. Campbell and Shiller iterest rate movements have been highly forecastable, such as the period 
(1991)f ind similar results using a vector autoregressive (VAR) methodologY "mediately before the founding of the Federal Reserve System. 
like that described in Section 7.2.3 of Chapter 7 .  They find that the long- 
term yield spread is highly correlated with an unrestricted VAR forecast of 
future short-rate movements, while the intermediate-term yield spread is 10.3 Conclusion 
much more weakly correlated with the VAR forecast. 

esults in Table 10.3 imply that naive investors, whojudge bonds by their 
"~ama( 1984) and Shiller, Campbell, and Schoenholtz (1983) use this approach at the short s to maturity and buy long bonds when their yields are relatively high, 

end of the term structure, while Fama and Bliss (1987) extend it to the long end. campbell e tended to earn superior returns in the postwar period in the United 
and Shiller (1991) provide a comprehensive review. 

, .-- --.-- .. . 



424 10. Fixed-Income Securities 

States. This finding is reminiscent of the finding discussed in Chapter 7, that where xt is a term premium that represents the deviation of the two-period 
stock returns tend to be higher when dividend yields are high at the start of yield from the log pure expectations hypothesis, equation (10.2.10). The 
the holding period. As in the stock market case, it is not clear whether this variable xt follows an AR(1) process 
result reflects a failure of rationality on the part of investors or the presence 
of time-varying risk premia. Froot (1989) has used survey data to argue that 
bond market investors have irrational expectations, but there is also much The error terms E, and v t  are serially uncorrelated and uncorrelated with 
theoretical work, discussed in the next chapter, that explores the impact of each other. 
time-varying risk on the term structure of interest rates. 

This chapter has concentrated on the forecasting power of yield spreads 10.2.1 Show that this model can be solved for an interest-rate process of 
for future movements in nominal interest rates. Yield spreads are also useful the form 
in forecasting other variables. For example, one can decompose nominal ylt = yi,t-1 + Yxt + Et.  

rates into inflation rates and real interest rates; the evidence is that most of Express the coefficient y as a function of the other parameters in the 
the long-run forecasting power of the term structure is for inflation rather model. 
than real interest rates (see Fama [1975,1990] and Mishkin [1990a, 1990bl). 
We mentioned in Chapter 8 that the slope of the term structure has some 10.2.2 The expectations hypothesis of the term structure is often tested 
ability to forecast excess returns on stocks as well as bonds. Other recent in the manner of equation (10.2.17) by regressing the scaled change in 
studies by Chen (1991b) and Estrella and Hardouvelis (1991) have shown the short rate onto the yield spread, 
that the term structure forecasts real economic activity, since inverted yield 
curves tend to precede recessions and steeply upward-sloping yield curves 
tend to precede expansions. and testing the hypothesis that the coefficient B = 1. If the model 

described above holds, what is the population value of the regression 
coefficient B? 
10.2.3 Now consider a version of the problem involving n-period bonds. 

10.1 You are told that an &year nominal zerocoupon bond has a log yield The monetary authority sets short-term interest rates as 
to maturity of 9.1 %, and a 9-year nominal zerocoupon bond has a log yield 
of 8.0%. 

10.1.1 Can the pure expectations theory of the term structure describe and the n-period bond yield is determined by 
these data? 

10.1.2 A year goes by, and the bonds in part (a) still have the same 
yields to maturity. Can the pure expectations theory of the term structure where xt now measures the deviation of the n-period yield from the log 
describe these new data? pure expectations hypothesis (10.2.14). (This formulation ignores the 

distinction between ynt and Y,-~,~A.)s before, xt follows an AR(1) process. 
10.1.3 How would your answers change if you were told that the bonds What is the coefficient y in this case? What is the regression coefficient 
have an 8% coupon rate per year, rather than zero coupons? B in a regression of the form (10.2.16), 

10.2 Suppose that the monetary authority controls short-term interest 
rates by setting 

ylt  = y l ~ - 1+  Upt - y d +  r t ,  10.2.4 Do you find the model you have studied in this problem to be a 
with h > 0. Intuitively, the monetary authority tries to smooth interest plausible explanation of empirical findings on the term structure? Why 
rates but raises them when the yield curve is steep. Suppose also that the or why not? 
two-period bond yield satisfies Note: This problem is based on McCallum (1994). 

mt = (ylt + Et[yl,t+ll)/2+  x,, 



Term-Structure Models 

THIS CHAPTER EXPLORES the large modern literature on fully specified gen- 
eralequilibrium models of the term structure of interest rates. Much of 
this literature is set in continuous time, which simplifies some of the thee  
retical analysis but complicates empirical implementation. Since we focus 
on the econometric testing of the models and their empirical implications, 
we adopt a discrete-time approach; however we take care to relate all our 
results to their continuous-time equivalents. We follow the literature by first 
developing models for real bonds, but we discuss in some detail how these 
models can be used to price nominal bonds. 

All the models in this chapter start from the general asset pricing con- 
dition introduced as (8.1.3) in Chapter 8: 1 = Et[ ( l  + &,t+l)Mt+l],w here 
&,t+l is the real return on some asset i and Mt+l is the stochastic discount fac- 
tor. As we explained in Section 8.1 of Chapter 8 ,  this condition implies that 
the expected return on any asset is negatively related to its covariance with 
the stochastic discount factor. In models with utility-maximizing investors, 
the stochastic discount factor measures the marginal utility of investors. As- 
sets whose returns covary positively with the stochastic discount factor tend 
to pay off when marginal utility is high-they deliver wealth at times when 
wealth is most valuable to investors. Investors are willing to pay high prices 
and accept low returns on such assets. 

Fixed-income securities are particularly easy to price using this frame- 
work. When cash flows are random, the stochastic properties of the cash 
flows help to determine the covariance of an asset's return with the stochas- 
tic discount factor. But a fixed-income security has deterministic cash flows, 
so it covaries with the stochastic discount factor only because there is time- 
variation in discount rates. This variation in discount rates is driven by the 
time-series behavior of the stochastic discount factor, so term-structure mod- 
els are equivalent to time-series models for the stochastic discount factor. 

From (10.1.4) in Chapter 10, we know that returns on n-period real 
zero-coupon bonds are related to real bond prices in a particularly simple 



428 I I .  Term-Structure Models 11 .  I .  AfJine-Yield Models 

way: (1 + = P,-l,t+l/P,t. Substituting this into (8.1.3), we find that discrete-time versions of the well-known models of Vasicek (1977) and Cox, 
the real price of an n-period real bond, Pnt,s atisfies Ingersoll, and Ross (l985a), respectively. Section 11 .1.3 then considers a 

more general model with two state variables, a discrete-time version of the 
model of Longstaff and Schwartz (1992). All of these models have the 
property that log bond prices, and hence log bond yields, are linear or 

This equation lends itself to a recursive approach. We model Pnta s a function afJine in the state variables. This ensures the desired joint lognormality of 
of those state variables that are relevant for forecasting the Mt+l process. bond prices with the stochastic discount factor. Section 11.1.4 describes the 
Given that process and the function relating Pn-l*tlo  state variables, we can general properties of these affineyield models, and discusses some alternative 
calculate the function relating Pntt o state variables. We start the calculation modelling approaches.' 
by noting that Pot=  1. 

Equation (11.0.1) can also be solved forward to express the n-period 
bond price as the expected product of n stochastic discount factors: 1 I .1 . 1  A Homoskedastic SingleFactor Model 

It is convenient to work with the negative of the log stochastic discount 
factor, -mt+l. Without loss of generality, this can be expressed as the sum 

Although we emphasize the recursive approach, in some models it is more of its one-period-ahead conditional expectation xt and an innovation ct+l: 
convenient to work directly with (11 .0.2). 

Section 11.1 explores a class of simple models in which all relevant 
variables are conditionally lognormal and log bond yields are linear in state We assume that E I + ~i s normally distributed with constant variance. 
variables. These afJineyield models include all the most commonly used term- Next we assume that xt+l follows the simplest interesting time-series 
structure models. Section 11.2 shows how these models can be fit to nominal process, a univariate AR(1) process with mean and persistence 4. The 
interest rate data, and reviews their strengths and weaknesses. One of the shock to xt+l is written tt+l: 
main uses of term-structure models is in pricing interest-rate derivative se- 
curities; we discuss this application in Section 11.3. We show how standard 
term-structure models can be modified so that they fit the current term 
structure exactly. We then use the models to price forwards, futures, and The innovations to mt+l and xt+l may be correlated. To capture this, we 
options on fixed-income securities. write ~ ~as+ l  

ct+l = Kt+l + rlt+l? (11.1.4) 

where et+l and qt+l are normally distributed with constant variances and are 
1 1.1 Affine-Yield Models uncorrelated with each other. 

The presence of the uncorrelated shock ql+l only affects the average 
To keep matters simple, we assume throughout this section that the distri- level of the term structure and not its average slope or its time-series behavior. 
bution of the stochastic discount factor Mt+l is conditionally lognormal. We To simplify notation, we accordingly drop it and assume that ct+l = Bet+i. 
specify models in which bond prices are jointly lognormal with Mt+l. We Equation (11 .1.2) can then be rewritten as 
can then take logs of (11 .0.1) to obtain 

The innovation et+l is now the only shock in the system; accordingly we can 
where as usual lowercase letters denote the logs of the corresponding up- write its variance simply as 0*w ithout causing confusion. 
percase letters so for example mt+l = 10g(M~+~T)h.i s is the basic equation Equations (11  .l.5) and (11 .1.3) imply that -mt+l can be written as an 
we shall use. ARMA(1,l) process since it is the sum of an M ( 1 )  process and white noise. 

We begin with two models in which a single state variable forecasts the 
stochastic discount factor. Section 11.1.1 discusses the first model, in which 'our  discrete-time presentation follows Singleton (1990), Sun (1992), and especially 
mt+l is homoskedastic, while Section 11.1.2 discusses the second model, Backus (1993). Sun (1992) explores the relation between discretetime and continuous-time 
in which the conditional variance of m,+l changes over time. These are models in more detail. 



430 11.  Tm-Structure Models 11.1. Affine-Yield Models 43 1 

In fact, -mt+l has the same structure as asset returns did in the example of This must hold for any xt, so the coefficients on xt must sum to zero and the 
Chapter 7 , S ection 7.1.4. As in that example, it is important to realize that remaining coefficients must also sum to zero. This implies 
-mt+l is not a univariate process even though its conditional expectation xt 
is univariate. Thus the univariate autocorrelations of -mt+l do not tell us 
all we need to know for asset pricing; different sets of parameter values, with 
different implications for asset pricing, could be consistent with the same 
set of univariate autocorrelations for -mt+l. For example, these autocorre- 
lations could all be zero because a 2  = 0, which would make interest rates We have now verified the guess ( 1 1 .1.8), since with the coefficients in 
constant, but they could also be zero for a2 # 0 if p takes on a particular ( 1 1 . 1 . 1 1  ) the price function ( 1 1 .1.8) satisfies the asset pricing equation 
value, and in this case interest rates would vary over time. ( 1 1 .1 .1  ) and its assumption that bond returns are conditionally lognormal. 

We can determine the price of a one-period bond by noting that when 
n = 1 ,  p,-1, t+l  = &,t+l = 0 ,  SO the terms involving p,- l , t+l  in equation Implications of the Homoskedastic Model 
(11.1.1)d rop out. Substituting (11.1.5)a nd (11.1.3)i nto ( l l . l . l ) ,w e have The homoskedastic bond pricing model has several interesting implications. 

First, the coefficient Bn measures the fall in the log price of an n-period 
bond when there is an increase in the state variable xt or equivalently in 

The one-period bond yield the one-period interest rate ylt. It therefore measures the sensitivity of the 
y l t  = - p i t ,  SO 

n-period bond return to the one-period interest rate. Equation ( 1 1 . 1 . 1 1 )  
shows that the coefficient B, follows a simple univariate linear difference 
equation in n, with solution ( 1  -$") / ( I  - 4) .A s n increases, Bn approaches 

The short rate equals the state variable less a constant term, so it inherits a limit B = 1 / ( 1  - 4 ) .T hus bond prices fall when short rates rise, and the 
the AR(1) dynamics of the state variable. Indeed, we can think of the short sensitivity of bond returns to short rates increases with maturity. 
rate as measuring the state of the economy in this model. Note that there Note that Bn is different from duration, defined in Section 10.1.2 of 
is nothing in equation (11.1.7)t hat rules out a negative short rate. Chapter 10. Duration measures the sensitivity of the n-period bond return 

We now guess that the form of the price function for an n-period bond to the n-period bond yield, and for zero-coupon bonds duration equals 
is maturity. Bn measures the sensitivity of the n-period bond return to the 

- pnt = An + Bn xt. (11.1.8) one-period interest rate; it is always less than maturity because the n-period 
Since the n-period bond yield ynt = -pnt/  n, we are guessing that the yield on bond yield moves less than one-for-one with the one-period interest rate. 
a bond of any maturity is linear or afJine in the state variable xt (Brown and A second implication of the model is that the expected log excess return 
Schaefer [1991] ) .W e already know that bond prices for n = 0 and n = 1 on an n-period bond over a one-period bond, Et [rSst+I l- y l t  = Et [ p n - l , t + l  1-  
satisfy equation (11.1.8),w ith A. = Bo = 0 ,  A1 = - p 2 a 2 / 2 ,  and B1 = 1 .  We Pnt + p l t ,  is given by 
proceed to verify our guess by showing that it is consistent with the pricing 
relation ( 1 1 . 1 . 1  ) . At the same time we can derive recursive formulas for the 
coefficients A, and B,. 

Our guess for the price function ( 11 .1.8) implies that the two terms on 
= - ~ ~ - 1 p-a ~~:  - ~ a ~ / 2 .  (11.1.12) 

the right-hand side of ( 1 1 . 1 . 1 )  are 
The first equality in ( 1 1 .1.12) is a general result, discussed in Chapter 8 ,  
that holds for the excess log return on any asset over the riskfree interest 
rate. It can be obtained by taking logs of the fundamental relation 1 = 
Et[ ( l  + R , t + l ) M t + lf]o r the n-period bond and the short interest rate, and 

Substituting (11.1.8)a nd (11.1.9)i nto ( l l . l . l ) ,w e get then taking the difference between the two equations. It says that the ex- 
pected excess log return is the sum of a risk premium term and a Jensen's 
Inequality term in the own variance which appears because we are working 
in logs. 



432 I I .  Term-Structure Models 11.1. Af$ne-Yield Models 433 

The second equality in ( 11 .1.12) uses the fact that the unexpected com- As maturity n increases, the forward rate approaches 
ponent of the log return on an n-period bond is just -Bn-, times the in- 
novation in the state variable. The third equality in (11.1.12) uses the fact 
that the conditional variance of xt+1 and its conditional covariance with mt+l a constant that does not depend on the current value of the state variable xt. 
are constants to show that the expected log excess return on any bond is Equation (11.1.7) implies that the average short rate is a - B2a2 /2 .  Thus 
constant over time, so that the log expectations hypothesis-but not the log the difference between the limiting forward rate and the average short rate 
pure expectations hypothesis-holds. is 

-Bn-l is the coefficient from a regression of n-period log bond returns -(MI - 4 ) ) 2 f f 2 / 2- - #4)ff2.  
on state variable innovations, so we can interpret -BnP1 as the bond's load- This is the same as the limiting expected log excess return on a long-term 
ing on the single source of risk and pa2 as the reward for bearing a unit of bond. Because of the Jensen's Inequality effect, the log forward-rate curve 
risk. Alternatively, following Vasicek (1977) and others, we might calculate tends to slope downwards towards its limit unless /? is sufficiently negative, 
the price of risk as the ratio of the expected excess log return on a bond, plus p < -1 /2(1  - 4 ) .  
one half its own variance to adjust for Jensen's Inequality, to the standard As xt varies, the forward-rate curve may take on different shapes. The 
deviation of the excess log return on the bond. Defined this way, the price second equality in ( 11 .1.14)s hows that the forward-rate curve can be written 
of risk is just -pa in this model. as the sum of a component that does not vary with n, a component that dies 

The homoskedastic bond pricing model also has implications for the out with n at rate 4 ,  and a component that dies out with n at rate @ .  The 
pattern of forward rates, and hence for the shape of the yield curve. To third component has a constant coefficient with a negative sign; thus there 
derive these implications, we note that in any termstructure model the n- is always a steeply rising component of the forward-rate curve. The second 
period-ahead forward rate fnt satisfies component has a coefficient that varies with xt, so this component may 

be slowly rising, slowly falling, or flat. Hence the forward-rate curve may be 
rising throughout, falling throughout (inverted),o r may be rising at first and 
then falling (humpshaped) if the third component initially dominates and 
then is dominated by the second component further out along the curve. 
These are the most common shapes for nominal forward-rate curves. Thus, 

In this model Et [pn,t+ll-  pnt = Bn Et  [Axt+l l ,a nd Et [r,+l,t+ll - ylt is given if one is willing to apply the model to nominal interest rates, disregarding 
by (11.1.12).  Substituting into (11.1.13) and using B, = (1  - @")/(I-  $) ,  the fact that it allows interest rates to go negative, one can fit most observed 
we get nominal term structures. However the model cannot generate a forward- 

rate curve which is falling at first and then rising (inverted hump-shaped), as 
occasionally seen in the data. 

It is worth noting that when 4 = 1, the one-period interest rate follows 
a random walk. In this case the coefficients A, and B, never converge as n 
increases. We have B, = n and A, - AnP1 = -(B + n - 1)*a2 /2 .  The 
forward rate becomes fnt = xt - ( /?  + n12a2/2,w hich may increase with 
maturity at first if /? is negative but eventually decreases with maturity forever. 
Thus the homoskedastic bond pricing model does not allow the limiting 
forward rate to be both finite and time-varying; either 4 < 1 ,  in which case 
the limiting forward rate is constant over time, or 4 = 1, in which case 

The first equality in ( 11 .1.14) shows that the change in the n-period forward there is no finite limiting forward rate. This restriction may seem rather 

rate is 4 ,  times the change in xt. Thus movements in the forward rate die counterintuitive; in fact it follows from the very general result--derived 

out geometrically at rate 4 .  This can be understood by noting that the by Dybvig, Ingersoll, and Koss (1996)-that the limiting forward rate, if it 
exists, can never fall. In the homoskedastic model with 4 < 1 the limiting 

log expectations hypothesis holds in this model, so forward-rate movements 
forward rate never falls because it is constant; in the homoskedastic model 

reflect movements in the expected future short rate which are given by +n 
with @ = 1 the limiting forward rate does not exist. 

times movements in the current short rate. 



434 I I .  Term-Structure Models 11.1.  Affine-Yield Models 435 

The discrete-time model developed in this section is closely related to B is negative, a positive shock to consumption lowers interest rates so bonds 

the continuous-time model of Vasicek (1977). Vasicek specifies a continu- have positive risk premia. 

ous-time AR(1) or Ornstein-Uhlenbeck process for the short interest rate r, Campbell (1986) explores the relation between bond risk premia and 

given by the following stochastic differential equation: the timeseries properties of consumption in a related model. Campbell's 
model is similar to the one here in that consumption and asset returns are 
conditionally lognormal and homoskedastic. It is more restrictive than the 
model here because it makes consumption growth (rather than expected 

where K ,  6, and a are constants.* Also, Vasicek assumes that the price of consumption growth) a univariate stochastic process, but it is more general 
interest rate risk-the ratio of the expected excess return on a bond to the in that it does not require expected consumption growth to follow an AR(1) 
standard deviation of the excess return on the bond-is a constant that does process. Campbell shows that the sign of the risk premium for an n-period 
not depend on the level of the short interest rate. The model of this section bond depends on whether a consumption innovation raises or lowers con- 
derives an AR(1) process for the short rate and a constant price of risk from sumption growth expected over (n - 1) periods. Backus and Zin (1994) 
primitive assumptions on the stochastic discount factor. explore this model in greater detail. Backus, Gregory, and Zin (1989) also 

relate bond risk premia to the time-series properties of consumption growth 
Equilibrium Interpretation of the Model and interest rates. 
Our analysis has shown that the sign of the coefficient B determines the Cox, Ingersoll, and Ross (1985a) show how to derive a continuous- 
sign of all bond risk premia. To understand this, consider the effects of time term-structure model like the one in this section from an underlying 
a positive shock t t+1 which increases the state variable xt+l and lowers all production model. Sun (1992) and Backus (1993) restate their results in 
bond prices. When /l is positive the shock also drives down mt+l, so bond discrete time. Assume that there is a representative agent with discount 
returns are positively correlated with the stochastic discount factor. This factor 6 and time-separable log utility. Suppose that the agent faces a budget 
correlation has hedge value, so risk premia on bonds are negative. When B constraint of the form 
is negative, on the other hand, bond returns are negatively correlated with 
the stochastic discount factor, and risk premia are positive. 

We can get more intuition by considering the case where the stochastic 
discount factor reflects the power utility function of a representative agent, where Kt is capital at the start of the period, (Kt - Ct) is invested capital, 

as in Chapter 8. In this case Mt+1 = C~(C,C+t~)-/Y , where 6 is the discount and Xt &+l is the return on capital. This budget constraint has constant 

factor and y is the risk-aversion coefficient of the representative agent. Tak- returns to scale because the return on capital does not depend on the level 

ing logs, we have of capital. Xt is the anticipated component of the return and %+l is an 

mt+l = log(6) - Y Act+l. (11.1.16) unanticipated technology shock. With log utility it is well-known that the 
agent chooses Ct/Kt = (1 - 6). Substituting this into (11 .1.17) and taking 

It follows that x, = Et [-mt+l] = - log(&)+  yE, [Act+l], and €,+I logs we find that 
-mt+l - Et[-mt+l] = ~ ( A C-~ E+t[~Ac t+l]). xt is a linear function of k + l  = log@)+  xt + vt+l, (11.1.18) 
expected consumption growth, and rt+l is proportional to the innovation 
in consumption growth. The term-structure model of this section then where vt+l = log(K+l), and -mt+l = -log(&) + Act+l = xt + €(+I. This 

derivation allows xt to follow any process, including the AR(1) assumed by 
implies that expected consumption growth is an AR(1) process, so that 

the term-structure model. 
realized consumption growth is an ARMA(1,l) process. The coefficient 
/? governs the covariance between consumption innovations and revisions 
in expected future consumption growth. If B is positive, then a positive 11.1.2 A Square-Root Single-Factor Model 
consumption shock today drives up expected future consumption growth The homoskedastic model of the previous section is appealing because of 
and increases interest rates; the resulting fall in bond prices makes bonds its simplicity, but it has several unattractive features. First, it assumes that 
covary negatively with consumption and gives them negative risk premia. If interest rate changes have constant variance. Second, the model allows in- 

terest rates to go negative. This makes it applicable to real interest rates, but 
'As in Chapter 9, dB in (1 1.1.15) denotes the increment to a Brownian motion; it should less appropriate for nominal interest rates. Third, it implies that risk premia 

not be confused with the bond price coefficients B, of this section. 



436 11. Term-Structure Mo&ls 11.1. AfJine-Yield Models 437 

are constant over time, contrary to the evidence presented in Section 10.2.1 show that negative interest rates are ruled out in the continuous-time version 
of Chapter 10. One can alter the model to handle these problems, while of this model, where the instantaneous interest rate follows the process 
retaining much of the simplicity of the basic structure, by allowing the state dr  = K ( @-  r)d t+  a r l / ' d ~ T. ~im e-variation in volatility also produces time- 
variable xt to follow a conditionally lognormal but heteroskedastic square-root variation in term premia, so that the log expectations hypothesis no longer 
process. This change is entirely consistent with the equilibrium foundations holds in this model. 
for the model given in the previous section. We now guess that the price function for an n-period bond has the same 

The square-root model, which is a discrete-time version of the famous linear form as before, -pnt = A, + B, xt, equation (1 1 .1  3).I n this model 
Cox, Ingersoll, and Ross (1985a) continuous-time model, replaces (11 .1.5) A. = & = 0, Al = 0, and B1 = 1 - /12a2/2. It is straightforward to 
and (11.1.3)w ith verify the guess and to show that A, and B, obey 

The new element here is that the shock ct+i is multiplied by x,'I2. To 
understand the importance of this, recall that in the homoskedastic model Comparing (11 .1.23) with (11 .1.1 l ) ,w e see that the term in a2h as been 
xt+i and mt+i are normal conditional on xt for all i > 1. This means that moved from the equation describing A, to the equation describing B,. This 
one can analyze the homoskedastic model either by taking logs of (11 .0.1) is because the variance is now proportional to the state variable, so it affects 
to get the recursive equation (1 1.1.1),o r by taking logs of (11.0.2) to get an the slope coefficient rather than the intercept coefficient for the bond price. 
n-period loglinear equation: The limiting value of B,, which we write as B, is now the solution to a 

quadratic equation, but for realistic parameter values this solution is close 
to the limit 1/(1 - 6) from the previous model. Thus B, is positive and 

Calculations based on (11 .1.21) are more cumbersome than the analysis increasing in n. 
presented in the previous section, but they give the same result. In the The expected excess log bond return in the square-root model is given 
square-root model, by contrast, xt+l and mt+l are normal conditional on by 
xt but xt+i and mt+i are nonnormal conditional on xt for all i > 1. This 
means that one can only analyze the square-root model using the recursive 
equation (11 . l.1 ); the n-period loglinear relation (11 .1.21) does not hold 
in the square-root model. 

Proceeding with the recursive analysis as before, we can determine the 
price of a one-period bond by substituting (11 .1.19) into (11 .1 . l )  to get 

The first two equalities here are the same as in the previous model. The 
third equality is the formula from the previous model, (1 1.1.12),m ultiplied 
by the state variable xt. Thus the expected log excess return is proportional 

The one-period bond yield ylt  = -pit is now proportional to the state to the state variable xt or, equivalently, to the short interest rate ylt. This is 
variable xt. Once again the short rate measures the state of the economy in the expected result since the conditional variance of interest rates is p r e  
the model. portional to xt. Once again the sign of /l determines the sign of the risk 

Since the short rate is proportional to the state variable, it inherits the premium term in (11 .1.24). Since the standard deviation of excess bond 
property that its conditional variance is proportional to its level. Many returns is proportional to the square root of xt, the price of interest rate 
authors have noted that interest rate volatility tends to be higher when risk-the ratio of the expected excess log return on a bond, plus one half its 
interest rates are high; in Section 11.2.2 we discuss the empirical evidence own variance to adjust for Jensen's Inequality, to the standard deviation of 
on this point. This property also makes it hard for the interest rate to go 
negative, since the upward drift in the state variable tends to dominate the 3~ependinogn  the parametervalues, it may be possible for the interest rate to be zero in the 

continuous-time model. Longstaff (1992) discusses alternative ways to model this possibility. 
random shocks as x, declines towards zero. Cox, Ingersoll, and Ross (19854 



438 11. Term-Stmcture Models 11.1.  AfJine-Yield Models 439 

the excess log return on the bond-is also proportional to the square root Finally, the relation between the shocks is 
of xt. 

The forward rate in the square-root model is given by 

and the shocks $l , t+l  and t2,t+arle  uncorrelated with each other. We will 
write a: for the variance of tl,t+aln d 4f or the variance of t2,t+1. 

In this model, minus the log stochastic discount factor is forecast by two 
state variables, xlt and xzt. The variance of the innovation to the log stochas- 
tic discount factor is proportional to the level of X I , ,  as in the square-root 

The first equality in (11 .1.25) is the same as in the homoskedastic model, model; and each of the two state variables follows a square-root autore- 
while the second equality multiplies variance terms by xt where appropri- gressive process. Finally, the log stochastic discount factor is conditionally 
ate. It can be shown that the square-root model permits the same range of correlated with xl but not with x2. This last assumption is required to keep 
shapes for the yield curve-upward-sloping, inverted, and humped-as the the model in the tractable affine-yield class. Note that the two-factor model 
homoskedastic model. nests the single-factor square-root model, which can be obtained by setting 

Pearson and Sun (1994) have shown that the square-root model can be xzt = 0, but does not nest the single-factor homoskedastic model. 
generalized to allow the variance of the state variable to be linear in the level Proceeding in the usual way, we find that the price of a one-period bond 
of the state variable, rather than proportional to it. One simply replaces the is 
xfI2 terms, multiplying the shocks in (11.1.19) and (11 .1.20) with terms of 
the form (ao+  a1x t)Il2. The resulting model is tractable because it remains 
in the affine-yield class, and it nests both the homoskedastic model (the 
case ao = 1, al = 0) and the basic square-root model (the case a. = 0, 

The one-period bond yield yl, = -p l ,  is no longer proportional to the 
a1 = 1). state variable X I , ,  because it depends also on ~ 2 T~he.  s hort interest rate 

is no longer sufficient to measure the state of the economy in this model. 
11.1.3 A Two-Factor Model Longstaff and Schwartz (1992) point out, however, that the conditional 

variance of the short rate is a different linear function of the two state 
So far we have only considered single-factor models. Such models imply 

variables: 
that all bond returns are perfectly correlated. While bond returns do tend 
to be highly correlated, their correlations are certainly not one and so it is 
natural to ask how this implication can be avoided. 

We now present a simple model in which there are two factors rather 
than one, so that bond returns are no longer perfectly ~orre la ted.T~h e Thus the short rate and its conditional volatility summarize the state of the 
model is a discrete-time version of the model of Longstaff and Schwartz economy, and one can always state the model in terms of these two variables. 
(1992). It replaces (11.1.19) with We guess that the price function for an n-period bond is linear in the 

two state variables: -pnt = A, + Bl ,  xl, + &, x2,. We already know that 
fb = Blo = &O = 0, Al = 0, Bll  = 1 - 0:/2, and = 1. It is 
straightforward to show that A,, B1,, and &, obey 

and replaces (11 .1.20) with a pair of equations for the state variables: 

*.41though bond returns are not perfectly correlated in this model, the covariance matrix 
of bond returns has rank two and hence is singular whenever we observe more than two bonds. 
We discuss this point further in Section 11.1.4. 



440 I I .  Term-Structure Models 11.1. AfJine-Yield Models 44 1 

The difference equation for Bin is the same as in the single-factor square- square-root term structure driven by the xlt process and a special square-root 
root model, (11 .1.23), but the difference equation for B2, includes only a term structure with parameter restriction = 0 driven by the x2, process. 
term in the own variance of x2 because x:, is uncorrelated with m and does 
not affect the variance of m. The difference equation for A, is just the sum 
of two terms, each of which has the familiar form from the single-factor 1 1.1.4 Beyond Afjne-Yield Models 
square-root model. We have considered a sequence of models, each of which turns out to have 

The expected excess log bond return in the two-factor model is given the property that log bond yields are linear or afJine in the underlying state 
by variables. Brown and Schaefer (1991) and Duffie and Kan (1993) have 

clarified the primitive assumptions necessary to get an affine-yield model. 
In the discrete-time framework used here, these conditions are most easily 
stated by defining a vector xt which contains the log stochastic discount 
factor mt and the time t values of the state variables relevant for forecasting 
future mt+i, i = 1 . . . n. If the conditional forecast of x  one period ahead, 
El [xt+lI ,  is affine in the state variables, and if the conditional distribution of 
x  one period ahead is normal with a variance-covariance matrix Var, [ x , + ~ ]  
which is affine in the state variables, then the resulting term-structure model 
is an affine-yield model. 

This is the same as in the square-root model, with the addition of an extra To see this, consider the steps we used to derive the implications of 
term, arising from Jensen's Inequality, in the variance of xzXt+l. each successive term-structure model. We first calculated the log short- 

The forward rate in the two-factor model is given by term interest rate; this is affine in the underlying state variables if mt+l is 
conditionally normal and Et[mt+ l ]a nd Vart[mt+l]a re affine in the state 
variables. We next guessed that log bond yields were affine and proceeded 
to verify the guess. If yields are affine, and if x  is conditionally normal with 
affine variance-covariance matrix, then the risk premium on any bond is 
affine. Finally we derived log forward rates; these are affine if the short rate, 
risk premium, and the expected change in the state variable are all affine. 
Affine forward rates imply affine yields, verifying that the model is in the 
affine-yield class. 

Brown and Schaefer (1991) and Duffie and Kan (1993) state conditions 
This is the obvious generalization of the square-root model. Importantly, on the short rate which deliver an affine-yield model in a continuous-time 
it can generate more complicated shapes for the yield curve, including setting. They show that the risk-adjusted drift in the short rate-the ex- 
inverted hump shapes, as the independent movements of both xlt and ~2~ pected change in the short rate less the covariance of the short rate with 
affect the term structure. the stochastic discount factor-and the variance of the short rate must both 

The analysis of this model illustrates an important principle. As Cox, be affine to get an affine-yield model. The models of Vasicek (1977), Cox, 
Ingersoll, and Ross (1985a) and Dybvig (1989) have emphasized, under Ingersoll, and Ross (1985a), and Pearson and Sun (1994) satisfy these re- 
certain circumstances one can construct multifactor term-structure models quirements, but some other continuous-time models such as that of Brennan 
simply by "adding up" single-factor models. Whenever the stochastic dis- and Schwartz (1979) do not. 
count factor mt+l can be written as the sum of two independent processes, Affine-yield models have a number of desirable properties which help to 
then the resulting term structure is the sum of the term structures that would explain their appeal. First, log bond yields inherit the conditional normality 
exist under each of these processes. In the Longstaff and Schwartz (1992) assumed for the underlying state variables. Second, because log bond yields 

are linear functions of the state variables we can renormalize the model 
model the stochastic discount factor is the sum of -xlt - X ; ( ~ ~ ( Ia,n~d + I  

so that the yields themselves are the state variables. This is obvious in a 
-qt,a nd these components are independent of each other. Inspection of onefactor model where the short rate is the state variable, but it is equally 
(11.1.34) shows that the resulting term structure isjust the sum of a general 



442 I I. Tm-Structure Models 11.2. Fitting Term-Structure Models to the Data 443 

possible in a model with any number of factors. Longstaff and Schwartz riskless in nominal terms.5 We now discuss how the models can be adapted 
(1992) present their two-factor model as one in which the volatility of the to deal with this fact. 
short rate and the level of the short rate are the factors; the model could To study nominal bonds we need to introduce some new notation. We 
be written equally well in terms of any two bond yields of fixed maturities. write the nominal price index at time t as Q ,a nd the gross rate of inflation 
Third, affine-yield models with K state variables imply that the term structure from t to t + 1 as lIt+l = Q+l /Q .  We have already defined Pnt to be 
of interest rates can be summarized by the levels of K bond yields at each the real price of an n-period real bond which pays one goods unit at time 
point in time and the constant coefficients relating other bond yields to the t + n; we now define 2, to be the nominal price of an n-period nominal 
K basis yields. In this sense affine-yield models are linear; their nonlinearity bond which pays $1 at time t + n. From these definitions it follows that the 
is confined to the process governing the intertemporal evolution of the K nominal price of an n-period real bond is P,, Q , a nd the real price of an 
basis yields and the relation between the cross-sectional coefficients and the n-period nominal bond is e , / Q .  We do not adopt any special notation for 
underlying parameters of the model. these last two concepts. 

Affine-yield models also have some disadvantages. The linear relations If we now apply the general asset pricing condition, 
among bond yields mean that the covariance matrix of bond returns has 
rank K--equivalently, we can perfectly fit the return on any bond using a 
regression on K other contemporaneous bond returns. This implication 
will always be rejected by a data set containing more than K bonds, unless to the real return on an n-period nominal bond, we find that 
we add extra error terms to the model. Affine-yield models also limit the way 
in which interest rate volatility can change with the level of interest rates; 
for example a model in which volatility is proportional to the square of the 
interest rate is not affine. Finally, as Constantinides (1992) emphasizes, L 

single-factor affine-yield models imply that risk premia on long-term bonds Multiplying through by Q , w e have 
always have the same sign. 

If we move outside the affine-yield class of models, we can no longer 
work with equation (11 .1. l )  but must return to the underlying nonlinear 
difference equation (11.0.1) or its n-period representation (11 .0.2). In gen- 
eral these equations must be solved numerically. One common method is to 
set up a binomial tree for the short-term interest rate. Black, Derman, and 
Toy (1990) and Black and Karasinski (1991),f or example, assume that the 
simple one-period yield Ylt is conditionally lognormal (as opposed to the 
assumption of affine-yield models that (1+  Ylt)i s conditionally lognormal). where ML1 = Ml+l/171+lc an be thought of as a nominal stochastic discount 
They use a binomial tree to solve their models for the implied term struc- factor that prices nominal returns. 
ture of interest rates. Constantinides (1992), however, presents a model The empirical literature on nominal bonds uses this result in one of two 
that can be solved in closed form. His model makes the log stochastic dis- ways. The first approach is to take the primitive assumptions that we made 
count factor a sum of noncentral chi-squared random variables rather than 

about Mt+l in Section 11.1 and to apply them instead to ML1.  The real 
a normal random variable, and Constantinides is then able to calculate the term-structure models of the last section are then reinterpreted as nominal 
expectations in (11 .0.2) analytically. term-structure models. Brown and Dybvig (1986),f or example, do this when 

1 1.2 Fitting Term-Structure Models to the Data 5 ~ o mgeo vernments, notably those of Canada, Israel, and the UK have issued bonds whose 
nominal payoffs are linked to a nominal price index. In 1996 the US Treasury is considering 
issuing similar securities. These index-linked bonds approximate real bonds but are rarely 

11.2.1 Real Bonds, Nominal Bonds, and Injlatim exactly equivalent to real bonds. Brovm and Schaefer (1994) give a lucid discussion of the 
imperfections in the UK indexing system, and apply the Cox, Ingersoll, and Ross (1985a) 

The term-structure models described so far apply to bonds whose payoffs are model to UK index-linked bonds. See also Barn and Campbell (1995) and Campbell and 

riskless in real terms. Almost all actual bonds instead have payoffs that are Shiller (1996). 



444 11. Term-Structure Models 11.2. Fitting Term-Structure Models to the Data 445 

they apply the Cox, Ingersoll, and Ross (1985a) square-root model directly 
to data on US nominal bond prices. The square-root model restricts interest 
rates to be positive, and in this respect it is more appropriate for nominal 
interest rates than for real interest rates. 

The second approach is to assume that the two components of the 
nominal stochastic discount factor, M,+l and 1 / ll t+l, are independent of where the last equality uses both the independence of real variables from 
each other. To see how this assumption facilitates empirical work, take logs the price level (which enables us to replace the expectation of a product 
of the nominal stochastic discount factor to get by the product of expectations), and the fact that Pnt = Et[Pn-l,t+lM t+l]. 

Equation (11.2.5) is the desired result that the nominal price of a bond 
which pays $1 at time t + n is the nominal price of a bond which pays one 
unit of goods at time t + n, times the expected real value of $1 at time t + n. 

When the components mt+la nd nt+la re independent, we can price nominal Dividing (11 .2.5) by Q ,  we can see that the same relationship holds between 
bonds by using the insights of Cox, Ingersoll, and Ross (1985a) and Dybvig the real prices of nominal bonds and the real prices of real bonds. Further, 
(1989). Recall from Section 11.1.3 their result that the log bond price in a (11 .2.5) implies that the expected real return on a nominal bond equals the 
model with two independent components of the stochastic discount factor expected real return on a real bond: 
is the sum of the log bond prices implied by each component. We can, for 
example, apply the Longstaff and Schwartz (1992) model to nominal bonds 
by assuming that mt+l is described by a square-root single-factor model, 

112 
-mt+l = xlt + xl, BCl,,+l,a nd that nt+l is known at t and equal to a state 
variable $ 112 

We then get -mt+l = -mt+l + nt+l = xlt + xlt B h , t + l  + x2t, 
and the Longstaff-Schwartz model describes nominal bonds. 

More generally, the assumption that Mt+1a nd l/ n,+l are independent Gibbons and Ramaswamy (1993) use these results to test the implications 
implies that prices of nominal bonds are just prices of real bonds multiplied of real term-structure models for econometric forecasts of real returns on 
by the expectation of the future real value of money, and that expected real nominal bonds. 
returns on nominal bonds are the same as expected real returns on real Although it is extremely convenient to assume that inflation is indepen- 
bonds. To see this, consider equation (11.2.2) with maturity n = 1, and dent of the real stochastic discount factor, this assumption may be unreal- 
note that the independence of Mt+l and 1/11,+1 allows us to replace the istic. Barr and Campbell (1995), Campbell and Ammer (1993), and Pen- 
expectation of their product by the product of their expectations: nacchi (1991), using respectively UK data on indexed and nominal bonds, 

rationalexpectations methodology applied to US data, and survey data, all 
find that innovations to expected inflation are negatively correlated in the 
short run with innovations to expected future real interest rates. More 

since Pit = Et[Mt+1]a nd l/lTt+l = Thus the nominal price of directly, Campbell and Shiller (1996) find that inflation innovations are 
a bond which pays $1 tomorrow is the nominal price of a bond which pays correlated with stock returns and real consumption growth, proxies for the 
one unit of goods tomorrow, times the expectation of the real value of $1 stochastic discount factor suggested by the traditional CAPM of Chapter 5 
tomorrow. and the consumption CAPM of Chapter 8. 

We now guess that a similar relationship holds for all maturities n, and 
we prove this by induction. If the (n-1 )-period relationship holds, e-l,=t  11.2.2 Empirical Evidence on Affine-Yield Models 
pn-1,t  Q Et[l/Q+n-11, then 

All the models we have discussed so far need additional error terms if they 
are to fit the data. To see why, consider a model in which the real stochastic 
discount factor is driven by a single state variable. In such a model, returns 
on all real bonds are perfectly correlated because the model has only a single 
shock. Similarly, returns on all nominal bonds are perfectly correlated in any 



446 11.  Term-Structure Models 11.2. Fitting TmStructure Models to the Data 447 

model where a single state variable drives the nominal stochastic discount Now suppose that we do not observe the true excess returns on long 
factor. In reality there are no deterministic linear relationships among bonds, but instead observe a noisy measure 
returns on different bonds, so these implications are bound to be rejected 
by the data. Adding extra state variables increases the rank of the variance- 
covariance matrix of bond returns from one to K, where K is the number where q,,,+l is an error term. We assume that q,,t+l is orthogonal to avector 
of state variables, but whenever there are more than K bonds the matrix ht  containing J instruments 4" j = 1 . . . J :  
remains singular-equivalently, there are deterministic linear relationships 
among bond returns. So these models, too, are trivially rejected by the data. 

To handle this problem empirical researchers allow additional error 
terms to affect bond prices. These errors may be thought of as measurement The vector ht  might contain lagged variables, for example, if the return 
errors in bond prices, errors in calculating implied zerocoupon prices from error q , , + l  is serially uncorrelated. We further assume that for each state 
an observed coupon-bearing term structure, or model specification errors variable xk,, k = 1 . . . K, the expectation of the state variable conditional 
arising from tax effects or transactions costs. Alternatively, if one uses a on the instruments is linear in the instruments: 
model for the real stochastic discount factor and tests it on nominal bonds 
in the manner of Gibbons and Ramaswamy (1993) , t he errors may arise 
from unexpected inflation. 

Whatever the source of the additional errors, auxiliary assumptions 
about their behavior are needed to keep the model testable. One common for some constant coefficients Okj.  
assumption is that bond-price errors are serially uncorrelated, although they These assumptions imply that the expectation of en,t+l conditional on 
may be correlated across bonds. This assumption makes it easy to examine the instruments, which from (11.2.10)i s the same as the expectation of the 
the time-series implications of term-structure models. Other authors as- true excess return rn,t+l-  ylt  conditional on the instruments, is linear in the 
sume that bond-price errors are uncorrelated across bonds, although they instruments: 
may be correlated over time. 

AfJine-Yield Models as Latent-Variable Models 
Stambaugh (1988) and Heston (1992) show that under fairly weak assump- 
tions about the additional bond price errors, an affine-yield model implies 
a latent-variable structure for bond returns. Variables that forecast bond 
returns can do so only as proxies for the underlying state variables of the If we define et+l to be the vector [ Q , ~ +.] .  . e ~ , , + lf]o r assets n = 1 . . . N, 
model; if there are fewer state variables than forecasting variables, this puts then ( 11 .2.12) can be rewritten in vector form as 
testable restrictions on forecasting equations for bond returns. 

A general affine-yield model with K state variables takes the form 

where A* is a vector whose nth element is A*,a nd C is a matrix of coefficients 
whose ( n ,  j )  element is 

where xkt, k = 1 . . . K, are the state variables, and A, and Bkn,k  = 1 . . . K ,  K 

are constants. The model also implies that expected excess returns on long Cnl = C Bin@kj- (11.2.14) 
bonds over the short interest rate can be written as k= 1 

Equations ( 11 .2.13) and ( 1 1 .2.14) define a latent-variable model for 
expected excess bond returns with K latent variables. Equation ( 1 1 .2.14) 

where A*, and Bin, k = 1 . . .  K ,  are constants. The model puts cross- says that the (N x J ) m atrix of coefficients of N assets on J instruments has 
rank at most K, where K is the number of state variables in the underlying 

sectional restrictions on these constants which are related to the time-series 
term-structure model. The instruments forecast excess bond returns only 

process driving the state variables, but we ignore this aspect of the model through their ability to proxy for the state variables (measured by the Bkj 
here. 



448 I I. Term-Structure Models 11.2. Fitting Term-Structure Models to the Data 449 

coefficients) and the role of the state variables in determining excess bond Evidence on the Short-Rate Process 
returns (measured by the Bin coefficients). The system is particularly easy If one is willing to assume that there is negligible measurement error in 
to understand in the single-factor case. Here K = 1, we can drop the k the short-term nominal interest rate, then time-series analysis of short-rate 
subscripts, and (11 .2.14) becomes behavior may be a useful first step in building a nominal term-structure 

model. Chan, Karolyi, Longstaff, and Sanders (1992) estimate a discrete- 
time model for the short rate of the form 

Equation (11 .2.15) says that each row of the matrix C is proportional to each where 

other row, and the coefficients of proportionality are ratios of Bi coefficients. Et[rt+ll = 0, E,[G2 ,+~I 2 2~ 
= a ylt . (11.2.17) 

Note that the rank of the matrix C could be less than K; for example, it is This specification nests the single-factor models we discussed in Section 11.1; 
zero in a homoskedastic model with K state variables because in such a the homoskedastic model has y = 0, while the square-root model has 
model the coefficients Bin are zero for all k and n. y = 0.5. It also approximates a continuous-time diffusion process for the 

Latent-variable models of the form (11 .2.14) or (11 .2.15) have been instantaneous short rate r(t) of the form dr = (Po + B1r)dt + arYdB. Such 
applied to financial data by Campbell (1987), Gibbons and Ferson (1985), a diffusion process nests the major single-factor continuous-time models for 
and Hansen and Hodrick (1983). They can be estimated by Generalized the short rate. The Vasicek (1977) model, for example, has y = 0; the 
Method of Moments applied to the system of regression equations (11 .2.13). Cox, Ingersoll, and Ross (1985a) model has y = 0.5, and the Brennan and 
Heston (1992) points out that one can equivalently estimate a system of in- Schwartz (1979) model has y = 1.6 
strumental variables regressions of excess returns on K basis excess returns, Chan et al. (1992) estimate (11.2.16) and (11.2.17) by Generalized 
where the elements of ht are the instruments. Method of Moments. They define an error vector with two elements, the first 

A key issue is how to choose instruments ht that satisfy (11 .2.10) (or- 2 
beingyl,t+l-(l+B)ylt-aandthesecondbeing(yl,t+l - (1 +B)ylt -a) - 

thogonality of instruments and bond pricing errors) and (11.2.11) (state a 2 y 2 .  These errors are orthogonal to any instruments known at time t; a 
variables linear in the instruments). In an affine-yield model without bond constant and the level of the short rate ylt are used as the instruments. In 
pricing errors, bond yields and forward rates are linear in the state variables; monthly data on a one-month Treasury bill rate over the period 1964:6 to 
hence the state variables are linear in yields and forward rates. This property 1989:12, Chan et al. find that cr and B are small and often statistically in- 
survives the addition of normally distributed bond pricing errors. Thus it is significant. The short-term interest rate is highly persistent so it is hard to 
natural to choose yields or forward rates as instruments satisfymg (11 .2.11 ). reject the hypothesis that it follows a random walk. They also find that y 

To satisfy (11.2.10), one must be more specific about the nature of the is large and precisely estimated. They can reject all models which make 
bond pricing errors. The error in a bond price measured at time t affects y < 1, and their unrestricted estimate of y is about 1.5 and almost two 
both the time t bond yield and the excess return on the bond from t to standard errors above 1. 
t + 1. Hence yields and forward rates measured at time t are not likely to To understand these results, consider the case where a = B = 0, SO 
be orthogonal to errors in excess bond returns from t to t + 1. If the bond the short rate is a random walk without drift. Then the error term rt+l in 
price errors are uncorrelated across time, however, then yields and forward (11.2.16) isjust the change in the short rate yl,t+l -ylt, and (11 .2.17) says that 
rates measured at time t - 1 will be orthogonal to excess bond return errors the expectation of the squared change in the short rate, Et [(yl,t+i-  yi t )2~=  
from t to t + 1; and if the bond price errors are uncorrelated across bonds, a2y:. Equivalently, 
then one can choose a set of yields or forward rates measured at different 
maturities than those used for excess returns. Stambaugh (1988) applies 
both these strategies to monthly data on US Treasury bills of maturities two 

- to six months over the period 1959:3 to 1985:ll. He finds strong evidence 
against a model with one state variable and weaker evidence against a model so when the change in the short rate is scaled by the appropriate power of 
with two state variables. Heston (1992) studies a more recent period, 1970:2 the short rate, it becomes homoskedastic. Figures 1l . la through d illustrate 
to 1988:5, and a data set including longer maturities (6, 12, 36, and 60 
months) and finds little evidence against a model with one state variable. ' ~ o t eh owever that ( 1  1.2.16) and (11.2.17) do not nest the PearsonSun model. 



11.2. Fitting Term-Structure Models to the Data 45 1 

the results of Chan et al. by plotting changes in short rates scaled by various 
powers of short rates. The figures show (y l , t+ l  - y l t ) l ( y r t )  for y = 0, 0.5, 
1, and 1.5, using the data of McCulloch and Kwon (1993) over the period 
1952:l to 1991:2. Over the period since 1964 studied by Chan et a]., it is 
striking how the variance of the series appears to stabilize as one increases 
y from 0 to 1.5. 

These results raise two problems for single-factor affine-yield models of 
the nominal term structure. First, when there is no mean-reversion in the 
short rate, forward rates and bond yields may rise with maturity initially, but 
they eventually decline with maturity and continue to do so forever. Second, 
single-factor affine-yield models require that y = 0 or 0.5 in (11 .2.17). The 
estimated value of 1.5 takes one outside the tractable class of affine-yield 
models and forces one to solve the term-structure model numerically. 

There is as yet no consensus about how to resolve these problems. Ait- 
Sahalia (1996b) argues that existing parametric models are too restrictive; 
he proposes a nonparametric method for estimating the drift and volatility of 
the short interest rate. He argues that the short rate is very close to a random 
walk when it is in the middle of its historical range (roughly, between 4% and 
17%),b ut that it mean-reverts strongly when it gets outside this range. Chan 
et al. miss this because their linear model does not allow mean-reversion to 
depend on the level of the interest rate. Ait-Sahalia also argues that interest- 
rate volatility is related to the level of the interest rate in a more complicated 
way than is allowed by any standard model. His most general parametric 
model, and the only one he does not reject statistically, has the short interest 
rate following the diffusion dr  = (Do + Dl r + D2r2 + D ~ / r ) d+t  (00+  0 1 r  + 
02ry)dB. He estimates y to be about 2, but the other parameters of the 
volatility function also play an important role in determining volatility. 

Following Hamilton (1989), an alternative view is that the short rate 
randomly switches among different regimes, each of which has its own mean 
and volatility parameters. Such a model may have mean-reversion within 
each regime, but the short rate may appear to be highly persistent when one 
averages data from different regimes. If regimes with high mean parameters 
are also regimes with high volatility parameters, then such a model may also 
explain the apparent sensitivity of interest rate volatility to the level of the 
interest rate without invoking a high value of y .  Figures 1 l.la-d show that 
no single value of y makes scaled interest rate changes homoskedastic over 
the whole period since 1952; the choice of y = 1.5 works very well for 
1964 to 1991 but worsens heteroskedasticity in the 1950s and early 1960s.' 
Thus at least some regime changes are needed to fit the data, and it may 
be that a model with y = 0 or y = 0.5 is adequate once regime changes 

' ~ l t h o u ~thi s is not shown in the figures, the y = 1.5 model also breaks down in the 
1990s. 



452 1 1. Term-Structure Models 11.2. Fitting Term-Structure Models to the Data 453 

are allowed. Gray (1996) explores this possibility but estimates only slightly 
lower values of y than Chan et al., while Naik and Lee (1994) solve for bond 
and bond-option prices in a regimeshift model with y = 0. 

Brenner, Harjes, and Kroner (1996) move in a somewhat different di- 
rection. They allow for GMCH effects on interest rate volatility, as de- 
scribed in Section 12.2 of Chapter 12, as well as the level effect on volatility 
described by (11.2.17). They replace (11.2.17) by E~[ G:+~I = o:y:r and 

= OJ -I-0 6: + a GARCH(1,l) model. They find that a 
model with y = 0.5 fits the short rate series quite well once GARCH effects The first-order autocorrelation of the short rate identifies the autoregressive 
are included in the model; however they do not explore the implications of parameter 4. Given 4, the variance of the short rate then identifies the 
this for bond or bond-option pricing. innovation variance a 2.  Given 4 and a2, the average excess return on a 

very long-term bond, or equivalently the average difference between a very 
ems-Sectional Restrictions 0% the Term Structure long-term forward rate and the short rate, identify the parameter 8. Finally, 
So far we have emphasized the time-series implications of affine-yield mod- given 4, a2,a nd 8, the mean short rate identifies k.  
els and have ignored their cross-sectionali mplications. Brown and Dybvig In the zero-coupon yield data of McCulloch and Kwon (1993) over the 
(1986) and Brown and Schaefer (1994) take the opposite approach, ignor- period 1952 to 1991, the monthly first-order autocorrelation of the short 
ing the models' time-series implications and estimating all the parameters rate is 0.98, implying 4 = 0.98. The standard deviation of the short rate is 
from the term structure of interest rates observed at a point in time. If this 3.064%a t an annual rate or 0.00255 in natural units, implyinga, = 0.00051 
procedure is repeated over many time periods, it generates a sequence of in natural units or 0.610% at an annual rate. 
parameter estimates which in theory be identical for all time periods In the data there is some discrepancy between the average excess return 
but which in practice varies over time. The procedure is analogous to the on long bonds, which from Table 10.2 is negative at -0.048%a t an annual 
common practice of calculating implied volatility by inverting the Black- rate for n = 120, and the average slope of the forward-rate curve, which 
Scholes formula using traded option prices; there too the model requires is positive at 1.507% at an annual rate when measured by the difference 
that volatility be constant over time, but implied volatility tends to move over between a 60-120 month forward rate and the 1-month short rate. The 
time. difference occurs because interest rates rose over the period 1952 to 1991; 

Of course, bond pricing errors might cause estimated parameters to stationary term-structure models force the true mean change in interest 
shift over time even if true underlying parameters are constant. But in simple rates to be zero, but an increase in interest rates in a particular sample can 
term-structure models there also appear to be some systematic differences make the sample mean excess return on long bonds negative even when 
between the parameter values to fit cross-sectional term-structure the sample mean slope of the forward-rate curve is positive. The value of 
data and the parameter values implied by the time-series behavior of interest B required to fit the average slope of the forward-rate curve is -122. The 
rates. These systematic differences are indicative of misspecification in the implied value for p - a,2/2, expressed at an annual rate, is 7.632%. 
models. To understand the problem, we will choose parameters in the The difficulty with the homoskedastic single-factor model is that with 
single-factor homoskedastic and square-root models to fit various simple these parameters the average forward-rate term-structure curves very grad- 
moments of the data and will show that the resulting model does not match ually from its initial value to its asymptote, as shown by the dashed line in 
some other characteristics of the data. Figure 11.2. The sample average forward-rate curve over the 1952 to 1991 

In the homoskedastic single-factor model, the important parameters of period, shown by the solid line in Figure 11.2, rises much more steeply at 
the model can be identified by the following four moments of first and then flattens out at about five years maturity. 

the data: This problem arises because the theoretical average forward-rate curve 
approaches its asymptote approximately at geometric rate 4. One could 
match the sample average f0nm-d-rate curve more closely by choosing a 
smaller value of 4. Unfortunately this would be inconsistent not only with 
the observed ~ersistencoe f the short rate, but also with the observed pattern 
of volatility in forward rates. Equation (11-1.14) shows that the standard 



11.3. Pricing Fixed-Income Derivative Securities 455 
454 11. Term-Structure Models 

where B is the limiting value of Bn from equation (11 .1.23). As before, we 
9 can identify 4 = 0.98 from the estimated first-order autocorrelation of the 
 -.I I short rate, but now the other parameters of the model are simultaneously 

determined. One can of course estimate them by Generalized Method of 
Moments. The square-root model, like the homoskedastic model, produces 
an average forward-rate curve that approaches its asymptote very ~ 1 0 w l y w h ~ ~  
the short rate is highly persistent; thus the model has many of the same 
empirical limitations as the homoskedastic model. 

In summary, the single-factor affine-yield models we have described in 
this chapter are too restrictive to fit the behavior of nominal interest rates. 
The latent-variable structure of the data, the nature of the short rate pro- 
cess, and the shape of the average term structure are all hard to fit with 
these models. In response to this researchers are exploring more general 
models, including affine-yield models in which the single state variable fol- 
lows a higher-order ARMA process (Backus and Zin [I9941) , affine-yield 
models with several state variables (Longstaff and Schwartz [1992]), regime- 
switching models (Gray [1996], Naik and Lee [I9941) , and GARCH models 
of interest rate volatility (Brenner, Harjes, and Kroner [1996]). No one 

In model has yet emerged as the consensus choice for modeling the nominal 
0 2 4 6 8 10 12 

Maturity in years term structure. We note however that Brown and Schaefer (1994) and Gib 
bons and Ramaswamy (1993) have achieved some success in fitting simple 
models to prices of UK index-linked bonds and econometric forecasts of 

Figure 11.2. Sample and Theoretical Average Finward-Rate Curves 
real returns on US nominal bonds. Thus single-factor affine-yield models 
may be more appropriate for modeling real interest rates than for modeling 

deviation of the forward rate declines at rate 4. In the 1952 to 1991 period nominal interest rates. 
the standard deviation of the n-period forward rate barely declines at all 
with maturity n, and this feature of the data can only be fit by an extremely 
persistent short-rate process. Backus and Zin (1994) discuss this problem 
in detail and suggest that a higher-order model which allows both transitory 
and persistent movements in the short rate can fit the term structure more 1 1.3 Pricing Fixed-Income Derivative Securities 
successfully. 

Parameter identification is somewhat more difficult in the square-root One of the main reasons for the explosion of interest in term-structure 
model. Here the moments given in equation (11.2.19) become models is the practical need to price and hedge fixed-income derivative se- 

curities. In this section we show how term-structure models can be used in 
this context. Section 11.3.1 begins by discussing ways to augment standard 
term-structure models so that they fit the current yield curve exactly. Deriva- 
tives traders usually want to take this yield curve as given, and so they want 
to use a pricing model that is fully consistent with all current bond prices. 
We explain the popular approaches of Ho and Lee (1986), Black, Derman, 
and Toy (1990), and Heath, Jarrow, and Morton (1992). Section 11.3.2 
shows how term-structure models can be used to price forward and futures 
contracts on fixed-income securities, while Section 11.3.3 explores option 
pricing in the context of a term-structure model. 



456 I I .  Term-Structure Models 11.3. Pricing Fixed-Income Derivative Securities 457 

11.3.1 Fitting the Current Term Structure Exactly Backus, Foresi, and Zin (1996) illustrate this problem as follows. They as- 
sume that the homoskedastic single-factor model of subsection 11 .1.1 holds, 

In general a model gives an exact fit to as many data points as it has parame- with a mean-reverting short rate so 4 c 1. They show that one can exactly 
ters. The homoskedastic single-factor model presented in Section 11 .1, for fit the current term structure with a homoskedastic random walk model, a 
example, has four parameters, 4, B ,  a', and p. Inevitably this model does lognormal version of Ho and Lee (1986). The model uses equation (1 1.1.5), 
not fit the whole term structure exactly. To allow for this the empirical work but replaces equation (11 .1.3) with 
of the previous section added error terms, reflecting model specification 
error and measurement error in bond prices. 

In pricing fixed-income derivative securities it may be desirable to have where gt+, is a deterministic drift term that is specified at time t for all 
a model that does fit the current term structure exactly. To achieve this, we future dates t + i in order to fit the time t term structure of interest rates, 
can use the result of Cox, Ingersoll, and Ross (1985a) and Dybvig (1989) and as before t t+i  is a normally distributed shock with constant variance a'. 
that one can add independent term-structure models together. A simple Backus, Foresi, and Zin (1996) show that this model does not capture the 
approach, due originally to Ho and Lee (1986), is to break observed forward conditional means or  variances of future interest rates, and so it misprices 
rates fnt into two components: options on bonds. Problem 11.1 works this out in detail. 

A somewhat more sophisticated procedure for fitting the term struc- 
ture of interest rates specifies future deterministic volatilities of short rate 

where f,", is the forward rate implied by a standard tractable model and fit is movements, as well as future deterministic drifts. Black, Derman, and Toy 
the residual. The residual component is then attributed to a deterministic (1990) do  this in a lognormal model for the short rate. In the present 
term-structure model. Since a deterministic process is independent of any model one can replace the constant variance of t t + i ,a *,w ith a determin- 
stochastic process, the decomposition (11 .3.1) is always legitimate. There istically time-varying one-period-ahead conditional variance Backus, 
is a corresponding decomposition of the stochastic discount factor, Foresi, and Zin (1996) show that if the true model is the mean-reverting ho- 

moskedastic model, the misspecified random walk model with deterministic 
mt+1 = mp+1 + mp+1. (1 volatilities and drifts can fit any two of the current term structure, the con- 

ditional means of future short rates, and the conditional variances of future 
In a deterministic model, the absence of arbitrage requires that short rates. However it still cannot fit all three of these simultaneously, and 

b b so it cannot correctly price a complete set of bond options. The lesson of 
fnt = ~ l . t + n=  mt6,n. (1 this example is that fixed-income derivative security prices depend on the 

dynamic behavior of interest rates, so it is important to model interest rate 
Thus we are postulating that future stochastic discount factors contain a 

dynamics as accurately as possible even if one is interested only in pricing 
deterministic component that is reflected in future short-term interest rates 

derivative securities today. 
and current forward rates. 

A related approach that has become very popular is due to Heath, Jar- 
Although this procedure works well in any one period, there is nothing 

row, and Morton (1992). These authors start from the current forward-rate 
to ensure that it will be consistent from period to period. A typical applica- 

curve discussed in Chapter 10, and they suggest that one should specify a 
tion of the approach sets y;, = 0, so that the current short rate is used as 

term structure of forward volatilities to determine the movements of future 
an input into the stochastic term-structure model without any adjustment 

risk-adjusted forward rates. To understand this approach as simply as possi- 
for a deterministic component. Deterministic components of future short 

ble, suppose that interest-rate risk is unpriced, so there are no risk premia 
rates y;,,+, are then set to nonzero values to fit the time t term structure. in bond markets and the objective process for forward rates coincides with 
When time t + n arrives, however, this procedure is repeated; now y;,,+, is the risk-adjusted process. In this case bonds of all maturities must have 
set to zero and deterministic components of more distant future short rates the same expected instantaneous return in a continuous-time setting, and 
are made nonzero to fit the time t + n term structure. As Dybvig (1989) the same expected one-period return in a discrete-time setting. That is, the 
emphasizes, this time inconsistency is troublesome although the procedure one-period version of the pure expectations hypothesis of the term structure 
may work well for some purposes. (PEH) holds, so from (10.2.6) of Chapter 10 we have 

It is also important to understand that fitting one set of asset prices ex- 
actly does not guarantee that a model will fit other asset prices accurately. 



458 11. T d t m c t u r e  Models 11.3. Pricing Fixed-Income Derivative Securities 459 

The expected log excess return on a bond of any maturity over the one- is the price of a claim to a payoff of St+,. Intuitively, the Pntt erms appear 
period interest rate is minus one-half the variance of the log excess return. because no money need be paid until time t + n; thus the purchaser of a 

Now recall the relation between an n-period-ahead 1-period log forward forward contract has the use of money between t and t + n. Cox, Ingersoll, 
rate fnt and log bond prices, given as (10.1.8) in Chapter 10: fnt = pnt-  and Ross establish this proposition using a simple arbitrage argument. They 
pn+l,tT. his implies that the change from time t to time t + 1 in a forward consider the following investment strategy: At time t, take a long position 
rate for an investment to be made at time t + n is in l/Pntf orward contracts and put Grit into n-period bonds. By doing this 

one can purchase Gnt/Pntb onds.   he payoff from this strategy at time t + n 
i s  

where the first term is the profit or loss on the forward contracts and the 
Taking expectations of (11 .3.6) and using (11 .3.5), we find that second term is the payoff on the bonds. Since this investment strategy costs 

Grit at time t and pays St+,/Pnt at time t + n, the proposition is established. 
1 

t h - t  - ~ L I=  (5) (vart[rn+i,t+i- ?it] -vart[rn,t+l- y1t1). It can also be stated using stochasticdiscount-factor notation as 

(1 1.3.7) 
The conditional variances of future excess bond returns determine the ex- 
pected changes in forward rates, and these expected changes together with where the n-period stochastic discount factor Mn,,+, is the product of n 
the current forward-rate curve determine the forward-rate curves and yield successive one-period stochastic discount factors: Mn,,+, = Mt+l.  . . Mt+,. 
curves that are expected to prevail at every date in the future. Similar prop- A futures contract differs from a forward contract in one important re- 
erties hold for the risk-adjusted forward-rate process even when interest-rate spect: It is marked to market each period during the life of the contract, so that 
risk is priced. the purchaser of a futures contract receives the futures price increase or pays 

Heath, Jarrow, and Morton (1992) exploit this insight in a continuous- the futures price decrease each period. Because of these margin payments, 
time setting and show how it can be used to price fixed-income derivative futures pricing-unlike forward pricing-generally involves more than just 
securities. It is still important to model interest-rate dynamics accurately the two periods t and t + n.g If we write the price of an n-period futures 
in this framework, but now the parameters of the model are expressed as contract as Hnt,t hen we have 
volatilities; many participants in the markets for fixed-income securities find 
it easier to work with these parameters than with the parameters that govern 
short-rate dynamics and interest-rate risk prices in traditional models. A 
drawback of this approach, however, is that the implied process for the This can be established using a similar argument to that of Cox, Ingersoll, 
short-term interest rate is generally extremely complicated. and Ross. Consider the following investment strategy: At time t, take a long 

position in l/Pltf utures contracts and put Hnti nto one-period bonds. By 
doing this one can purchase Hnl/Plt bonds. At time t + 1, liquidate the 

11.3.2 Forwards and Futures futures contracts. The payoff from this strategy at time t + 1 is 
A particularly simple kind of derivative security is a forward contract. An 
n-period forward contract, negotiated at time t on an underlying security 
with price St+, at time t + n, specifies a price at which the security will be 
purchased at time t + n. Thus the forward price, which we write Grit, is 
determined at time t but no money changes hands until time t + n.8  he Treasury-bond and Treasury-note futures contracts traded on the Chicago Board of 

Trade also have a number of special option features that affect their prices. A trader with a 
Cox, Ingersoll, and Ross (1981b) show that the forward price Gnti s the short position can choose to deliver on any day within the settlement month and can choose 

time t price of a claim to a payoff of St+,/Pnta t time t+ n. Equivalently, Gnt P n t  to deliver a number of alternative bonds. The short trader also has a "wild card option" to 
announce delivery at a particular day's settlement price any time in the six hours after that 
price is determined. The discussion here abstracts from these option features; see Hull (1993, 

 he n-period forward rate defined in Section 10.1.1 of Chapter 10 is the yield on a forward Chapter 4) for an introduction to them. 
contract to buy a zero-coupon bond with maturity date t + n + 1 at time t + n. 



460 11. Term-Structure Models 11.3. Pricing Fixed-Income Dm'vatiue Securities 46 1 

where the first term is the mark-to-market payment on the futures contracts Problem 11.2 is to to do this for the homoskedastic single-factor model de- 
purchased at time t and the second term is the payoff on the bonds. Be- veloped in Section l l .1 .1. The problem is to show that the ratio of forward 
cause the futures contracts are marked to market, the entire position can to futures prices is constant in that model, and that it exceeds one so that 
be liquidated at time t + 1 without generating any further cash flows at time forward prices are always greater than futures prices. 
t + n. Since this investment strategy costs Hnta t time t and pays Hn-l,t+l/Plt 
at time t+ 1, we have shown that (11 .3.10) holds. Furthermore, we can solve 11.3.3 Option Pricing in a Term-Structure Model 
(11.3.10) forward to time t + n, using the fact that Ho,~+=,  St+,, to obtain 

Suppose one wauts to price a European call option written on an underlying 
securitywith price St.'' If the option has n periods to expiration and exercise 
price X, then its terminal payoff is Max(&+, - X, 0). It can be priced like any 
other n-period asset using the n-period stochastic discount factor Mn,,+, = 

Comparing equations (11 .3.9) and (11 .3.12), we can see that there are Mt+,.  . . Mt+,. Writing the option price as Cnt(X),w e have 
some circumstances where forward contracts and futures contracts written 
on the same underlying asset with the same maturity have equal prices. 
First, if bond prices are not random then absence of arbitrage requires that 
Pnt = Pl,t+i,s o Gnt = Hnt. This means that forward and futures 
prices are equal in any model with a constant interest rate. Second, if 
there is only one period to maturity then Pnt = Plt and again Gnt = Hnt. In general equation (11 .3.16) must be evaluated using numerical meth- 
Since futures contracts are marked to market daily the period here must ods, but it simplifies dramatically in one special case. Suppose that Mn,,+, 
be one day, so this result is of limited interest. Third, if the price of the and St+, are jointly lognormal conditional on time t information, with con- 
underlying asset is not random, then forward and futures prices both equal ditional expectations of their logs p, and p,, and conditional variances and 
the underlying asset price. To see this, note that if St+n = V, a constant, covariance of their logs a, a,,, and a,. All these moments may depend 
then (11 .3.9) becomes on t and n, but we suppress this for notational simplicity. Then we have 

since Pnt = Et [MnIt+,]. Under the same conditions Hlt = V, and we can am, + a,, + 2am, 
show that Hnt = V if Hn-l,t+l=  V because (11 .3.10) becomes 2 

Thus Hnt = V for all n, so forward and futures prices are equal. 
More generally, however, forward and futures prices may differ. In the and 

case where the underlying asset is an n+ t-period zero-coupon bond at time 
t, which will be a t-period bond at time t + n, we can write the forward price 
as Grnta nd the futures price as Hrnt.T he forward price is easy to calculate 
in this case: where @(.) is the cumulative distribution function of a standard normal 

random variable, and x r log(X).ll 
Equations (11 .3.17) and (11 .3.18) hold for any lognormal random vari- 

ables M and Sand do not depend on any other properties of these variables. 
When t = 1 the yield on this forward contract is the forward rate defined 
in Section 10.1.1 of Chapter 10: F,, = l/Glnt. 'O~hen otation here differs from the notation used in Chapter 9. There Pt is used for the 

The futures price must be calculated recursively from equation underlying security price, but here we reserve P for zero-coupon bond prices and use St for a 
(11.3.10). In a particular term-structure model one can do the calculation generic security price. 

explicitly and solve for the relation between forward and futures prices. ' ' ~ h e sree sults were derived by Rubinstein (1976); s ee also Huang and Litzenberger (1988). 



462 1 I. Term-Structure Models 11 .?. Pricing Fixed-Income Den'vatiue Securities 463 

But we know from asset pricing theory that the underlying security price St X, 0). The relevant bond price at expiration is the r-period bond price since 
must satisfy the maturity of the bond shrinks over time. In a term- structure model the 

conditional volatility of the r-period bond price n periods ahead is not 
generally n times the conditional volatility of the (n + r - 1)-period bond 
price one period ahead. Also, of course, the term structure is generally not 
flat in a term-structure model. 

We also know that the price of an n-period zerocoupon bond, Pnt,m ust To get closed-form solutions for interest-rate derivatives prices we need 
satisfy a term-structure model in which bond prices and stochastic discount factors 

P n t  = Et[Mn,t+nI = exp (11 .3.20) are conditionally lognormal at all horizons; that is, we need the homoskedas- 
tic single-factor model of Section 11 .1.1 or some multifac tor generalization 

Using (11 .3.19) and (11.3.20) to simplify (11 .3.17) and (11 .3.18), and substi- of it. In the single-factor model we can use the option pricing formula 
tuting into (11 .3.16),w e get an expression for the price of a call option when (11.3.21) with the following inputs: St = Pn+,,,=  exp(-An+, - Bn+, x,), 
the underlying security is jointly lognormal with the multiperiod stochastic Pnt = exp(-A, - Bn xt), and 
discount factor: 

This expression for a,, does not grow linearly with n. Hence if one uses the 
Black-Scholes formula (11.3.22) and calculates implied volatility, the im- 

st - x - p n t  - oss/2) 
-XPnt * ( plied volatility will depend on the maturity of the option; there will be a term 

ass  structure of implied uolatility that will depend on the parameters of the under- 

To get the standard option pricing formula of Black and Scholes (1973), lying term- structure model. Jamshidian (1989) presents a continuous-time 

we need two further assumptions. First, assume that the conditional variance version of this result, and Turnbull and Milne (1991) derive it in discrete 

of the underlying security price n periods ahead, a,,, is proportional to n: time along with numerous results for other types of derivative securities. 

a,, = Option pricing is considerably more difficult in a square-root model, but 
no2 for some constant a*. Second, assume that the term structure 

is flat so that Pnt = e-'" for some constant interest rate r. With these Cox, Ingersoll, and Ross (1985a) present some useful results. 

additional assumptions, (11 .3.21) yields the Black-Scholes f o r m ~ l a , ' ~  Investment professionals often want to price options in a way that is ex- 
actly consistent with the current term structure of interest rates. To do this, 
we can break the n-period stochastic discount factor into two components: 

where, as in Section 11.3.1, the a-component is stochastic while the b 
component is deterministic. There is a corresponding decomposition of 

For fixed-income derivatives, however, the extra assumptions needed to bond prices for any maturity j: 4, = Z$l$. Then it is easy to show that 
get the Black-Scholes formula (11 .3.22) are not reasonable. Suppose that 
the asset on which the call option is written is a zerocoupon bond which 
currently has n+ r periods to maturity. If the option has exercise price X and 
n periods to expiration, the option's payoff at expiration will be Max(P,, t+n - 

"of  course, for any given n  we can always define a' = a , , / n  and r = -p , r /n  so that 
the Black-Scholes formula applies for that n. The assumptions given are needed for the Black- 
Scholes formula to apply to all n  with the same rand a'. 



464 11. Ten-Structure M o d A  

where Ca is the call option price that would prevail if the stochastic discount 11.1.3 Compare the time t conditional variances of log bond prices at 
factor were Ma. In other words options can be priced using the stochas- time t + 1 implied by the true bond pricing model and the random walk 
tic term-structure model, using the deterministic model only to adjust the model with deterministic drifts. 
exercise price and the final solution for the option price. This approach 
was first used by Ho and Lee (1986); however as Dybvig (1989) points out, 11.1.4 Compare the prices of bond options implied by the true bond 

Ho and Lee choose as their a-model the single-factor homoskedastic model pricing model and the random walk model with deterministic drifts. 

with 4 = 1, which has numerous unappealing properties. Black, Derman, Note: This question is based on Backus and Zin (1994). 
and Toy (1990), Heath, Jarrow, and Morton (1992), and Hull and White 11.2 Define Grntt o be the price at time t of an n-period forward contract 
(1990a) use similar approaches with different choices for the a-model. on a zero-coupon bond which matures at time t + n + r .  Define H,,, to be 

the price at time t of an n-period futures contract on the same zero-coupon 
1 1.4 Conclusion bond. Assume that the homoskedastic single-factor term-structure model 

of Section 11.1.1 holds. 

In this chapter we have thoroughly explored a tractable class of interest- 11.2.1 Show that both the log forward price grnt and the log futures 
rate models, the so-called affine-yield models. In these models log bond price &,, are affine in the state variable xt. Solve for the coefficients 
yields are linear in state variables, which simplifies the analysis of the term determining these prices as functions of the term-structure coefficients 
structure of interest rates and of fixed-income derivative securities. We have A, and B,. 
also seen that affine-yield models have some limitations, particularly in de- 
scribing the dynamics of the short-term nominal interest rate. There is 11.2.2 Show that the ratio of forward to futures prices is constant and 

accordingly great interest in developing more flexible models that allow for greater than one. Give some economic intuition for this result. 

such phenomena as multiple regimes, nonlinear mean-reversion, and seri- 11.2.3 For the parameter values in Section 11.1.2, plot the ratio of for- 
ally correlated interest-rate volatility, and that fully exploit the information ward prices to futures prices as a function of maturity n. 
in the yield curve. 

As the term-structure literature moves forward, it will be important to Note: This question is based on Backus and Zin (1994). 

integrate it with the rest of the asset pricing literature. We have seen that 
term-structure models can be viewed as time-series models for the stochastic 
discount factor. The research on stock returns discussed in Chapter 8 also 
seeks to characterize the behavior of the stochastic discount factor. By com- 
bining the information in the prices of stocks and fixed-income securities 
it should be possible to gain a better understanding of the economic forces 
that determine the prices of financial assets. 

11.1 Assume that the homoskedastic lognormal bond pricing model given 
by equations (11 .1.3) and (11  .l.5) holds with 4 < 1. 

11.1.1 Suppose you fit the current term structure of interest rates using 
a random walk model augmented by deterministic drift terms, equation 
(11 .3.4). Derive an expression relating the drift terms to the state variable 
xt and the parameters of the true bond pricing model. 

11.1.2 Compare the expected future log short rates implied by the true 
bond pricing model and the random walk model with deterministic drifts. 



Nonlinearities in Financial Data 

THE ECONOMETRIC METHODS we discuss in this text are almost all designed 
to detect linear structure in financial data. In Chapter 2, for example, we 
develop time-series tests for predictability of asset returns that use weighted 
combinations of return autocorrelations-linear predictability is the focus. 
The event study of Chapter 4, and the CAPM and APT of Chapters 5 and 
6, are based on linear models of expected returns. And even when we 
broaden our focus in later chapters to include other economic variables such 
as consumption, dividends, and interest rates, the models remain linear. 
This emphasis on linearity should not be too surprising since many of the 
economic models that drive financial econometrics are linear models. 

However, many aspects of economic behavior may not be linear. Exper- 
imental evidence and casual introspection suggest that investors' attitudes 
towards risk and expected return are nonlinear. The terms of many finan- 
cial contracts such as options and other derivative securities are nonlinear. 
And the strategic interactions among market participants, the process by 
which information is incorporated into security prices, and the dynamics 
of economy-wide fluctuations are all inherently nonlinear. Therefore, a 
natural frontier for financial econometrics is the modeling of nonlinear 
phenomena. 

This is quite a challenge, since the collection of nonlinear models is 
much "larger" than the collection of linear models-after all, everything 
which is not linear is nonlinear. Moreover, nonlinear models are generally 
more difficult to analyze than linear ones, rarely producing closed-form ex- 
pressions which can be easily manipulated and empirically implemented. 
In some cases, the only mode of analysis is computational, and this is unfa- 
miliar territory to those of us who are accustomed to thinking analytically, 
intuitively, and linearly. 

But economists of a new generation are creating new models and tools 
that can capture nonlinearities in economic phenomena, and some of these 
models and tools are the focus of this chapter. Exciting advances in dynam- 



468 12. Nonlinearities in Financial Data 12. I. Nonlinear Structure in Uniuariate Time Series 469 

ical systems theory nonlinear time-series analysis, stochastic-volatility mod- where the shocks are assumed to have mean zero and unit variance, and f (.) 
els, nonparametric statistics, and artificial neural networks have fueled the is some unknown function. The generality of this representation makes it 
recent interest in nonlinearities in financial data, and we shall explore each very hard to work with-most models used in practice fall into a somewhat 
of these topics in the following sections. more restricted class that can be written as 

Section 12.1 revisits some of the issues raised in Chapter 2  regarding 
predictability, but from a linear-versus-nonlinear perspective. We present a 
taxonomy of models that distinguishes between models that are nonlinear The function g ( . )r epresents the mean of xt conditional on past information, 
in mean and hence depart from the martingale hypothesis, and models that since [x t ]=  g(ct-1, ct-2, . . .). The innovation in xt is proportional to the 
are nonlinear in variance and hence depart from independence but not shock c t ,  where the coefficient of proportionality is the function h( . ) .  The 
from the martingale hypothesis. square of this function is the variance of xt conditional on past information, 

Section 12.2 explores in greater detail models that are nonlinear in since EtPl[ (xt-Et-1 [ x ~ ] )=~ ]h (ct- l ,  ~ ~ - .2 . .,) 2 .  Models with nonlinear g ( . )  
variance, including univariate and multivariate Generalized Autoregressive are said to be nonlinear in mean, whereas models with nonlinear h( . )2a re 
Conditionally Heteroskedastic (GARCH) and stochastic-volatility models. said to be nonlinear in variance. 

In Sections 12.3 and 12.4 we move beyond parametric time-series mod- To understand the restrictions imposed by ( 12 .1.2)o n ( 12 .1.l ) ,c onsider 
els to explore nonparametric methods for fitting nonlinear relationships expanding (12.1.1)i n a Taylor series around ct = 0  for given € , - I ,  el-2, . . .: 
between variables, including smoothing techniques and artificial neural 
networks. Although these techniques are able to uncover a variety of non- 
linearities, they are heavily datadependent and computationally intensive. 
To illustrate the power of these techniques, we present an application to the 

where f i  is the derivative off  with respect to c t , i ts first argument; f i l  is the 
pricing and hedging of derivative securities and to estimating state-price 

second derivative off with respect to et ;a nd so forth. To obtain (12.1.2),w e 
densities. 

drop the higher-order terms in the Taylor expansion and set g ( e t p l ,. . .) = 
We also discuss some of the limitations of these techniques in Sec- 

tion 12.5. The most important limitations are the twin problems of overfit- f (0, ct-1, . . .) and h(ct-1, . . .) = f i  ( 0 ,~ ~ - .1 . ., ).  By dropping higher-order 
terms we link the time-variation in the higher conditional moments of xt 

ting and data-snooping, which plague linear models too but not nearly to 
inflexibly with the time-variation in the second conditional moment of xt, 

the same degree. Unfortunately, we have very little to say about how to deal 
with these issues except in very special cases, hence this is an area with many since for all powers p>2, EtP1[ (x t-  [x , ] )P]=  h ( . )E~ [  $ I .  Those who 
open research questions to be answered. are interested primarily in the first two conditional moments of xt regard 

this restriction as a price worth paying for the greater tractability of (12 .1 .2) .  
Equation (12.1.2) leads to a natural division in the nonlinear time- 

series literature between models of the conditional mean g ( . )  and models 
12.1 Nonlinear Structure in Univariate T i e S eries of the conditional variance h(.)?-. Most time-series models concentrate on 

one form of nonlinearity or the other. A simple nonlinear moving-average 
A typical time-series model relates an observed time series xt to an underlying model, for example, takes the form 
sequence of shocks c t .  In linear time-series analysis the shocks are assumed 
to be uncorrelated but are not necessarily assumed to be IID. By the Wold 
Representation Theorem any time series can be written as an infinite-order 
linear moving average of such shocks, and this linear moving-average rep- Here g(.)  = a c E 1  and h(.)  = 1 .  This model is nonlinear in mean but not 
resentation summarizes the unconditional variance and autocovariances of in variance. The first-order Autoregressive Conditionally Heteroskedastic 

the series. (ARCH) model of Engle (1982) ,o n the other hand, takes the form 

In nonlinear time-series analysis the underlying shocks are typically as- 
sumed to be IID, but we seek a possibly nonlinear function relating the series 
xt to the history of the shocks. A general representation is 

Here g(.)  = 0 and h ( - )=  \ / a r t l .  This model is nonlinear in variance but 
not in mean. 



470 12. Nonlinearities in Financial Data 12.1. Nonlinear Structure in Univariate Time Series 471 

One way to understand the distinction between nonlinearity in mean this section: polynomial models, piecewise-linear models, Markov-switching 
and nonlinearity in variance is to consider the moments of the xt process. models, and deterministic chaotic models. 
As we have emphasized, nonlinear models can be constructed so that sec- 
ond moments (autocovariances) E[xt xtPi] are all zero for i>O. In the Polynomial Models 
two examples above it is easy to confirm that this is the case provided One way to represent the function g(.) is expand it in a Taylor series around 
that tt is symmetrically distributed, i.e., its third moment is zero. For t1-1=t t -2= . . . =0, which yields a discrete-time Volterra series (see Volterra 
the nonlinear moving average (12.1.4),f or example, we have E[xt xt-l ] = [1959]): 
~ [ ( t ~ + a c ~ , ) ( t ~ - 1 + a=t  ~aE, )[]tE 1] = 0 when E[tL1]=O. 

Now consider the behavior of higher moments of the form 

Models that are nonlinear in the mean allow these higher moments to be 
nonzero when i, j, k, . . . >O. Models that are nonlinear in variance but obey 
the martingale property have E [xt I xt-1, . . .] =0, so their higher moments 
are zero when i, j, k, . . . >O. These models can only have nonzero higher The single summation in (12.1.6) is a standard linear moving average, the 
moments if at least one time lag index i, j, k, . . . is zero. In the nonlinear- double summation captures the effects of lagged cross-products of two inno- 
moving-average example, (12.1.4), the third moment with i= j= 1, vations, the triple summation captures the effects of lagged cross-products 

of three innovations, and so on. The summations indexed by j start at i, 
the summations indexed by k start at j, and so on to avoid counting a given 
cross-product of innovations more than once. The idea is to represent the 
true nonlinear function of past innovations as a weighted sum of polyno- 

In the first-order ARCH example, (12.1.5),t he same third moment E[xt xE1]  mial functions of the innovations. Equation (12.1.4) is a simple example of 
a model of this form. Robinson (1979) and Priestley (1988) make extensive 

= E [ ( ) t 1 a 2 ] =  0. But for this model the fourth moment with use of this specification. 
i=0, j = k = l ,  E[x; x] ,: = E[t; a2t $-, c2, -~ ]#  0. Polynomial models may also be written in autoregressive form. The 

We discuss ARCH and other models of changingvariance in Section 12.2; function g ( ~ , - ~ ~,  -. 2. .),  relating the conditional mean to past shocks may 
for the remainder of this section we concentrate on nonlinear models of be rewritten as a function g * ( ~ ~xt_-2,~ . ., . ) relating the conditional mean 
the conditional mean. In Section 12.1.1 we explore several alternative ways to lags of xt. The autoregressive version of (12.1.6) is then 
to parametrize nonlinear models, and in Section 12.1.2 we use these para- 
metric models to motivate and explain some commonly used tests for non- 
linearity in univariate time series, including the test of Brock, Dechert, and 
Scheinkman (1987). 

12.1. I Some Parametric Models 

It is impossible to provide an exhaustive account of all nonlinear specifi- 
It is also possible to obtain mixed autoregressive/moving-average repre- 

cations, even when we restrict our attention to the subset of parametric sentations, the nonlinear equivalent of ARMA models. The bilinear model, 
models. Priestley (1988), TerBsvirta, Tjostheim, and Granger (1994), and for example, uses lagged values of x,, lagged values of t,,a nd cross-products 
Tong (1990) provide excellent coverage of many of the most popular non- of the two: 
linear time-series models, including more-specialized models with some very 
intriguing names, e.g., selfexciting threshold autoregression (SETAR), amplitude- 
dependent exponential autoregression (EXPAR),a nd state-dqendentm odels (SDM).  
To provide a sense of the breadth of this area, we discuss four examples in 



472 12. Nonlinearities in Financial Data 12. I. Nonlinear Structure in Univariate Time Series 473 

This model can capture nonlinearities parsimoniously (with a finite, short where s, is an unobservable two-state Markov chain with some transition 
lag length) when pure nonlinear moving-average or nonlinear autoregres- probability matrix P. Note the slightly different timing convention in 
sive models fail to do so. Granger and Andersen (1978) and Subba Rao and (12.1.10): st determines the regime at time t, not st-1. In both regimes, xt 
Gabr (1984) explore bilinear models in detail. is an AR(l), but the parameters (including the variance of the error term) 

differ across regimes, and the change in regime is stochastic and possibly 
Piecewise-Linear Models serially correlated. 
Another popular way to fit nonlinear structure is to use piecewise-linear This model has obvious appeal from an economic perspective. Changes 
functions, as in the first-order threshold autoregression (TAR): in regime are caused by factors other than the series we are currently mod- 

eling (sf determines the regime, not x,), rarely do we know which regime 
we are in (st is unobservable), but after the fact we can often identify which 
regime we were in with some degree of confidence (st can be estimated, 
via Hamilton's [I9891 filtering process). Moreover, the Markov-switching 
model does not suffer from some of the statistical biases that models of 

Here the intercept and slope coefficient in a regression of x, on its lag x,-1 structural breaks do; the regime shifts are "identified" by the interaction be- 
depend on the value of xt-1 in relation to the threshold k. This model can be tween the data and the Markov chain, not by a F'ori inspection of the data. 
generalized to higher orders and multiple thresholds, as explained in detail Hamilton's (1989) application to business cycles is an excellent illustration 
in Tong (1983,1990). of the power and scope of this technique. 

Piecewise-linear models also include change-point models or, as they are 
known in the economics literature, models with structural breaks. In these 

Deterministic Nonlinear Dynamical System 
models, the parameters are assumed to shift-typically once--during a fixed There have been many exciting recent advances in modeling deterministic 
sample period, and the goal is to estimate the two sets of parameters as well as 

nonlinear dynamical system, and these have motivated a number of techniques 
the change point or structural break. Perron (1989) applies this technique 

for estimating nonlinear relationships. Relatively simple systems of ordinary 
to macroeconomic time series, and Brodsky (1993) and Carlstein, Muller, 

differential and difference equations have been shown to exhibit extremely 
and Siegmund (1994) present more recent methods for dealing with change complex dynamics. The popular term for such complexity is the Butterfly 
points, including nonparametric estimators and Bayesian inference. 

Effect, the notion that "a flap of a butterfly's wings in Brazil sets off a tornado 
Change-point methods are very wellestablished in the statistics and op- 

in Texas".' This refers, only half-jokingly, to the following simple system 
erations research literature, but their application to economic models is of deterministic ordinary differential equations proposed by Lorenz (1963) 
not without controversy. Unlike the typical engineering application where for modeling weather patterns: 
a structural break is known to exist in a given dataset, we can never say with 
certainty that a structural break exists in an economic time series. And if 
we think a structural break has occurred because of some major economic 
event, e.g., a stock market crash, this datadriven specification search can 
bias our inferences dramatically towards finding breaks where none exist 
(see, for example, Leamer [I9781 and Lo and MacKinlay [1990]). 

Markov-Switching Models Lorenz (1963) observed that even the slightest change in the starting values 
The Markov-switching model of Hamilton (1989, 1990, 1993) and Sclove of this system-in the fourth decimal place, for example-produces dra- 
(1983a, 1983b) is closely related to the TAR. The key difference is that matically different sample paths, even after only a short time. This sensitivity 
changes in regime are determined not by the level of the process, but by an to initial conditions is a hallmark of the emerging field of chaos themy. 

unobserved state variable which is typically modeled as a Markov chain. For 
example, 'This is adapted from the title of Edward Lorenz's address to the American Association 

for the Advancement of Science in Washington, D.C., December 1979. See Gleick (1987) for 
a lively and entertaining layman's account of the emerging science of nonlinear dynamical 
systems, or chaos theory. 



474 12. Nonlinearities in Financial Data 12.1. Nonlinear Structure in Uniuariate Time Series 475 

There are two serious problems in modeling economic phenomena as de- 
terministic nonlinear dynamical systems. First, unlike the theory that is 
available in many natural sciences, economic theory is generally not spe- 
cific about functional forms. Thus economists rarely have theoretical rea- 
sons for expecting to find one form of nonlinearity rather than another. 
Second, economists are rarely able to conduct controlled experiments, and 
this makes it almost impossible to deduce the parameters of a deterministic 
dynamical system governing economic phenomena, even if such a system ex- 
ists and is low-dimensional. When controlled experiments are feasible, e.g., 
in particle physics, it is possible to recover the dynamics with great precision 
by taking many "snapshots" of the system at closely spaced time intervals. 
This technique, known as a stroboscopic map or a Poincark section, has given 
empirical content to even the most abstract notions of nonlinear dynamical 
systems, but unfortunately cannot be applied to nonexperimental data. 

The possibility that a relatively simple set of nonlinear deterministic 
equations can generate the kind of complexities we see in financial markets 
is tantalizing, but it is of little interest if we cannot recover these equations 

Figure 22.1. The Ta t  Map with any degree of precision. Moreover, the impact of statistical sampling 
errors on a system with sensitive dependence to initial conditions makes 
dynamical systems theory even less practical. Of course, given the rapid 
pace at which this field is advancing, these reservations may be much less 

An even simpler example of a chaotic system is the well-known tent map: serious in a few years. 

12.1.2 Uniuariate Tests for Nonlinear Structure 

Despite the caveats of the previous section, the mathematics of chaos theory 
The tent map can be viewed as a first-order threshold autoregression with has motivated several new statistical tests for independence and nonlinear 
no shock et and with parameters al=O, p1=2, a2=2, and p2= - 2. If xt-1 structure which are valuable in their own right, and we now discuss these 
lies between 0 and 1, xt also lies in this interval; thus the tent map maps the tests. 
unit interval back into itself as illustrated in Figure 12.1. Data generated 
by (12.1.14) appear random in that they are uniformly distributed on the Tests Based on Higher Moments 
unit interval and are serially uncorrelated. Moreover, the data also exhibit Our earlier discussion of higher moments of nonlinear models can serve 
sensitive dependence to initial conditions, which will be verified in Problem as the basis for a statistical test of nonlinearity. Hsieh (1989), for example, 
12.1. Hsieh (1991) presents several other leading examples, while Brock defines a scaled third moment: 
(1986), Holden (1986), and Thompson and Stewart (1986) provide more 
formal discussions of the mathematics of chaotic systems. 

Although the many important breakthroughs in nonlinear dynamical 
systems do have immediate implications for physics, biology, and other 
"hardn sciences, the impact on economics and finance has been less dra- and observes that ~ ( i j,)= O for all i, j>O for IID data, or data generated by a 
matic. While a number of economic applications have been considered,? martingale model that is nonlinear only in variance. He suggests estimating 
none are especially compelling, particularly from an empirical perspective. 

and O'Brien (1993), Pesaran and Potter (1992), Scheinkman and LeBaron (1989), and 
2See,f or example, Boldrin and Woodford (1990).B rock and Sayers ( l988) ,C raig, Kohlase, Scheinkman and Woodford (1994). 

and Papell (1 991), Day ( l983),  Grandmont and Malgrange ( l986),  Hsieh ( l993),  Kennan 



476 12. Nonlinearities in Financial Data 12.1. Nonlinear Structure in Uniuariate Time Series 477 

~ ( ij), i n the obvious way: k: maxi,-, ,..., ,-I I X ~ - ~ - X ~ - ~ (  < k. We define a closeness indicator K,, that is 

+ one if the two n-histories are close to one another and zero otherwise: 

EtX t Xt-i xt-j 
$ ( i  j) - 

[$ Et x:]~'~ ' 

Under the null hypothesis that q(i, j)=O, and with sufficient regularity condi- We define C,,T(~t)o  be the fraction of pairs that are close in this sense, in 
tions imposed on xt so that higher moments exist, 2/7;$(i, j) is asymptotically a sample of n-histories of size T: 
normal and its variance can be consistently estimated by 

The correlation integral Cn(k) is the limit of this fraction as the sample size 
increases: 

Hsieh's test uses one particular third moment of the data, but it is also Cn(k) F lim Cn,~(k). (12.1.21) 
possible to look at several moments simultaneously. The autoregressive poly- T+ cc 

nomial model (12.1.7), for example, suggests that a simple test of nonlinear- Equivalently, it is the probability that a randomly selected pair of n-histories 
ity in the mean is to regress xt onto its own lags and cross-products of its own is close. 
lags, and to test for thejoint significance of the nonlinear terms. Tsay (1986) Obviously the correlation integral will depend on both the embedding 
proposes a test of this sort using second-order terms and M lags for a total dimension n and the parameter k. To see how k can matter, set the embed- 
of M(M+1)/2 nonlinear regressors. One can calculate heteroskedasticity- ding dimension n=l so that n-histories consist of single data points, and 
consistent standard errors so that the test becomes robust to the presence consider the case where the data are IID and uniformly distributed on the 
of nonlinearity in variance. unit interval (0, 1). In this case the fraction of data points that are within a 

distance k of a benchmark data point is 2k when the benchmark data point 
The Correlation Integral and tne Correlation Dimension is in the middle of the unit interval (between k and 1 -k), but it is smaller 
To distinguish a deterministic, chaotic process from a truly random process, when the benchmark data point lies near the edge of the unit interval. In 
it is essential to view the data in a sufficiently highdimensional form. In the the extreme case where the benchmark data point is zero or one, only a 
case of the tent map, for example, the data appear random if one plots xt fraction k of the other data points are within k of the benchmark. The gen- 
on the unit interval since xt has a uniform distribution. If one plots x, and eral formula for the fraction of data points that are close to a benchmark 
xt-1 on the unit square, however, the data will all fall on the tent-shaped line point b is min(k+b, 2k, k+l-6). As k shrinks, however, the complications 
shown in Figure 12.1. caused by this "edge problem" become negligible and the correlation inte- 

This straightforward approach can yield surprising insights, as we saw in gral approaches 2k. 
analyzing stock price discreteness in Chapter 3. However it becomes difficult Grassberger and Procaccia (1983) investigate the behavior of the cor- 
to implement when higher dimensions or more complicated nonlinearities relation integral as the distance measure k shrinks. They calculate the ratio 
are involved. Grassberger and Procaccia (1983) have suggested a formal of log Cn(k) to log k for small k: 
approach to capture this basic idea. Their approach begins by organizing 
the data (prefiltered, if desired, to remove linear structure) into n-histories un = lim log Cn(k) 
x :, defined by k+O logk 

X : = {~t-n+.l ,. . t  xth (12.1.18) which measures the proportional decrease in the fraction of points that are 
close to one another as we decrease the parameter that defines closeness. 

The parameter n is known as the embedding dimension. In the IID uniform case with n=l, the ratio log Cl(k)/log k approaches 
The next step is to calculate the fraction of pairs of n-histories that are log 2k/ log k=(log 2+ log k)/ log k= 1 as k shrinks. Thus vl =l for IID uni- 

"close" to one another. To measure closeness, we pick a number k and call form data; for small k, the fraction of points that are close to one another 
a pair of n-histories x : and X : close to one another if the greatest absolute shrinks at the same rate as k.  
difference between the corresponding members of the pair is smaller than 



478 12. Nonlinearities in Financial Data 12.2. Models of Changing Volatility 479 

Now consider the behavior of the correlation integral with higher em- To understand this result, note that the ratio Cn+l(k)/Cn(k)c an be inter- 
bedding dimensions n. When n=2, we are plotting 2-histories of the data preted as a conditional probability: 
on a 2-dimensional diagram such as Figure 12.1 and asking what fraction 
of the 2-histories lie within a square whose center is a benchmark 2-history 
and whose sides are of length 2k. With uniformly distributed IID data, a 
fraction 4k2 of the data points lie within such a square when the benchmark 
2-history is sufficiently far away from the edges of the unit square. Again 
we handle the edge problem by letting k shrink, and we find that the ratio 
log G(k)/ log k approaches log 4k2/ log k = (log 4+2 log k)/ log k = 2 as That is, C,+, (k)/ Cn(k)i s the probability that two data points are close, given 
k shrinks. Thus ~ = fo2r II D uniform data; for small k, the fraction of pairs that the previous n data points are close. If the data are IID, this must equal 
of points that are close to one another shrinks twice as fast as k. In general the unconditional probability that two data points are close, Cl (k). Setting 
vn=n for IID uniform data; for small k, the fraction of n-histories that are Cn+,( k)/ Cn(k)=Cl( k) for all positive n, we obtain (12.1.24). 
close to one another shrinks n times as fast as k. Brock, Dechert, and Scheinkman (1987) propose the BDS test statistic, 

The correlation integral behaves very differently when the data are gen- 
erated by a nonlinear deterministic process. To see this, consider data 
generated by the tent map. In one dimension, such data fall uniformly on 
the unit line so we again get vl =l .  But in two dimensions, all the data points where Cn,~(ka)n d Cl ,~(ka)r e the sample correlation integrals defined in 
fall on the tent-shaped line shown in Figure 12.4. For small k, the fraction (12.1.20), and Gn,j-(k)i s an estimator of the asymptotic standard deviation 
of pairs of points that are close to one another shrinks at the same rate as k of Cn,~ ( k ) C-  l, ~ ( k ) T~h.e  BDS statistic is asymptotically standard normal 
so ~ = 1 I.n higher dimensions a similar argument applies, and vn=l for all under the IID null hypothesis; it is applied and explained by Hsieh (1989) 
n when data are generated by the tent map. and Scheinkman and LeBaron (1989), who provide explicit expressions for 

The correlation dimension is defined to be the limit of v, as n increases, 6 ,~ ( k )H.  sieh (1989) and Hsieh (1991) report Monte Carlo results on the 
when this limit exists: size and power of the BDS statistic in finite samples. 

u = lim v,. (12.1.23) While there are some pathological nonlinear models for which Cn(k)= 
n+ LY) 

Cl (k), as in IID data, the BDS statistic appears to have good power against 
Nonlinear deterministic processes are characterized by finite v. the most commonly used nonlinear models. It is important to understand 

The contrast between nonlinear deterministic data and IID uniform that it has power against models that are nonlinear in variance but not in 
data generalizes to IID data with other distributions, since vn=n for IID mean, as well as models that are nonlinear in mean. Thus a BDS rejection 
data regardless of the distribution. The effect of the distribution averages does not necessarily imply that a time-series has a time-varying conditional 
out because we take each n-history in turn as a benchmark n-history when mean; it could simply be evidence for a time-varying conditional variance: 
calculating the correlation integral. Thus Grassberger and Procaccia (1983) Hsieh (1991), for example, strongly rejects the hypothesis that common 
suggest that one can distinguish nonlinear deterministic data from IID ran- stock returns are IID using the BDS test. He then estimates models of the 
dom data by calculating v, for different n and seeing whether it grows with time-varying conditional variance of returns and gets much weaker evidence 
n or converges to some fixed limit. This approach requires large amounts against the hypothesis that the residuals from such models are IID. 
of data since one must use very small k to calculate v, and no distribution 
theory is available for v,. 

12.2 Models of Changing Volatility 
The Brock-Dechert-Scheinkman Test 
Brock, Dechert, and Scheinkman (1987) have developed an alternative ap- In this section we consider alternative ways to model the changing volatil- 
proach that is better suited to the limited amounts of data typically available ity of a time-series ql+l. Section 12.2.1 presents univariate Autoregressive 
in economics and finance. They show that even when k is finite, if the data Conditionally Heteroskedastic (ARCH) and stochastic-volatilitym odels, and 
are IID then for any n Section 12.2.2 shows how these may be generalized to a multivariate setting. 

Cn(k) = C1(k)". (12.1.24) Section 12.2.3 covers models in which time-variation in the conditional mean 



12.2. Models of Changing Volatility 
480 12. Nonlinearities in Financial Data 481  

where the first equality follows from the independence ofa, and c , + ~a,n d the 
is linked to time-variation in the conditional variance; these models are non- inequality is implied by Jensen's Inequality. Intuitively, the unconditional 
linear in both mean and variance. distribution is a mixture of normal distributions, some with small variances 

In order to concentrate on volatility, we assume that qt+l is an innova- that concentrate mass around the mean and some with large variances that 
tion, that is, it has mean zero conditional on time t information. In a finance put mass in the tails of the distribution. Thus the mixed distribution has 
application, qt+l might be the innovation in an asset return. We define a: fatter tails than the normal. 
to be the time t conditional variance of qt+l or equivalently the conditional We now consider alternative ways of modeling and estimating the a: 
expectation of n:+l. We assume that conditional on time t information, the process. The literature on this subject is enormous, and so our review is 
innovation is normally distributed: inevitably selective. Bollenlev, Chou, and Kroner (19923, Bollenlev, Engle, 

and Nelson (1994), Hamilton (1994) provide much more comprehensive 
surveys. 

The unconditional variance of the innovation, a 2,  is just the unconditional 
expectation of 12.2.1 Univariate Models 

Early research on time-varying volatility extracted volatility estimates from 
asset return data before specifying a parametric time-series model for volatil- 

Thus variability of a: around its mean does not change the unconditional ity. Officer (l973),f or example, used a rolling standard deviation-the stan- 
variance a2. dard deviation of returns measured over a subsample which moves forward 

The variability of a; does, however, affect higher moments of the un- through time-to estimate volatility at each point in time. Other researchers 
conditional distribution of %+I.I n particular, with time-var/ing a; .t he. u n- have used the difference between the high and low prices on a given day to 
conditional distribution of qt+l has fatter tails than a normal distribution. estimate volatility for that day (Garman and Klass [1980], Parkinson [I9801)  . 
To show this, we first write: Such methods implicitly assume that volatility is constant over some interval 

of time. 
These methods are often quite accurate if the objective is simply to 

measure volatility at a point in time; as Merton (1980) observed, if an asset 
where el+l is an IID random variable with zero mean and unit variance (as price follows a diffusion with constant volatility, e.g., a geometric Brownian 
in the previous section) that is normally distributed (an assumption we did motion, volatility can be estimated arbitrarily accurately with an arbitrarily 
not make in the previous section). short sample period if one measures prices sufficiently frequently.4 Nelson 

As we discussed in Chapter 1, a useful measure of tail thickness for the (1992) has shown that a similar argument can be made even when volatility 
distribution of a random variable y is the normalized fourth moment, or changes through time, provided that the conditional distribution of returns 
kurtosis, defined by K(y) = E[y4]/E[y2I2.I t is well known that the kurtosis is not too fat-tailed and that volatility changes are sufficiently gradual. 
of a normal random variable is 3; hence K ( E ~ +=~ )3 . But for innovations It is, however, both logically inconsistent and statistically inefficient to 

use volatility measures that are based on the assumption of constantvolatility 
over some period when the resulting series moves through time. To handle 
this, more recent work specifies a parametric model for volatility first, and 
then uses the model to extract volatility estimates from the data on returns. 

ARCH Models 
A basic observation about asset return data is that large returns (of either 
sign) tend to be followed by more large returns (of either sign). In other 

4 ~ eSee ction 9.3.2 of Chapter 9. Note however that high-frequency price data are often 
severely affected by microstructure problems of the sort discussed in Chapter 3. This has 

'This result holds only because we are working with an innovation series that has a constant limited the usefulness of the high-low method of Garman and Klass (1980) and Parkinson 
(zero) conditional mean. For a series with a time-varying conditional mean, the unconditional ( 1980). 
variance is not the same as the unconditional expectation of the conditional variance. 



482 12. Nonlineam'ties in Financial Data 12.2. Models of Changing Volatility 483 

els. These write conditional variance as a distributed lag of past squared 
innovations: 

a: = w+a(~)rl : ,  (12.2.4) 
where a(L) is a polynomial in the lag operator. To keep the conditional 
variance positive, w and the coefficients in a(L) must be nonnegative. 

As a way to model persistent movements in volatility without estimat- 
ing a very large number of coefficients in a high-order polynomial a(L), 
Bollerslev (1986) suggested the Generalized Autoregressive Conditionally 
Heteroskedastic, or GARCH, model: 

where /?(L) is also a polynomial in the lag operator. By analogy with ARMA 
models, this is called a GARCH(P, q) model when the order of the polyno- 
mial /?(L)i s p and the order of the polynomial a(L) is q. The most commonly 
used model in the GARCH class is the simple GARCH(1,l) which can be 
written as 

Figure 12.2. Monthly Excess Log US Stock Returns, 1926 to 1994 

In the second equality in (12.2.6), the term (17;-a:,) has mean zero, con- 
ditional on time t-1 information, and can be thought of as the shock to 
volatility. The coefficient a measures the extent to which a volatility shock 

words, the volatility of asset returns appears to be serially correlated. This today feeds through into next period's volatility, while (a+/?) measures the 
can be seen visually in Figure 12.2, which plots monthly excess returns on rate at which this effect dies out over time. The third equality in (12.2.6) 
the CRSP value-weighted stock index over the period from 1926 to 1994. rewrites the volatility shock as aL1(c:-l), the square of a standard normal 
The individual monthly returns vary wildly, but they do so within a range less its mean-that is, a demeaned X2(1)r andom variable-multiplied by 
which itself changes slowly over time. The range for returns is very wide in past volatility aE1. 
the 1930s, for example, and much narrower in the 1950s and 1960s. The GARCH(1,l) model can also be written in terms of its implications 

An alternative way to understand this is to calculate serial correlation for squared innovations We have 
coefficients for squared excess returns or absolute excess returns. At 0.23 
and 0.21, respectively, the first-order serial correlation coefficients for these 
series are about twice as large as the first-order serial correlation coefficient 
for returns themselves, 0.11 , and are highly statistically significant since the This representation makes it clear that the GARCH(1,l) model is an 
standard error under the null of no serial correlation is 1/fi = 0.036. ARMA(1,l) model for squared innovyions; but a standard ARMA(1,l) 
The difference is even more dramatic in the average of the first 12 auto- model has homoskedastic shocks, while here the shocks (r$+,-a;) are them- 
correlation coefficients: 0.20 for squared excess returns, 0.21 for absolute selves heteroskedastic. 
excess returns, and 0.02 for excess returns themselves. This reflects the fact 

Persistence and Stationarity 
that the autocorrelations of squared and absolute returns die out only very 
slowly. In the GARCH(1,l) model it is easy to construct multiperiod forecasts of 

volatility. When a + B <1 , the unconditional variance of qt+l,o r equivalently 
To capture the serial correlation of volatility, Engle (1982) ~ r o ~ o s e d  

the unconditional expectation of a:, is w/(l-a-B). Recursively substitut- 
the class of Autoregressive Conditionally Heteroskedastic, or ARCH, mod- 



484 12. Nonlinearities in Financial Data 12.2. Models of Changing Volatility 485 

ing in (12.2.6), and using the law of iterated expectations, the conditional Alternatiue Functional Forms 
expectation of volatility j periods ahead is In the standard GARCH model, forecasts of future variance are linear in 

current and past variances and squared returns drive revisions in the fore- 
casts. An alternative model, sometimes known as the absolute value GARCH 
model, makes forecasts of future standard deviation linear in current and 
past standard deviations and has absolute values of returns driving revisions 

The multiperiod volatility forecast reverts to its unconditional mean at rate in the forecasts. An absolute value GARCH(1,l) model, for example, would 
(a + B). This relation between single-period and multiperiod forecasts is be 
the same as in a linear ARMA(1,l) model with autoregressive coefficient 
(a + B). Multiperiod forecasts can be constructed in a similar fashion for 
higher-order GARCH models. Schwert (1989) and Taylor (1986) estimate absolute value ARCH models, 

When a +,!?=I, the conditional expectation of volatility j periods ahead while Nelson and Foster (1994) discuss the absolute value GARCH (1,I ) .  
is instead The models we have considered so far are symmetric in that negative and 

Et[azj1 = a: + j~ (12.2.9) positive shocks et+l have the same effect on volatility. However Black (1976) 

The GARCH(1,l) model with a + B = 1 has a unit autoregressive root so and many others have pointed out that there appears to be an asymmetry 

that today's volatility affects forecasts of volatility into the indefinite future. in stock market data: Negative innovations to stock returns tend to increase 

It is therefore known as an integrated GARCH, or IGARCH(l,l), model. volatility more than positive innovations of the same magnitude. Possible 

The IGARCH(1,l) process for a: looks very much like a linear random explanations for this asymmetry are discussed in Section 12.2.3. To handle 
this, one can generalize the absolute value GARCH model to 

walk with drift w. However Nelson (1990) shows that this analogy must be 
treated with caution. A linear random walk is nonstationary in two senses. 
First, it has no stationary distribution, hence the process is not strictly station- 
ary. Second, it has no unconditional first or second moments, hence it is where 
not couariancestationary. In the IGARCH(1,l) model, on the other hand, a: f(et) = - bl - c(ct - 6). (12.2.12) 
is strictly stationary even though its stationary distribution generally lacks 
unconditional moments. Thus the IGARCH(1,l) model is strictly stationary Here the shift parameter b and the tilt parameter c measure two different 
but not generally covariance stationary. types of asymmetry. b is unrestricted but we need Icl 5 1 to ensure that 

It is particularly easy to show that the IGARCH(1,l) model has a sta- f (et)?O. When c=O but b#O, the effect of a shock on volatility depends on 
tionary distribution in the case where w=O. Here (12.2.9) simplifies to its distance from b, so that volatility increases more when there is no shock 
Et [az j ]= a:, so volatility is a martingale. At the same time, volatility remains than when there is a shock of size b. When b=O but c#O, a zero shock has 
bounded because it cannot go negative. But the martingale convergence the smallest impact on volatility but there is a distinction between positive 
theorem states that a bounded martingale must converge; in this case, the and negative shocks; a shock of given size may have a larger effect when it is 
only value to which it can converge is zero. The stationary distribution for negative than when it is positive, or vice versa. Following Hentschel(1995),a  
a: is then a degenerate distribution with point mass at zero, and this implies nice way to understand (12.2.12) is to plot f (et) against et,a s in Figure 12.3.~ 
that the stationary distribution for qt+l is also degenerate at zero. In this Panel (a) of the figure shows the absolute-value function (b=O, c=O); this is 
case the stationary distributions for a: and qt+l have moments, but they are plotted again as a dashed line in each of the other panels. Panel (b) shows 
all trivially zero. the shifted absolute-value function (b=0.5, c=O), panel (c) shows the tilted 

When w>O, Nelson (1990) shows that there exists a nondegenerate absolute-value function (b=O, c=0.25), and panel (d) shows a shifted and 
stationary distribution for a:. But this distribution does not have a finite tilted absolute-value function (b=0.5, c= - 0.25). 
mean or higher moments. The innovation qt+l then has a stationary distri- Hentschel(1995) further generalizes (12.2.1 1) to allowapower of f (el), 
bution with a zero mean, but with tails that are so thick that no second- or rather than f (e,) itself, to affect volatility, and to allow a power of at ,r ather 
higher-order moments exist.5 

"his is similar to the "news impact curve" of Pagan and Schwert (1990) and Engle and 
Ng (1993), which plots of against at, holding any other relevant state variables at their uncon- 

5 ~ e l s o nsh ows that these proB e rties hold more generally for GARCH(1,l) models with ditional means. 
(Y + fi r 1 but with E[log(j? + a r t  )] < 0. 



486 12. Nonlinean'ties in Financial Data 12.2. Models of Changing Volatility 487 

This model is appealing because it does not require any parameter restric- 
tions to ensure that the conditional variance of the return is always positive. 
Also it becomes both strictly nonstationary and covariance nonstationary 
when cu + p=1, so it does not share the unusual statistical properties of the 
IGARCH (1,l) model. On the other hand, multiperiod forecasts of future 
variances are harder to calculate in the EGARCH model; no closed-form 
expressions like (12 .2.8) are available. 

Estimation 
We have introduced an almost bewildering variety of volatility models. To 
discover which features of these models are important in fitting financial 
data, one must be able to estimate the models' parameters. Fortunately 
this is fairly straightforward for GARCH models and other models in the 
class defined by (12.2.13). Conditional on the parameters of the model and 
an initial variance estimate, the data are normally distributed and we can 
construct a likelihood function recursively. We write the vector of model 
parameters as 6, define at(6) to be the conditional standard deviation at 
time t implied by the parameters and the history of returns, and define 
et+l( 6) -- qt+l/al(6). When 6 contains the true parameters of the model, 
et+l( 6) is IID with density function g ( ~ ~(6+)l)  which we have assumed to be 
standard normal: 

Figure 22.3. Shifted and Tilted Absolute-Value Function 

The conditional log likelihood of qt+l is therefore 

than at itself, to be the variable that follows a linear difference equation. 
The resulting equation is 

where the last term is a Jacobian term that appears because we observe qt+l 
Equation (12.2.13) defines a family of models that includes most of the pop- and not qt+l/at(6).T he log likelihood of the whole data set 41, . . . , ~ J iTs  
ular GARCH-type models in the literature.' The standard GARCH model 
sets h=v=2, and b=c=O. Glosten, Jagannathan, and Runkle (1993) have 
generalized the standard GARCH model to allow nonzero c. Engle and Ng 
(1993) have instead allowed nonzero b. The absolute value GARCH model 
sets h=v=l with free band c. Another particularly important member of the The maximum likelihood estimator is the choice of parameters 6 that max- 
family (12.2.12) is the exponential GARCH or EGARCH model of Nelson imizes (12.2.17).~ 
(1990), which is obtained by setting h=O, v = l ,  and b=O to get 

'In practice one needs an initial o: to begin calculating the conditional likelihoods in 
(12.2.16). The influence of the initial condition diminishes over time and becomes negligi- 
ble asymptotically; thus the choice of initial condition does not affect the consistency of the 
estimator. 

7 ~ eaels o Ding, Granger, and Engle (199.7) for a related family of models. 



488 12. Nonlinearities in Financial Data 12.2. Models of Changing Volatility 489 

Although it is easy to show that the maximum likelihood estimator is One way to handle this problem is to continue to work with the con& 
consistent, it is harder to prove that it is asymptotically normal. The difficulty tional normal likelihood function defined by (12 .2.16) and (12.2.17 ), but 
is that this requires regularity conditions which are hard to verify for GARCH to interpret the estimator as a quasi-maximum likelihood estimator (White 
processes. Lee and Hansen (1994) give some results for the GARCH( 1,l) [I9821) . Standard errors for parameter estimates can then be calculated 
model but few other results are available. Empirical researchers typically using a robust covariance matrix estimator as discussed by Bollerslev and 
ignore this problem and assume that the usual regularity conditions hold. Wooldridge (1992). 
Some simulation evidence (Bollerslev and Wooldridge [I9921 and Lums- Alternatively, one can explicitly model the fat-tailed distribution of the 
daine [I9951)  supports this practice. shocks driving a GARCH process. Bollerslev (1987), for example, suggests 

Hentschel (1995) provides maximum likelihood estimates for a great a Student-t distribution with k degrees of freedom: 
variety of models in the family (12 .2.13) using daily and monthly stock return 
data over the period from 1926 to 1990. To estimate the parameters h and 
v with any precision, Hentschel finds that he needs the very large number 
of observations provided by daily data. These data suggest that h is close to (12.2.19) 
one (as in the absolute value GARCH model), but that v is greater than one, where r(.)i s the gamma function. The t distribution converges to the 
in fact close to 1.5. In both daily and monthly data, Hentschel finds that normal distribution as k increases, but has excess kurtosis; indeed its fourth 
asymmetry is better modeled with the shift parameter b than with the tilt moment is infinite when k 5 4. In a similar spirit Nelson (1991) uses 
parameter c. Thus US stock returns are welldescribed by a GARCH model a Generalized Error Distribution, while Engle and Gonzalez-Rivera (1991) 
for the conditional standard deviation, driven by the shifted absolute value estimate the error density nonparametrically. 
of shocks raised to the power three halves. The volatility process is highly GARCH models can also be estimated by Generalized Method of Mo- 
persistent in all the models estimated, although the degree of persistence is ments (GMM). This is appealing when the conditional volatility a,? can be 
sensitive to specification in the post-World War I1 period. written as a fairly simple function of observed past variables (past squared 

returns and additional variables such as interest rates). Then the model 
Additional Explanatory Variables implies that squared returns, less the appropriate function of the observed 
Up to this point we have modeled volatility using only the past history of variables, are orthogonal to the observed variables. GMM estimation has the 
returns themselves. It is straightforward to add other explanatory variables: usual attraction that one need not specify a density for shocks to returns. 
For example, one can write an augmented GARCH(1,l) model as 

Stochastic-Volatility Models 
Another response to the nonnormality of returns conditional upon past re- 
turns is to assume that there is a random variable conditional upon which 
returns are normal, but that this variable-which we may call stochastic volatil- 

where Xt is any variable known at time t. Provided that Xt10 and y20, this i t p i s  not directly observed. This kind of assumption is often made in 
model still constrains volatility to be positive. Alternatively, one can add continuous-time theoretical models, where asset prices follow diffusions with 
explanatory variables to the EGARCH model without any sign restrictions. volatility parameters that also follow diffusions. Melino and Turnbull (1990) 
Glosten,J agannathan, and Runkle (1993) add a short-term nominal interest and Wiggins (1987) argue that discrete-time stochastic-volatility models are 
rate to various GARCH models and show that it has a significant positive natural approximations to such processes. If we parametrize the discrete- 
effect on stock market volatility. time process for stochastic volatility, we then have a filtering problem: to pro- 

cess the observed data to estimate the parameters driving stochastic volatility 
Conditional Nonnormality and to estimate the level of volatility at each point in time. 
The GARCH models we have considered imply that the distribution of re- A simple example of a stochastic-volatility model is the following: 
turns, conditional on the past history of returns, is normal. Equivalently, 
the standardized residuals of these models, ~~+l(O)=r)~+l/as~h(oOu)ld,   be 
normal. Unfortunately, in practice there is excess kurtosis in the standard- 
ized residuals of GARCH models, albeit less than in the raw returns (see, where ct-N(O, a:), tt--N(O, a;), and we assume that ct and tt are serially 
for example, Bollerslev [I9871 and Nelson [1991]) . uncorrelated and independent of each other. Here at measures the dif- 



490 12. Nonlinearities in Financial Data 12.2. Models of Changing Volatility 
491  

ference between the conditional log standard deviation of' returns and its is ~ ' ( ~ + 1 ) ' / 2+  N(N+1)/2 which grows with the fourth power of N. ~t 

mean; it follows a zero-mean AR(1) process. is clear that this model becomes unmanageable very quickly; much of the 

We can rewrite this system by squaring the return equation and taking literature on multivariate GARCH models therefore seeks to place plausible 

logs to get restrictions on (12.2.22) to reduce the number of parameters. Another irn- 
portant goal of the literature is to find restrictions which guarantee that the 
covariance matrix El is positive definite. Such restrictions are comparatively 
straightforward in a univariate setting-for example, all the parameters in a 

This is in linear state-space form except that the first equation of (12.2.21) univariate GARCH(1,l) model must be positive-but are much less obvious 
has an error with a log X 2  distribution instead of a normal distribution. TO in a multivariate model. 
appreciate the importance of the nonnormality, one need only consider the Kroner and Ng (1993) provide a nice survey of the leading multivari- 
fact that when ct is very close to zero (an "inlier"), log(€:) is a very large ate GARCH models. A first specification, the WZCH model of Bollerslev, 
negative outlier. Engle, and Wooldridge (1988) (named after the vech operator), writes the 

The system can be estimated in a variety of ways. Melino and Turnbull covariance matrix as a set of univariate GARCH models. Each element of 
(1990) and Wiggins (1987) use GMM estimators. While this is straightfor- Xt  follows a univariate GARCH model driven by the corresponding element 
ward, it is not efficient. Harvey, Ruiz, and Shephard (1994) suggest a quasi- of the cross-product matrix qtr& The (i, j )  element of C ,  is given by 
maximum-likelihood estimator which ignores the nonnormality of log(€:) 
and proceeds as if both equations in (12.2.21) had normal error terms. 
More recently, Jacquier, Polson, and Rossi (1994) have suggested a Bayesian 
approach and Shephard and Kim (1994) have proposed a simulation-based This model is obtained from (12 .2.22) by making the matrices A and 9 diag- 
exact maximum-likelihood estimator. onal. The implied conditional covariance matrix is always positive definite if 

the matrices of parameters [wii], [Bii],a nd [ao]a re all positive definite. The 
12.2.2 Multivariate Models model has three parameters for each element of C t  and thus 3N(N+1)/2 

parameters in all. 
So far we have considered only the volatility of a single asset return. More A second specification, the BEKK model of Engle and Kroner (1995) 
generally, we may have a vector of asset returns whose conditional covari- (named after an earlier working paper by Bollerslev, Engle, Kraft, and 
ance matrix evolves through time. Suppose we have N assets with re- 
turn innovations ~ , ~ + il=,l . . . Kroner), guarantees positive definiteness by working with quadratic forms 

N. We stack these innovations into a 
rather than the individual elements of C,. The model is 

vector qt+l=[ m , t + ~ .  . . ) 7 ~ , t + 1 1 '  and define ~,, , t=Vart(qi,t+a~n)d  a,j,r= 
Covt( qi,t + l ,  q,t+ l);h ence C t=[ av,t ] is the conditional covariance matrix of 
all the returns. It is often convenient to stack the nonredundant elements C t  = C'C + B'Cl-1B + Atqt~'f i ,  (12.2.24) 
of Ct-those on and below the main diagonal-into a vector. The operator 
which performs this stacking is known as the vech operator: vech(Ct) is a where C is a lower triangular matrix with N(N + 1)/2 parameters, and B and 
vector with N(N+1)/2 elements. A are square matrices with N2 parameters each, for a total parameter count 

of ( 5 ~ ~ + ~ )W/ 2eak.   restrictions on B and A guarantee that C t  is always 
Multivariate GARCH Models positive definite. 
Many of the ideas we have considered in a univariate context translate nat- Aspecial case of the BEKKmodel is the single-factor GARCH(1,l) model 
urally to the multivariate setting. The simplest generalization of the uni- of Engle, Ng, and Rothschild (1990). In this model we define N-vectors 
variate GARCH(1, I )  model (12.2.6) relates vech(Cl) to vech(q,q;) and t~ and w and scalars a and B, and then have 
v e ~ h ( C ~ - ~ ) :  

Ct = C'C + XX' [~~W'X~+- ~a W( ~ ' q , ) ~ ] .  (12.2.25) 

Here C is restricted as in the previous equation. We can impose one nor- 
Here w is avector with N(N+1)/2 elements, and 9 and A are N(N+1)/2 x malizing restriction on this model; it is convenient to set ~ 'w=l ,w here L is 
N(N+1)/2 matrices; hence the total number of parameters in this model 



492 12. Nonlinearities in Financial Data 12.2. Models of Changing Volatility 493 

a vector of ones. The vector w can then be thought of as a vector of port- Multivariate Stochastic-Volatility Models 
folio weights. We define qpt=w'vat nd a ~ , ~ = d & Twh.e  model can now be The univariate stochastic-volatility model given in (12.2.19) is also easily 
restated as extended to a multivariate setting. We have 

where qt ,e l ,  at,a nd 6 ,  are now (Nx 1) vectors and 9 is an (NxN) matrix. 
This model has N~ parameters in the matrix 9, N(N+1)/2 parameters in 

The covariances of any two asset returns move through time only with the the covariance matrix of et,a nd N(N+1)/2 parameters in the covariance 

variance of the portfolio return, which follows a univariate GARCH(1,l) matrix of qt ,S O the total number of parameters is N(2N+1). There is no 

model. The single-factor GARCH(1,l) model is a special case of the BEKK need to restrict at to be positive and it is straightforward to estimate the E ,  

model where the matrices A and B have rank one: A = &wX' and B = and v,c ovariance parameters in square-root form to ensure that the implied 

,@wXf. It has ( ~ ~ + 5 N + 2 )f/r2ee  parameters. The model can be extended covariance matrix is positive definite. Harvey, Ruiz, and Shephard (1994) 
straightforwardly to allow for multiple factors or a higher-order GARCH suggest restricted versions of this model in which 9 is diagonal (reducing the 

structure. number of parameters to N(N+2)) or is even the identity matrix (further 
Finally, Bollerslev (1990) has proposed a constant-correlation model in reducing the number of parameters to N(N+l)). 

which each asset return variance follows a univariate GARCH(1,l) model Even without such extra restrictions, it is important to understand that 
and the covariance between any two assets is given by a constantcorrelation the specification (12 .2.28) imposes constant conditional correlations of asset 

coefficient multiplying the conditional standard deviations of the returns: returns. In this respect it is as restrictive as Bollerslev's (1990) constant- 
correlation GARCH model, and it has more parameters than that model 
whenever N>3. 

A Conditional Market Model 
Even the most restrictive of the models we have discussed so far are hard to 

This model has N(N+5)/2 parameters. It gives a positive definite covariance apply to a large cross-sectional data set because the number of their param- 

matrix provided that the correlations pq make up a well-defined correlation eters grows with the square of the number of assets N. The problem is that 

matrix and the parameters wii, aii, and Bii are all positive. these models take the whole conditional covariance matrix of returns as the 

To understand the differences between these models, it is instructive object to be studied. An alternative approach, paralleling the much earlier 

to consider what happens to the conditional covariance between two asset development of static mean-variance analysis, is to work with a conditional 

returns after large shocks of opposite signs hit the two assets. In the VECH market model. Continuing to ignore nonzero mean returns, we write 

model with a positive aq coefficient, the negative cross-product qitqjl lowers 
the conditional covariance. In the constant correlation model, on the other 
hand, the sign of the cross-product qitqjt is irrelevant; any event that increases 
the variances of two positively correlated assets raises the covariance between where Bi t  = aim,,/ a,,,, is the conditional beta of asset iwith the market, and 
them. In the factor ARCH model aq,, only moves with a@t,,  so the effect of <i,t+l is an idiosyncratic shock which is assumed to be uncorrelated across as- 
a negative cross-product qitqjt depends on the weights in portfolio p. sets. Within this framework we might model the conditional variance 

As in the univariate case, return volatilities may be persistent in multi- of the market return as a univariate GARCH(1,l) process; we might model 
variate GARCH models. Multivariate models allow for the possibility that Bim, t  or equivalently aim,at s depending on a,,,,, /? im, t - l ,  and the returns q i t  

some asset volatilities may share common persistent components; for ex- and q m t ;  and we might model the conditional variance of the idiosyncratic 
shock to return as another univariate GARCH (1,l) process. The covariance 

ample, there might be one persistent component in a set of volatilities, so 
matrix implied by a model of this sort is guaranteed to be positive definite, 

that all changes in one volatility relative to another are transitory. Bollerslev 
and the number of parameters in the model grows at rate N rather than N2, 

and Engle (1993) explore this idea, which is analogous to the concept of which makes the model applicable to much larger numbers of assets. Braun, 
cointegration in the literature on linear unit-root processes. 



494 12. Nonlinearities in Financial Data 12.2. Models of Changing Volatility 495 

Nelson, and Sunier (1995) take this approach, using EGARCH functional Campbell and Harvey assume that conditional expected returns are linear 
forms for the individual components of the model. in the instruments and define errors 

12.2.3 Links between First and Second Moments 

We have reviewed some extremely sophisticated models of time-varying sec- 
ond moments in time series whose first moments are assumed to be constant Here b, is a vector of regression coefficients of the market return on the 
and zero. But the essence of finance theory is that it relates the first and instruments. The error u,,~+I is the difference between the market return 
second moments of asset returns. Accordingly we now discuss models in and a linear combination of the instruments, while the error e,,,+l is the 
which conditional mean returns may change with the conditional variances difference between the market return and a linear function of u:,,+~. The 
and covariances. model (12.2.31) implies that the errors U,,~+I and emSt+alr e both orthogo- 

nal to the instruments H,. With L instruments, there are 2L orthogonality 

The GARCH-M Model conditions available to estimate L+2 parameters (yo, y1, and the L coeffi- 

Engle, Lilien, and Robins (1987) suggest adding a time-varying intercept to cients in b,). Thus GMM delivers both parameter estimates and a test for 

the basic univariate model (12.2.2). Writing rt+l for a continuously com- the overidentifying restrictions of the model. 

pounded asset return which is the time series of interest (since we no longer This approach can easily be generalized to include other assets whose 

work with a mean-zero innovation), we have expected returns are given by 

If there are N such assets, we define a vector rt+l = [r1,,+i., . . , r ~t+l, 1 '. 
where ctfli s an IID random variable as before, and 0; can follow any GARCH The conditional expectation of rt+1 is given by E [r,+i I Ht]  = HtB, where 
process. This GARCH-in-mean or GARCH-M model makes the conditional B is a matrix with NL coefficients. We define errors 
mean of the return linear in the conditional variance. It can be straightfor- 
wardly estimated by maximum likelihood, although it is not known whether 
the model satisfies the regularity conditions for asymptotic normality of the 
maximum likelihood estimator. 

The GARCH-M model can also be specified so that the conditional mean and we get 2NL extra orthogonality conditions to identify NL + 1 extra 
is linear in the conditional standard deviation rather than the conditional parameters. The total number of orthogonality conditions in (12.2.33) and 
variance. It has been generalized to a multivariate setting by Bollerslev, (12.2.34) is 2(N + l)L and the total number of parameters is N(L + I) + 
Engle, and Wooldridge (1988) and others, but the number of parameters L + 2. Thus the model is identified whenever two or more instruments are 
increases rapidly with the number of returns and the model is typically available. 
applied to only a few assets. Harvey (1989) further generalizes the model to allow for a time-varying 

price of risk. He replaces (12.2.33) by 
The Instrumental Variables Approach 
As an alternative to the GARCH-M model, Campbell (1987) and Harvey 
(1989, 1991) have suggested that one can estimate the parameters linking where ylt varies through time but is common to all assets. Since (12.2.35) 
first and second moments by GMM. These authors start with a model for holds for the market portfolio itself, 
the "market" return that makes the expected market return linear in its 
own variance, conditional on some vector H, containing L instruments or 
forecasting variables: 

and Harvey uses this to estimate the model. He substitutes (12.2.36) into 
(12.2.35), multiplies through by Var[rm,,+l I Ht],  and uses E[rm,t+(l Ht1 = 



496 12. Nonlinearities i n  Financial Data 12.2. Models of Changing Volatility 
497 

Wooldridge [1988], French, Schwert, and Stambaugh [1987], and Harvey 
Htb, and E[rt+i I Ht] = HtB to construct a new error vector 

[1989]), but other papers which use the short-term nominal interest rate as 
an instrument find a negative relationship between the mean and volatility 
of returns (see Campbell [I9871 and Glosten, Jagannathan, and Runkle 
[1993]). 

Harvey replaces et+]i n (12.2.34) with vt+l in (12.2.37), and drops the error As French, Schwert, and Stambaugh (1987) emphasize, there is much 
hqt+in ,( 12.2.32). This gives a system with L fewer orthogonality conditions stronger evidence that positive innovations to volatility are correlated with 

and one less parameter to estimate (since yl drops out of the model). The negative innovations to returns. We have already discussed how asymmetric 

number of overidentifylng restrictions declines by L - 1. Harvey (1989) GARCH models can fit this correlation. At a deeper level, it can be explained 

finds some evidence that the price of risk varies when a US stock index is in one of two ways. One possibility is that negative shocks to returns drive 

used as the market portfolio; however he also rejects the overidentifying up volatility. The leverage hypothesis, due originally to Black (1976),s ays that 

restrictions of the model. Harvey (1991) uses a world stock index as the when the total value of a levered firm falls, the value of its equity becomes 

market portfolio and obtains similar results. a smaller share of the total. Since equity bears the full risk of the firm, the 
percentage volatility of equity should rise. Even if a firm is not financially 

The Conditional CAPM and the Unconditional CAPM levered with debt, this may occur if the firm has fixed commitments to 
Equations (12.2.35) and (12.2.36) can be rewritten as workers or suppliers. Although there is surely some truth to this story, it is 

hard to account for the magnitude of the return-volatility correlation using 
realistic leverage estimates (see Christie [I9821 and Schwert [1989]). 

An alternative explanation is that causality runs the other way: Positive 
where Bit = C ~ v [ r ~ , ~rm+t3+i l,  I Ht]/Var[r,,t+l I Ht] ,  the conditional 

shocks to volatility drive down returns. Campbell and Hentschel (1992) call 
beta of asset i with the market return, and ht = E[T,,,+~ I Ht] - yo, the this the volatility-feedback hypothesis. If expected stock returns increase when 
expected excess return on the market over a riskless return. volatility increases, and if expected dividends are unchanged, then stock 

Jagannathan and Wang (1996) emphasize that this conditional version prices should fall when volatility increases. Campbell and Hentschel build 
of the CAPM need not imply the unconditional CAPM that was discussed in this into a formal model by using the loglinear approximation for returns 
Chapter 5. If we take unconditional expectations of (12.2.38), we get (7.2.26):  

rt+l = Et [rt+ll + qd, t+l  - qr,t+l, (12.2.40) 

Here E[ht] is the unconditional expected excess return on the market. 
E[Bit] is the unconditional expectation of the conditional beta, which need 
not be the same as the unconditional beta, although the difference is likely 
to be small. Most important, the covariance between the conditional beta is the change in expectations of future dividends in (7.2.25), and 
and the expected excess market return A t  appears in (12.2.39). Assets whose 
betas are high when the market risk premium is high will have higher un- 
conditional mean returns than would be predicted by the unconditional 
CAPM. Jagannathan and Wang (1996) argue that the high average returns 
on small stocks might be explained by this effect if small-stock betas tend to is the change in expectations of future returns. 
rise at times when the expected excess return on the stock market is high. Campbell and Hentschel model the dividend news variable qd.t+l as 
They present some indirect evidence for this story although they do not a GARCH(1,l) process with a zero mean: fld,t+l-N(O, a:), where a: = 
directly model the time-variation of small-stock betas. w + BaZ1+  a!rlzt.g They model the expected return as linear in the variance 

Volatility Innovations and Return Innouations ' ~ nfa ct they use a more general asymmetric model, the quadratic GARCH or QGARCH 
Empirical researchen have found little evidence that periods of high volatil- model of Sentana (1991). This is to allow the model to fit asymmetry in returns even in 
ity in stock returns are periods of high expected stock returns. Some pa- the absence of volatility feedback. However the basic idea is more simply illustrated using a 

standard GARCH model. 
pers report weak evidence for this relationship (see Bollerslev, Engle, and 



498 12. Nonlinearities in Financial Data 12.3. Nonparametric Estimation 499 

of dividend news: Et[rt+i] = yo + ylof. These assumptions imply that the Perhaps the most commonly used nonparametric estimators are smooth- 
revision in expectations of all future returns is a multiple of today's volatility ing estimators, in which observational errors are reduced by averaging the 
shock - of): data in sophisticated ways. Kernel regression, orthogonal series expansion, 

projection pursuit, nearest-neighbor estimators, average derivative estima- 
tors, splines, and artificial neural networks are all examples of smoothing. 
To understand the motivation for such averaging, suppose that we wish to 
estimate the relation between two variables Y ,  and Xt which satis5 

where 6'=ylpcr/(l-p(a + B)). The coefficient 6' is large when yl is large 
(for then expected returns move strongly with volatility), when a is large where m(.)i s an arbitrary fixed but unknown nonlinear function and 16,) is 

(for then shocks feed strongly into future volatility), and when a! + /3 is a zero-mean IID process. 

large (for then volatility shocks have persistent effects on expected returns). Consider estimating m(.) at a particular date for which Xb=q, and 
Substituting into (12.2.40), the implied process for returns is suppose that for this one observation Xb, we can obtain repeated indepen- 

dent observations of the variable Yb, say Yi =yl, . . . , Y,"=yn Then a natural 
estimator of the function m(.) at the point q is 

This is not a GARCH process, but a quadratic function of a GARCH process. 
It implies that returns are negatively skewed because a large negative realiza- 
tion of qd,t +l will be amplified by the quadratic term whereas a large positive 
realization will be damped by the quadratic term. The intuition is that any 
large shock of either sign raises expected future volatility and required re- 
turns, driving down the stock return today. Conversely, "no news is good 
news"; if ~ d , ~ + l = Oth is lowers expected future volatility and raises the stock and by the Law of Large Numbers, the second term in (12.3.3) becomes 
return today. Campbell and Hentschel find much stronger evidence for a negligible for large n. 
positive price of risk yl when they estimate the model (12.2.42) than when Of course, if {Y , )i s a time series, we do not have the luxury of repeated 
they simply estimate a standard GARCH-M model. Their results suggest that observations for a given Xt. However, if we assume that the function m(.) is 
both the volatility feedback effect and the leverage effect contribute to the sufficiently smooth, then for time-series observations Xt near the value q ,  
asymmetric behavior of stock market volatility. the corresponding values of Y,  should be close to m(q). In other words, if 

m(.) is sufficiently smooth, then in a small neighborhood around q , m (%) 
will be nearly constant and may be estimated by taking an average of the Yt's 

12.3 Nonparametric Estimation that correspond to those Xt's near q .  The closer the Xt's are to the value 
q , t he closer an average of corresponding X's will be to m(q). This argues 

In some financial applications we may be led to a functional relation between for a weighted average of the Y,'s, where the weights decline as the Xt's get 
two variables Y and X without the benefit of a structural model to restrict the farther away from q .  This weighted average procedure of estimating m(x) 
parametric form of the relation. In these situations, we can use nonparamet- is the essence of smoothing. More formally, for any arbitrary x, a smoothing 
ric estimation techniques to capture a wide variety of nonlinearities without estimator of m(x) may be expressed as 
recourse to any one particular specification of the nonlinear relation. In 
contrast to the relatively highly structured or parametric approach to esti- 1 

- ~ W ~ , T ( X ) & ~  (12.3.4) 
mating nonlinearities described in Sections 12.1 and 12.2, nonparametric T t= 1 
estimation requires few assumptions about the nature of the nonlinearities. where the weights { W ~ , T ( X ) )  are large for those x's paired with X,'s near x, 
However, this is not without cost-nonparametric estimation is highly data- and small for those Y,'s with X,'s far from x. 
intensive and is generally not effective for smaller sample sizes. Moreover, To implement such a procedure, we must define whatwe mean by "near" 
nonparametric estimation is especially prone to overJitting, a problem that and "far". If we choose too large a neighborhood around x to compute the 
cannot be easily overcome by statistical methods (see Section 12.5 below). 



500 12. Nonlinearities in Financial Data 12.3. Nonparametric Estimation 501  

average, the weighted average will be too smooth and will not exhibit the 
genuine nonlinearities of m(.) .  If we choose too small a neighborhood 
around x, the weighted average will be too variable, reflecting noise as well 
as the variations in m(.). Therefore, the weights { w ~ , ~ (mxu)s}t  be chosen 
carefully to balance these two considerations. We shall address this and 
other related issues explicitly in Sections 12.3.1 to 12.3.3 and Section 12.5. 

12.3.1 Kernel Regression 

An important smoothing technique for estimating m(.) is kernel regression. 
In the kernel regression model, the weight function wt ,~(xi)s  constructed 
from a probability density function K(x), also called a kernel: 

Despite the fact that K(x) is a probability density function, it plays no p rob  
abilistic role in the subsequent analysis-it is merely a convenient method 
for computing a weighted average, and does not imply, for example, that X Figure 22.4. Simulation of Y, = Sin(X,)+  0.56, 
is distributed according to K(x) (which would be a parametric assumption). 

By rescaling the kernel with respect to a variable h>O, we can change 
its spread by varying h if we define: 

1 Under certain regularity conditions on the shape of the kernel K and the 
Kh(u) = -K(u/ h), S Kh(u)du = 1. (12.3.6) magnitudes and behavior of the weights as the sample size grows, it may 

h be shown that ihh(x) converges to m(x) asymptotically in several ways (see 
Now we can define the weight function to be used in the weighted average Hiirdle [1990] for further details). This convergence property holds for 
(12.3.4) as a wide class of kernels, but for the remainder of this chapter and in our 

empirical examples we shall use the most popular choice of kernel, the 
Gaussian kernel: 

An Illustration of Kernel Regression 
If h is very small, the averaging will be done with respect to a rather small To illustrate the power of kernel regression in capturing nonlinear relations, 
neighborhood around each of the Xt's. If h is very large, the averaging will we apply this smoothing technique to an artificial dataset constructed by 
be over larger neighborhoods of the Xt's. Therefore, controlling the degree Monte Carlo simulation. Denote by {X,] a sequence of 500 observations 
of averaging amounts to adjusting the smoothing parameter h, also known which take on values between 0 and 2n at evenly spaced increments, and let 
as the bandwidth.1° Substituting (12.3.8) into (12.3.4) yields the Nadaraya- { Yt} be related to {X,] through the following nonlinear relation: 
Watson kernel estimator ihh(x) of m(x): 

where {ct)i s a sequence of IID pseudorandom standard normal variates. 
Using the simulated data {X, ,  Y , )  (see Figure 12.4), we shall attempt to es- 
timate the conditional expectation E[ Yt I Xtl = Sin(X,), using kernel 

" ' ~ h o o s i nth~e  appropriate bandwidth is discussed more fully in Section 12.3.2. 



502 12. Nonlinearities in Financial Data 

regression. To do this, we apply the Nadaraya-Watson estimator (12.3.9) 
with a Gaussian kernel to the data, and vary the bandwidth parameter h 
between 0.16, and 0.56, where 8, is the sample standard deviation of {X,]. 
By varying h in units of standard deviation, we are implicitly normalizing the 
explanatory variable X, by its own standard deviation, as (12.3.10) suggests. 

For each value of h, we plot the kernel estimator as a function of X,, and 
these plots are given in Figures 12.5a to 12.5~O. bserve that for a bandwidth 
of 0.18,, the kernel estimator is too choppy-the bandwidth is too small 
to provide sufficient local averaging to recover Sin(X,). While the kernel 
estimator does pick up the cyclical nature of the data, it is also picking up 
random variations due to noise, which may be eliminated by increasing the 
bandwidth and consequently widening the range of local averaging. 

Figure 12.5bs hows the kernel estimator for a larger bandwidth of O.36,, (a) h = 0.13, 

which is much smoother and a closer fit to the true conditional expectation. 
As the bandwidth is increased, the local averaging is performed over 

successively wider ranges, and the variability of the kernel estimator (as 
a function of x) is reduced. Figure 1 2 . 5 ~pl ots the kernel estimator with a 
bandwidth of O.5aXw, hich is too smooth since some of the genuine variation 
of the sine function has been eliminated along with the noise. In the limit, 
the kernel estimator approaches the sample average of { Y , } ,a nd all the 
variability of Y, as a function of Xt is lost. 

12.3.2 Optimal Bandwidth Selection 

It is apparent from the example in Section 12.3.1 that choosing the proper 
bandwidth is critical in any application of kernel regression. There are 
several methods for selecting an optimal bandwidth; the most common of (b) h = 0.3&, 

these is the method of cross-validation, popular because of its robustness and 
asymptotic optimality (see Hardle [1990, Chapter 51 for further details). 
In this approach, the bandwidth is chosen to minimize a weighted-average 
squared error of the kernel estimator. In particular, for a sample of T 
observations {X, ,  Y,}:::, let 

which is simply the kernel estimator based on the dataset with observation 
j deleted, evaluated at the jth observation 4. Then the cross-validation 
function CV(h)i s defined as 

1 = 
CV(h)  = - - &h,t(xt)l2a(xt), (12.3.13) (c )  h = 0.5ex 

T 
1=1 

where 6(X,) is a nonnegative weight function that is required to reduce Figure 12.5. Kernel F~timator 

boundary effects (see Hardle [1990, p. 1621 for further discussion). The 



504 12. Nonlinearities in Financial Data 12.3. Nonparametm'c Estimation 505 

function CV(h) is called the cross-validation function because it validates Average derivative estimators are based on the fact that the expectation 
the success of the kernel estimator in fitting { Yt) across the T subsamples of the derivative of m(.) with respect to the X,'s is proportional to 0 :  
(X,, Y,),+,, each with one observation omitted. The optimal bandwidth is 
the one that minimizes this function. 

12.3.3 Average Deriuatiue Estimators Therefore, an estimator of the average derivative is equivalent to an esti- 
mator of 0 up to a scale factor, and this scale factor is irrelevant for our 

For many financial applications, we wish to relate Y, to seueral variables purposes since it may be subsumed by m(.) and consistently estimated by 
XI,, . . . , Xkt nonparametrically. For example, we may wish to model the kernel regression. 
expected returns of stocks and bonds as a nonlinear function of several There are several average derivative estimators available: the direct, in- 
factors: the market return, interest rate spreads, dividend yield, etc. (see Lo direct, and slope estimators. Stoker (1991, Theorem 1) shows that they are 
and MacKinlay [I9961)  . Such a task is considerably more ambitious than the all asymptotically equivalent; however, Stoker (1992, Chapter 3) favors the 
univariate example of Section 12.3.1. To see why, consider the case of five indirect slope estimator (ISE) for two reasons. First, if the relation between Y, 
independent variables and, without loss of generality, let these five variables and X, is truly linear, the indirect slope estimator is still unbiased whereas 
all take on values in the interval [0, 11. Even if we divide the domain of the others are not. Second, the indirect slope estimator requires less pre- 
each variable into only ten equally spaced pieces, this would yield a total cision from its nonparametric component estimators because of the ISE's 
of 105=100,000 neighborhoods each of width 0.10; hence we would need ratio form (see below). 
at least 100,000 observations to ensure an average of just one data point Heuristically, the indirect slope estimator exploits the fact that the 
per neighborhood! This curse of dimensionality can only be solved by placing unknown parameter vector 0 is proportional to the covariance between 
restrictions on the kinds of nonlinearities that are allowable. the dependent variable Y and the negative of the derivative of the loga- 

For example, suppose a linear combination of the XZt'sis  related to Y, rithm of the marginal density of independent variables X,, denoted by I(.). 
nonparametrically. This has the advantage of capturing important non- Therefore, by estimating Cov[ Y, 1(.)], we obtain a consistent estimator of 
linearities while providing sufficient structure to permit estimation with p up to scale. This covariance may be estimated by computing the sample 
reasonable sample sizes. Specifically, consider the following multivariate covariance between Y and the sample counterpart to 1(.). 
nonlinear model: More formally, PISE may be viewed as an instrumental variables (IV) 

estimator (see Section A.l of the Appendix) of the regression of Y,  on Xt 
with the instrument matrix H: 

where Xt = [Xlt... Xkt]'i s now a (k x 1) vector and m(.) is some arbitrary 
but fixed nonlinear function. The function m(.) may be estimated by the 
following two-step procedure: (1) estimate P with an average derivative 
estimator 8;a nd (2) estimate m(.) with a kernel regression of Y, on x$. 

Stoker (1986) observes that the coefficients P of (12.3.14) may be esti- 
mated up to a scale factor by ordinary least squares if either of the following 
two conditions is true: (1) the Xt's are multivariate normal vectors; or, more 
generally, (2) E [Xi,I  Xi41 is linear in XiP for i = 1. . . . , k." If neither 
of these conditions holds, Stoker (1986) proposes an ingenious estimator, 
the average dmiuatiue estimator, which can estimate P consistently (see also 
Stoker [1992]). e derivative of the log of the marginal 

density of X,, and Ib(x) is an indicator function that trims a portion of the 
sample with estimated marginal densities lower than a fixed constant 6: 

 his second condition is satisfied by multivariate normal Xi's but is also satisfied for non- 
normal elliptically symmetric distributions. See Chamberlain (1983b),C hung and Goldberger 
( l984),D eaton and Irish ( l984),a nd Ruud (1983). 



506 12. Nonlinearities in Financial Data 12.3. Nonparamtric Estimation 507 

In most empirical applications, the constant b is set so that between 1 % and 12.3.4 Application: Estimating State-Price Densities 
5% of the sample is trimmed. One of the most important theoretical advances in the economics of invest- 

To obtain ?(.), observe that iff (x) denotes the marginal density of X, ,  ment under uncertainty is the time-state preference model of Arrow (1964) 
then the Gaussian kernel estimator off (x) is given by12 and Debrev (1959) in which they introduce primitive securities, each pay- 

ing $1 in one specific state of nature and nothing otherwise. Now known 
as Amow-Debreu securities, they are the fundamental building blocks from 
which we have derived much of our current understanding of economic 
equilibrium in an uncertain environment. 

where In practice, since true Arrow-Debreu securities are not yet traded on any 
organized exchange, Arrow-Debreu prices are not observable.13 However, 
using nonparametric techniques-specifically, multivariate kernel regres- 
sion-Ait-Sahalia and Lo (1996) develop estimators for such prices, known 
as a statepice density (SPD) in the continuous-state case. The SPD contains a 
wealth of information concerning the pricing (and hedging) of risky assets 
in an economy. In principle, it can be used to price other assets, even assets 
that are currently not traded (see Ait-Sahalia and Lo [1995] for examples) .I4 

Therefore, we have More importantly, SPDs contain much formation about preferences and 
asset price dynamics. For example, if parametric restrictions are imposed 
on the data-generating process of asset prices, the SPD estimator may be 
used to infer the preferences of the representative agent in an equilibrium 
model of asset prices (see, for example, Bick [I9901 and He and Leland 
[1993]). Alternatively, if specific preferences are imposed, the SPD estima- 
tor may be used to infer the data-generating process of asset prices (see, 
for example, Derman and Kani [1994], Dupire [1994], Jackwerth and Ru- 
binstein [1995], Longstaff [1992, 19941, Rady [1994], Rubinstein [1985], 

and we can define i(x) to be and Shimko [1991, 19931). Indeed, Rubinstein (1985) has observed that 
any two of the following implies the third: (1) the representative agent's 
preferences; (2) asset price dynamics; and (3) the SPD. 

Definition of the State-Price Density 
Despite the multivariate nature of f (.), observe that there is still only To define the SPD formally, consider a standard dynamic exchange economy 

a single bandwidth to adjust in the kernel estimator (12.3.19). As in the (see Chapter 8) in which the equilibrium price pt of a security at date t with 
univariate case, the bandwidth controls the degree of local averaging, but a single liquidating payoff Y(CT) at date T that is a function of aggregate 

now over multidimensional neighborhoods. As a practical matter, the nu- consumption CT is given by: 

merical properties of this local averaging procedure may be improved by 
normalizing all the XZt7bsy  their own standard deviations before computing 
f(.),a nd then multiplying each of the B,'s by the standard deviation of the 
corresponding Xit to undo the normalization. 

I3This may soon change with the advent of supershares, first proposed by Garman (1978) 
and Hakansson (1976,1977) and currently under development by Leland O'Brien Rubinstein 
Associates, Inc. See Mason, Merton, Ptrold, and Tufano (1995) for further details. 

I 2 ~ o tteh at the bandwidth h implicit in f ( x )  is, in general, different from the bandwidth of I40fc ourse, markets must be dynamically complete for such prices to be meaningful-see, 
the nonparametric estimator of m(.)i n (12.3.14). Cross-validation techniques may be used to for example, Constantinides (1982). This assumption is almost always adopted, either explicitly 
select both; however, this may be computationally too demanding and simple rules-of-thumb or implicitly, in parametric derivative pricing models, and we adopt it as well. 
may suffice. 



508 12. Nonlinearities in Financial Data 12.3. Nonparametric Estimation 509 

where Mt,T is the marginal rate of substitution between dates t and T, and prices of all financial securities-derivatives or not-are the prices of Arrow- 
6 is the rate of time preference. This well-known equilibrium asset-pricing Debreu securities, and these prices may be used to value all other securities, 
relation equates current price of the security to its expected discounted no matter how complex. 
future payoff, discounted using the stochastic discount factor. 

Lucas (1978) observes that (12.3.25) need not imply a martingale pro- Pricing Derivatives with SPDs 
cess for { P * )s,u pporting Leroy's (1973) contention that the martingale p rop  Under some regularity conditions, we may express f* as an explicit function 
erty is neither a necessary nor sufficient condition for rationally determined of t and T  so that a single SPD ~ * ( C Tt ,;  T )m ay be used to price an asset at 
asset prices. However, assuming that the conditional distribution of future any date t with a single liquidating payoff Y(CT)a t any future date T 2 t 
consumption has a density representation ft(.),t he conditional expectation (see footnote 16): 
in (12.3.25) can be re-expressed in the following way: 

and we shall adopt this convention for notational simplicity. For example, a 
European call option on date-T aggregate consumption CT with strike price 
X has a payoff function Y(CT) = max[CT - X, 01 and hence its date-t price 
G, is simply 

where 

Even the most complex path-independent derivative security can be priced 
and r t ,T is the continuously compounded net rate of return between t and and hedged according to (12.3.30). For example, consider a security with 
T of an asset promising one unit of consumption at T ; i .e., it is the return the highly nonlinear payoff function: 
on the riskless asset. 

This version of the Euler equation shows that an asset's current price can 
be expressed as its discounted expected payoff, discounted at the riskless rate 
of interest (see Chapter 8 for a more detailed discussion). However, the ex- 
pectation is taken with respect to the SPD f*,a  marginal-rate-of-substitution- 
weighted probability density function, not the original probability density 
function f of future consumption. In a continuous-time setting, f* is also 
known as the risk-neutral pricing density (Cox and Ross [1976]) or the equiv- 
alent martingale measure (Harrison and Kreps [I9791)  . l5 This payoff function is a smoothed version of the payoff to an option p o r t b  

Once f,* is obtained, it can be used to price any asset at date t with a single lio commonly known as bullish vertical spread, in which a call option with a low 
liquidating payoff at date T that is an arbitrary function of consumption strike is purchased and a call option with a high strike price is written (see 
cT.16 SPDs also provide the link between preference-based equilibrium Figure 12.5 and Cox and Rubinstein [1985, Chapter 11 for further details). 
models of the type discussed in Chapter 8 and arbitrage-based derivative 
pricing models of the type discussed in Chapter 9. Indeed, implicit in the 

Extracting SPDs from Derivatives Prices 
'"ee Huang and Litzenberger (1988, Chapter 5 )  for a more detailed discussion of SPDs. There is an even closer relation between option prices and SPDs than 
'%ecurities with multiple payoffs and infinite horizons can also be priced by the SPD, but (12.3.30) suggests, which Ross (i976),B anz and Miller (1978),a nd Breeden 

in these cases the SPD must be appropriately redefined to capture the time-variation in the and Litzenberger (1978) first discovered. In particular, they show that the 
marginal rates of substitution--see Breeden and Litzenberger (1978) and Radner (1982) for second derivative of the call-pricing function Gt with respect to the strike 
further discussion. 



510 12. Nonlinearities in Financial Data 12.3. Nonparametric Estimation 51 1 

and hence 

xy=l kb(P - Pi)khx(X-  Xi)k& ( r  - ri)kh,(rr - rr,)C, 
C(P, X, r ,  r,) = xr=kl hp(P-  Pt)khX(X-  xi)kh,( r  - ri)kh,(rr - rrc) ' 

(12.3.37) 
The option's delta and SPD estimator then follow by differentiating F: 

Figure 12.6. Bullish Vertical Spread PayoffFunctiona nd Smoothed Version 

price X must equal the SPD: 
Under standard regularity assumptions on the data-generating process 

as well as smoothness assumptions on the true call-pricing function, Kit- 
Sahalia and Lo (1996) show that the estimators of the option price, the 
option's delta, and the SPD are all consistent and asymptotically normal, 

Therefore, impounded in every option pricing formula is the SPD f*. and they provide explicit expressions for the asymptotic variances. 
To estimate the SPD using (12.3.34), we require a call option pric- Armed with the SPD, any derivative security with characteristic { and 

ing formula. Although many parametric pricing formulas exist (see Hull payoff function Y(5, PT)a t T=t+ t can now be priced at date t by the pricing 
[1993, Chapter 171 for some popular examples), fit-Sahalia and Lo (1996) function: 
construct a nonparametric pricing formula that places fewer restrictions- 
primarily smoothness and weak dependence--on the data-generating pro- 
cess of the underlying asset's price. While parametric formulas such as 
those of Black and Scholes (1973) and Merton (1973) offer great advan- If the payoff function Y(.) is twicedifferentiable in PT,t hen 
tages when the parametric assumptions (e.g., geometric Brownian motion) 
are satisfied, nonparametric methods are robust to violations of these as- 
sumptions. Since there is some empirical evidence that casts doubt on such 
assumptions, at least for stock indexes," the nonparametric approach may 
have some important advantages.18 

Given observed call option prices {Gi,X i,ti )( where ri = T, - t,), the 
prices of the underlying asset {Pi},a nd the riskless rate of interest {r,,},w e 
may construct the smooth nonparametric call-pricing function as 

Integrating against 6 instead of its second derivative speeds up the conver- 

using a multivariate kernel K, formed as a product of d=4 univariate kernels: gence rate of the estimator-e converges at speed n1I2h 4I2a nd its integral 
against a smooth function of PT converges at speed n'l2 h3I2, whereas the 
second derivative of 6 converges at n1I2h sI2 and its integral against a smooth 
function of PTa t n1I2h 6I2.A  factor of h3I2 is gained in the speed of conver- 
gence by integrating the second derivative of the payoff function-when it 

"see Lo and MacKinlay (1988). for example. exists-against C instead of integrating the payoff function itself against the 
''see Hutchinson, Lo, and Poggio (1994) and Ait-Sahalia (1996a) for other nonparametric second derivative of 6. 

option pricing alternatives. 



512 12. Nonlinearities in Financial Data 12.4. ArtiJicial Neural Networks 

Output 
Ait-Sahalia and Lo (1996) apply this estimator to the pricing and delta- 

hedging of S&P 500 call and put options using daily data obtained from t 
the Chicago Board Options Exchange for the sample period from January 
4, 1993 to December 31, 1993, yielding a total sample size of 14,431 obser- 
vations. The estimates of the SPDs exhibit negative skewness and excess 
kurtosis, a common feature of historical stock returns (see Chapter 1 for 
example). Also, unlike many parametric option pricing models, the SPD- 
generated option pricing formula is capable of capturing persistent volatility 
"smiles" and other empirical features of market prices. 

12.4 Artificial Neural Networks 

An alternative to nonparametric regression that has received much recent Figure 22.7. Binary Threshold Model 
attention in the engineering and business communities is the artijicial neural 
network. Artificial neural networks may be viewed as a nonparametric tech- 
nique, hence these models would fit quite naturally in Section 12.3. How- 
ever, because initially they drew their motivation from biological phenom- According to (12.4.1), each input X, is weighted by a coefficient bj, called 
ena-in particular, from the physiology of nerve cells-they have become the connection strength, and then summed across all inputs. If this weighted 
part of a separate, distinct, and burgeoning literature (see Hertz, Krogh, sum exceeds the threshold p,  then the artificial neuron is switched on or 
and Palmer [1991], Hutchinson, Lo, and Poggio [1994], Poggio and Girosi activated via the activation function g(.); otherwise it remains dormant. This 
[1990], and White [I9921 for overviews of this literature). simple network is often represented graphically as in Figure 12.7, in which 

To underscore the common nonparametric origins of artificial neural the input layer is said to be connected to the output layer. 
networks, we describe three kinds of networks in this section, collectively Generalizations of the binary threshold model form the basis of most 
known as karning networks (see Barron and Barron [I9881) . In Section current applications of artificial neural network models. In particular, 
12.4.1 we introduce the multilayer perceptron, perhaps the most popular to allow for continuous-valued outputs, the Heaviside activation function 
type of artificial neural network in the recent literature-thisi s what the term (12.4.2) is replaced by the logistic function (see Figure 12.8): 
"neural network" is usually taken to mean. In Sections 12.4.2 and 12.4.3 we 
present two other techniques that also have network interpretations: radial 
basis functions, and projection pursuit regression. 

12.4.1 Multilayer Perceptrons 

Perhaps the simplest example of an artificial neural network is the binary 
threshold model of McCulloch and Pitts (1943), in which an output variable Y 
taking on only the values zero and one is nonlinearly related to a collection 
of J input variables 4, j = 1, . . . , J in the following way: 

1 i f u > O  
g(u)  = 0 i f u  < 0. Figure 12.8. Comparison of Heavisih and hgis t ic  Activation Functions 



12. Nonlinearities in Financial Data 12.4. Artijicial Neural Networks 

Output In the terminology of this literature, the process of parameter estimation is 

t called training the network. This is less pretentious than it may appear to 
be-an early method of parameter estimation was backpropagation, and this 
does mimic a kind of learning behavior (albeit a very simplistic one) . I g  How- 
ever, White (1992) cites a number of practical disadvantages with backprop- 
agation (numerical instabilities, occasional non-convergence, etc.), hence 
the preferred method for estimating the parameters of (12.4.4) is nonlinear 
least-squares. 

Input Even the single hidden-layer MLP (12.4.4) possesses the universal ap- 
Layer proximation property: It can approximate any nonlinear function to an arbi- 

trary degree of accuracy with a suitable number of hidden units (see White 
[1992]). However, the universal approximation property is shared by many 
nonparametric estimation techniques, including the nonparametric regres- 

Figure 22.9. Multilayer Perceptmn with a Single Hidden Layer sion estimator of Section 12.3, and the techniques in Sections 12.4.2 and 
12.4.3. Of course, this tells us nothing about the performance of such tech- 
niques in practice, and for a given set of data it is possible for one technique 
to dominate another in accuracy and in other ways. 

Also, without loss of generality, we set to zero since it is always possible Perhaps the most important advantage of MLPs is their ability to approx- 
to model a nonzero activation level by defining the first input Xl=l in imate complex nonlinear relations through the composition of a network of 
which case the negative of that input's connection strength - P I  becomes relatively simple functions. This specification lends itself naturally to parallel 
the activation level. processing, and although there are currently no financial applications that 

But perhaps the most important extension of the binary threshold exploit this feature of MLPs, this may soon change as parallel-processing 
model is the introduction of a hidden layer between the input layer and software and hardware become more widely available. 
the output layer. Specifically, let To illustrate the MLP model, we apply it to the artificial dataset gen- 

erated by (12.3.11). For a network with one hidden layer and five hidden 
units, denoted by MLP(1,5), with 0(.)se t to the identity function, we obtain 
the following model: 

where h(.)  is another arbitrary nonlinear activation function. In this case, 
the inputs are connected to multiple hidden units, and at each hidden unit 
they are weighted (differently) and transformed by the (same) activation 
function g ( . ) .T he output of each hidden unit is then weighted yet again- 
this time by the ak's-and summed and transformed by a second activation 
function h( . ) .  Such a network configuration is an example of a multilayer 
perceptron (MLP)-a single (hidden) layer in this case-and is perhaps the 
most common type of artificial neural network among recent applications. where g ( u )  = 1/(1 + ePU) .T his model is plotted in Figure 12.10 and 
In contrast to Figure 12.7, the multilayer perceptron has a more complex compares well with the kernel estimator described in Section 12.3.1. Despite 
network topology (see Figure 12.9). This can be generalized in the obvious the fact that (12.4.5) looks nothing like the sine function, nevertheless the 
way by adding more hidden layers, hence the term multilayer perceptron. MLP performs quite well numerically and is relatively easy to estimate. 

For a given set of inputs and outputs { X t ,Y , ) ,  MLP approximation 
amounts to estimating the parameters of the MLP network-the vectors 
Pka nd scalars a h ,  k = l ,  . . . , K-typically by minimizing the sum of square2d 
deviations between the output and the network, i.e., x,[Yt-xk  'g~ackpropagationis  essentially the method of stochastic approximation first proposed by 

Robbins and Monro (1951a). See White (1992) for further details. 
akg(P6X)l . 



12.4. ArtiJicial Neural Networks 
516 12. Nonlinearities in Financial Data 517 

where 11 . 1) is some vector norm and D is a differential operator. The first 
term of the sum in (12.4.7) is simply the distance between m e , )  and the 
observation Y,, the second term is a penalty function that is a decreasing 
function of the smoothness of m(.),a nd h controls the tradeoff between 
smoothness and fit. 

In its most general form, and under certain conditions (see, for exam- 
ple, Poggio and Girosi [1990]), the solution to the minimization of (12.4.7) 
is given by the following expression: 

where (Uk] are d-dimensional vector centers (similar to the knots of spline 
functions), (Bk] are scalar coefficients, (&} are scalar functions, P(.) is a 
polynomial, and K is typically much less than the number of observations T 
in the sample. Such approximants have been termed hypedasisf unctions by 
Poggio and Girosi (1990) and are closely related to splines, smoothers such 
as kernel estimators, and other nonparametric  estimator^.^^ 

Figure 12.10. MLP(1,5) Model of Y ,  = Sin(Xt) + 0 . 5 ~ ~  For our current purposes, we shall take the vector norm to be a weighted 
Euclidean norm defined by a (d x d) weighting-matrix W, and we shall take 
the polynomial term to be just the linear and constant terms, yielding the 
following specification for &(.): 

12.4.2 Radial Basis Functions 

The class of radial basis functions (RBFs) were first used to solve interpolation 
problems-fitting a curve exactly through a set of points (see Powell [I9871 
for a review). More recently, RBFs have been extended by several researchers 
to perform the more general task of approximation (see Broomhead and where a0 and a1 are the coefficients of the polynomial P(.). Miccheli (1986) 

Lowe [1988], Moody and Darken [I9891, and Poggio and Girosi [1990])  . In shows that a large class of basis functions &(.) are appropriate, but the most 

particular, Poggio and Girosi (1990) show how RBFs can be derived from the common choices for basis functions are Gaussians e-"Iu2 and multiquadrics 
classical regularization problem in which some unknown function Y=m(X) dG7. 
is to be approximated given a sparse dataset (X,, Yt) and some smoothness Networks of this type can generate any real-valued output, but in appli- 
constraints. In terms of our multiple-regression analogy, the d-dimensional cations where we have some a priori knowledge of the range of the desired 
vector Xt are the explanatory variables, Yt the dependent variable, and 4 . 1  outputs, it is computationally more efficient to apply some nonlinear trans- 
the possibly nonlinear function that is the conditional expectation of Yt fer function to the outputs to reflect that knowledge. This will be the case 
given Xt, and hence in our application to derivative pricing models, inwhich some of the RBF 

networks will be augmented with an output sigmoid, which maps the range 
(-W, W )in to the fixed range (0,l).  In particular, the augmented network 
will be of the form g(&(x)) where g(u) = 1/(1+  e-"). 

The regularization (or nonparametric estimation) problem may then be with MLPs, RBF approximation for a given set of inputs and out- 
viewed as the minimization of the following objective functional: puts (Xt, K),  involves estifnating the parameters of the RBF network-the 

'"TO economize on terminohgy, here we use RBFs to encompass both the interpolation 
techniques used by Powell (1987) and their subsequent generalizations. 



518 12. Nonlinearities in Financial Data 12.4. ArtificialN eural Networks 519 

d(d+1)/2 unique entries of the matrix W' W, the dk elements of the centers 12.4.5 Application: Learning the Black-Scholes Formula 
{Uk)a, nd the d+k+ 1 coefficients ao, al, and {BkJ-typically by nonlinear Given the power and flexibility of neural networks to approximate complex 
least-squares, i.e., by minimizing Ct[ Yt - m(Xt ) ]  numerically. nonlinear relations, a natural application is to derivative securities whose 

pricing formulas are highly nonlinear even when they are available in closed 
form. In particular, Hutchinson, Lo, and Poggio (1994) pose the following 

12.4.3 Projection Pursuit Regression 
challenge: If option prices were truly determined by the Black-Scholes for- 

Projection pursuit is a method that emerged from the statistics community mula exactly, can neural networks "learn" the Black-Scholes formula? In 
for analyzing highdimensional datasets by looking at their low-dimensional more standard statistical jargon: Can the Black-Scholes formula be esti- 
projections. Friedman and Stuetzle (1981) developed a version for the mated nonparametrically via learning networks with a sufficient degree of 
nonlinear regression problem called projection pursuit regression (PPR). accuracy to be of practical use? 
Similar to MLPs, PPR models are composed of projections of the data, i.e., Hutchinson, Lo, and Poggio (1994) face this challenge by performing 
products of the data with estimated coefficients, but unlike MLPs they also Monte Carlo simulation experiments in which various neural networks are 
estimate the nonlinear combining functions from the data. Following the trained on artificially generated Black-Scholes option prices and then com- 
notation of Section 12.4.1, the formulation for PPR with a univariate output pared to the Black-Scholes formula both analytically and in out-of-sample 
can be written as hedging experiments to see how close they come. Even with training sets 

K of only six months of daily data, learning network pricing formulas can 
m(X,) = a0 + akmk(~;xt) (12.4.10) approximate the Black-Scholes formula with reasonable accuracy. 

k= 1 Specifically, they begin by simulating a tweyear sample of daily stock 
where the functions mk(.) are estimated from the data (typically with a prices, and creating a cross-section of options each day according to the 
smoother), the {ak}a nd (PkJa re coefficients, K is the number of projections, rules used by the Chicago Board Options Exchange with prices given by 
and the Black-Scholes formula. They refer to this two-year sample of stock and 

a0 is commonly taken to be the sample mean of the outputs m(Xt). The 
similarities between PPR, RBF, and MLP networks should be apparent from (multiple) option prices as a single trainingpath, since the network is trained 
(12.4.10). on this sample.21 Given a simulated training path {P(t)]o f daily stock prices, 

they construct a corresponding path of option prices according to the rules 
of the Chicago Board Options Exchange (CBOE) for introducing options 

12.4.4 Limitations of Learning Networks on stocks. 
A typical training path is shown in Figure 12.11. Because the options 

Despite the many advantages that learning networks possess for approx- 
generated for a particular sample path are a function of the (random) stock 

imating nonlinear functions, they have several important limitations. In 
price path, the size of this data matrix (in terms of number of options and 

particular, there are currently no widely accepted procedures for determin- 
total number of data points) varies across sample paths. For their training 

ing the network architecture in a given application, e.g., the number of 
set, the number of options per sample path range from 71 to 91, with an 

hidden layers, the number of hidden units, the specification of the activa- 
average of 81. The total number of data points range from 5,227 to 6,847, 

tion function(s), etc. Although some rules of thumb have emerged from 
with an average of 6,001. 

casual empirical observation, they are heuristic at best. 
The nonlinear models obtained from neural networks yield estimates 

Difficulties also arise in training the network. Typically, network param- 
eters are obtained by minimizing the sum of squared errors, but because 
of the nonlinearities inherent in these specifications, the objective function 2 1 ~ h eayss ume that the underlying asset for the simulation experiments is a typical NYSE 

stock, with an initial price P(0)o f  $50.00, an annual continuously compounded expected rate 
may not be globally convex and can have many local minima. of return P of lo%,  and an annual volatility a of 20%. Under the Black-Scholes assumption of 

Finally, traditional techniques of statistical inference such as significance a geometric Brownian motion, 
testing cannot always be applied to network models because of the nesting of dP = pPdt + aPdB, 

layers. For example, if one of the ak7isn  (12.4.4) is zero, then the connection and taking the number of days per year to be 253, they draw 506 pseudorandom variates {ctl 

strengths P k  of that hidden unit are unidentified. Therefore, even simple from the distribution N(w/253 ,a 2/253)  to obtain two years of daily continuously compounded 
returns, which are converted to prices with the usual relation P ( t )  = P(0)e x p [ x : = ,  € , I  for 

significance tests and confidence intervals require complex combinations t>O. 
of maintained hypotheses to be interpreted properly. 



12. Nonlinearities in  Financial Data  12.4. Artijicial Neural Networks 52 1 

A 

(a) Network call price G ~ X  (b) Network delta 

-P 

Sep89 Dec89 Mar90 Jun9O Sep9O 
Futures Contract 

Figure 12.11. Typical Simulated Training Path (see the text for parameters) 
Dashed line represents stock pice, while the a m w s  represent the options on the stock. The 
y-coordinate ofthe tip of the arrow indicates the strikepce ( a m w s  areslanted to make dfferent A 

introduction and expiration dates visible). (c) Call price error G ~ -X G / X  (d) Delta error % - % 
Figure 12.12. Typical Behavior of Four-Nonlinear-Term RBF Model 

of option prices and deltas that are difficult to distinguish visually from the 
true Black-Scholes values. An example of the estimates and errors for an 
RBF network is shown in Figure 12.12. The estimated equation for this 
particular RBF network is 

where t e T-t. Observe from (12.4.11) that the centers in the M3F 
model are not constrained to lie within the range of the inputs, and in fact 

* PIX - 1.35
= -o'oG/[ r - 0.45 1 ' 59.79 -0.03 

[-0.03 10.241 [ PIX - 1.35
r - 0.45 ]  + 2.55 do not in the third and fourth centers in this example. The largest errors 

in these networks tend to occur at the kink-point for options at the money 
at expiration, and also along the boundary of the sample points. 

While the accuracy of the learning network prices is obviously of great 
interest, this alone is not sufficient to ensure the practical relevance of the 
nonparametric approach. In particular, the ability to hedge an option posi- 
tion is as important, since the very existence of an arbitrage-based pricing 
formula is predicated on the ability to replicate the option through a dy- 
namic hedging strategy (see the discussion in Chapter 9). This additional 



522 12. Nonlineanties in Financial Data 12.5. OuerJitting and Data-Snooping 523 

constraint provides additional motivation for regularization techniques and, structed, that can provide real-time estimates of approximation errors in 
specifically, the RBF networks used by Hutchinson, Lo, and Poggio (1994). much the same way that standard errors may be obtained for typical statis- 

In particular, delta-hedging strategies require an accurate approxima- tical estimators. 
tion of the derivative of the underlying pricing formula, and the need for And finally, the need for better performance measures is clear. While 
accurate approximations of derivatives leads directly to the smoothness typical measures of goodness-of-fit such as R~ do offer some guidance for 
constraint imposed by regularization techniques such as RBF networks." model selection, they are only incomplete measures of performance. More- 
Hutchinson, Lo, and Poggio (1994) show that both RBF and MLP networks over, the notion of degrees of freedom is no longer well-defined for nonlin- 
provide excellent delta-hedging capabilities for the simulated Black-Scholes ear models, and this has implications for all statistical measures of fit. 
data as well as in an empirical application to S&P 500 futures options, in a 
few cases outperforming the Black-Scholes formula (recall that the formula 
is derived under the assumption that delta-hedging is performed continu- 12.5 Overfitting and DataSnooping 
ously, whereas these simulations assume daily delta-hedging).  

Although parametric derivative pricing formulas are preferred when While each of the nonlinear methods discussed in this chapter has its own 
they are available, the results of Hutchinson, Lo, and Poggio (1994) show costs and benefits, the problems of ouer$tting and data-snooping affect all of 
that nonparametric learning-network alternatives can be useful substitutes them to the same degree. Overfitting occurs when a model fits "too well," in 
when parametric methods fail. While their findings are promising, we can- the sense that the model has captured both random noise as well as genuine 
not yet conclude that such an approach will be successful in general-their nonlinearities. Heuristically, the primary source of overfitting is having too 
simulations have focused only on the Black-Scholes model, and their empir- few "degrees of freedom" or too many parameters relative to the number 
ical application consists of only a single instrument and time period, S&P of datapoints, and a typical symptom is an excellent in-sample fit but poor 
500 futures options for 1987 to 1991. out-of-sample p e r f ~ r m a n c eD. ~a~ta -snooping is a related problem that can 

However, this general approach points to a number of promising direc- lead to excellent but spurious out-of-sample performance. Data-snooping 
tions for future research. Perhaps the most pressing item on this agenda is biases arise when we ignore the fact that many specification searches have 
the specification of additional inputs, inputs that are not readily captured by been conducted to obtain the final specification of a model we are fitting 
parametric models, e.g., the return on the market, general market volatility, to the data.24 Even if a model is in fact incorrect, by searching long enough 
and other measures of business conditions. over various datasets and/or parameter values, we are likely to find some 

Other research directions are motivated by the need for proper statis- combination that will fit the data. However, this fit is spurious and is merely 
tical inference in the specification of learning networks. First, we require a symptom of our extensive search procedure. 
some method of matching the network architecture-number of nonlinear Unfortunately, there are no simple remedies to these two problems 
units, number of centers, type of basis functions, etc.-to the specific dataset since the procedures that give rise to them are the same procedures that 
at hand in some optimal and, preferably, automatic fashion. produce genuine empirical discoveries. The source of both problems is the 

Second, the relation between sample size and approximation error inability to perform controlled experiments and, consequently, the heavy 
should be explored, either analytically or through additional Monte Carlo reliance on statistical inference for our understanding of the data. As with 
simulation experiments. Perhaps some data-dependent metric can be con- all forms of statistical inference, there is always a margin of error, and this 

margin is often sensitive to small changes in the way we process the data and 
revise our models. 

2 2 ~fnac t, it is well known that the problem of numerical differentiation is ill-posed. The 
classical approach [Reinsch (1967)l is to regularize it by finding a sufficiently smooth function 
that solves the variational problem in (12.4.7). As we discussed earlier, RBF networks as well 
as splines and several forms of MLP networks follow directly from the regularization approach 

2 3 ~ hdee grees of freedom of a nonlinear model are often difficult to determine because 
and are therefore expected to approximate not only the pricing formula but also its derivatives 

the notion of a "parameter" may be blurred. For example, the kernel regression may seem 
(provided the basis function corresponding to a smoothness prior is of a sufficient degree, see 

to have only one free parameter-the bandwidth h-but this is clearly misleading since each 
Poggio and Girosi (1990): in particular, the Gaussian is certainly sufficiently smooth for our datapoint serves as a center for local averaging. See Hampel (1986) and Wahba (1990) for 
problem). A special case of this general argument is the result of Gallant and White (1992) and further discussion. 
Hornik, Stinchcombe, and White (1990) who show that single-hidden-layer MLP networks can 2 4 ~ eLee amer (1978) and Lo and MacKinlay (1990b) for formal analyses of such biases, and 
approximate the derivative of an arbitrary nonlinear mapping arbitrarily well as the number Black (1993) for a recent example in the finance literature. 
of hidden units increases. 



524 12. Nonlinearities in Financial Data 

Nevertheless, there are several ways to mitigate the effects of overfitting of nonlinearity can be readily identified or, at the very least, characterized in 
and data-snooping. For example, the impact of systematic specification some fashion. In such situations, the techniques described in this chapter 
searches may often be calculated explicitly, as in Lo and MacKinlay (1990b). are powerful additions to the armory of the financial econometrician. 
In such instances, using a corrected statistical distribution for inference will 
safeguard against finding significant results where none exist. Careful out- 
of-sample performance evaluation can uncover overfitting problems, and 
if relatively few out-of-sample tests are conducted, or if they are conducted 
over different (and weakly correlated) datasets, this will minimize the effects 12.1 Most pseudorandom number generators implemented on digital com- 
of data-snooping. puters are multiplicatiue linear congruential generators (MLCG),  in which Xn = 

But perhaps the most effective means of reducing the impact of over- + c) mod m, where a is some "well-chosen" multiplier, c is an optional 
fitting and data-snooping is to impose some discipline on the specification constant, and m is equal to or slightly smaller than the largest integer that 
search by a priori theoretical considerations. These considerations may be can be represented in one computer word. (For example, let a = 1664525, 
in the form of well-articulated mathematical models of economic behav- c = 0, and m = 232.) In contrast to MLCG numbers, consider the fol- 
ior, or behavioral models motivated by psychological phenomena, or simply lowing two nonlinear recursions: the tent map (see Section 12.1.1) and the 
heuristic rules of thumb based on judgment, intuition, and past experience. logistic map, respectively: 
While all of these sources are also affected by data-snooping and overfitting 
to some extent-no form of inference can escape these problems-they are 
less susceptible and offer a less datadependent means of model validation. 

All this suggests the need for an a pri& framework or specification 
for the model before confronting the data. By proposing such a specifica- 
tion, along with the kinds of phenomena one is seeking to capture and the 
relevant variables to be used in the search, the chance of coming upon a 
spuriously successful model is reduced. These recursions are examples of chaotic systems, which exhibit extreme 

sensitive dependence to initial conditions and unusually complex dynamic 
behavior. 

12.6 Conclusion 12.1.1 What are good properties for pseudorandom number generators 
to have, and how should you make comparisons between distinct gener- 

Nonlinearities are clearly playing a more prominent role in financial appli- ators in practice (not in theory)? 
cations, thanks to increases in computing power and the availability of large 
datasets. Unlike the material presented in earlier chapters, some of the 12.1.2 Perform various Monte Carlo simulations comparing MLCG to 
ideas in this chapter are less well-established and more tentative. Within a the tent and logistic maps to determine which is the better pseudorandom 
short time many of the techniques we have covered will be refined, and some number generator. Which is better and why? In deciding which criteria 
may become obsolete. Nevertheless, it is important to develop a sense of to use, think about the kinds of applications for which you will be using 
the direction of research and the open questions to be addressed, especially the pseudorandom number generators. Hint: Use 1.99999999 instead of 
at the early stages of these explorations. 2 in your implementation of (12.6.l ) ,a nd 3.99999999 instead of 4 in your 

Despite the flexibility of the nonlinear models we have considered, they implementation of (12.6.2)-for extra credit: Explain why. 
do have some serious limitations. They are typically more difficult to esti- 
mate precisely, more sensitive to outliers, numerically less stable, and more 12.2 Estimate a multilayer perceptron model for monthly returns on the 
prone to overfitting and data-snooping biases than comparable linear mod- S&P5 00 index from 1926:1  to 1994:1 2 using five lagged returns as inputs and 
els. Contrary to popular belief, nonlinear models require more economic one hidden layer with ten units. Calculate the in-sample root-mean-squared- 
structure and aprion considerations, not less. And their interpretation often error (RMSE) of the one-step-ahead forecast of your model and compare 
requires more effort and care. However, nonlinearities are often a fact of it to the corresponding out-of-sample results for the test period 1986:l to 

economic life, and for many financial applications the sources and nature 1994:1 2. Can you explain the differences in performance (if any) ? 



526 12. Nonlinearities in Financial Data 

12.3 Use kernel regression to estimate the relation between the monthly 
returns of IBM and the S&P 500 from 1965:l to 1994:12. How would a 
conventional beta be calculated from the results of the kernel estimator? 
Construct at least two measures that capture the incremental value of kernel Appendix 
estimation over ordinary least squares. 

THIS APPENDIX PROVIDES a brief introduction to the most commonly used es- 
timation techniques in financial econometrics. Many other good reference 
texts cover this material in more detail, hence we focus only on those aspects 
that are most relevant for our immediate purposes. Readers looking for a 
more systematic and comprehensive treatment should consult Hall (1992), 
Hamilton (l994), Ogaki (l992), and White (1984). 

We begin by following Hall's (1992) exposition of linear instrumen- 
tal variables (IV) estimation in Section A.l as an intuitive introduction to 
Hansen's (1982) Generalized Method of Moments (GMM) estimator. We 
develop the GMM method itself in Section A.2, and discuss methods for 
handling serially correlated and heteroskedastic errors in Section A.3. In 
Section A.4 we relate GMM to maximum likelihood (ML) estimation. 

A. 1 Linear Instrumental Variables 

Consider a linear relationship between a scalar yt and a vector xt: yt = 
x:8o + €t(OO)t, = 1 . . . T. Stacking the T observations, this can be written as 

where y is a ( T x1 ) vector containing T observations of y,, X is a ( T xN x) 
matrix containing T  observations of the Nx independent variables in xt, 8 0  
is an (Nxx 1) parameter vector, and €(go) is a ( T x 1 ) vector containing T 
observations of the error term 6,. The error term is written as a function of 
the true parameter vector so that the notation c can be used for both the true 
equation error and the residual of an estimated equation. For simplicity, 
assume that the error term is serially uncorrelated and homoskedastic, with 
variance a*;t hus the variance of ~ ( 8is~ V)ar [~(Bo)]=  a21Tw, here IT  is a 
( T xT ) identity matrix. 



528 Appendix A. 1. Linear Instrumental Variables 529 

There are also available NH instruments in an (NHx  1 )  column vector h,. minimization problem is 
The T observations of thisvector form a ( T xN H)m atrix H. The instruments 
have the property that E(h:ct(OOi)s)  an (NHx  1 )  vector of zeroes; that is, the XIHWTH'y = X ' H W T H ' X ~ ~ .  (A.1 .6) 
instruments are contemporaneously uncorrelated with the error el.' The 
statement that a particular instrument is uncorrelated with the equation When the number of instruments, NH,e quals the number of parameters to 
error is known as an orthogonality condition, and N regression uses the NH be estimated, Nx, and the matrix HX is nonsingular, then XIHWT cancels 
available orthogonality conditions to estimate the model. out of the left- and right-hand sides of (A.1.6)a nd the minimization gives 

Given an arbitrary coefficient vector 0 ,w e can form the corresponding 
residual ct (0)=  yt - xi0. Stacking this residual into a vector, we get ~ ( 0=)  
y - XO. We can also define an (NHx  1 )  column vector containing the cross- 
product of the instrument vector with the residual, This estimate is independent of the weighting matrix W T ,s ince with NH = 

Nx all the orthogonality conditions can be satisfied exactly and there is no 

f l ( 0 )  = htet(O). (A.1 .2) need to trade off one against another. It is easy to see that (A.1 .7) gives the 
usual formula for an OLS regression coefficient when the instruments H 
are the same as the explanatory variables X. 

The expectation of this cross-product is an (NHx  1 )  vector of zeroes at the More generally, NH may exceed Nx so that there are more orthogonality 
true parameter vector: conditions to be satisfied than parameters to be estimated. In this case the 

model is overidentified and the solution for e~ is 
The basic idea of N estimation is to choose coefficients to satisfy this condi- 
tion as closely as possible. Of course, we do not observe the true expectation 
off  and so we must work instead with the sample average off. We write this Asymptotic Distribution Theory 
as gT(0),u sing the subscript T to indicate dependence on the sample: The next step is to calculate the asymptotic distribution of the parameter 

estimate e ~Su.bs tituting in for y from ( A . l . l )a nd rearranging, we find that 

a(e-T 6,)  = ( T - ~ x ' H WT~-~ H'x)- 'T - ' X ' H W ~T -'I*H'E(o,,). 
(A.1.9) 

Now suppose that as T increases, T-'H'H converges to MHH, a non- 
Minimum Distance Criterion singular moment matrix, and T-'X'H converges to M X H ,a  moment matrix 
In general there may be more elements of gT(0)t han there are coefficients, of rank Nx. Suppose also that as T increases the weighting matrix W T  
and so in general it is not possible to set all the elements of gT(0)t o zero. converges to some symmetric, positive definite limit W .  Because we have as- 
Instead, we minimize a quadratic form, a weighted sum of squares and cross- , sumed that the error ~ ( 0 0is) s erially uncorrelated and homoskedastic, when 
products of the elements of gT(0).W e define the quadratic form QT(O)a s the orthogonality conditions hold T - ' / ~ H ' E (cOo~nv)e rges in distribution to 

a normal vector with mean zero andvariancecovariance matrix a2MHH. We 
use the notation S for this asymptotic variancecovariance matrix: 

where W Ti s an (NHx  NH)s ymmetric, positive definite weighting matrix. 
IV regression chooses eTa s the value of 0 that minimizes Q T ( 0 ) .S ub- 

stituting the definition of ~ ( 0in)to  (A.1.5)t,h e first-order condition for the Using (A.1.4),S  can be interpreted more generally as the asymptotic vari- 
ance of T ' / t~im es the sample average off, that is, T1I2t imes g ~ :  

'In many applications, t, is a forecast error that is uncorrelated with any variables known in 
advance. In this case the instrument vector h, will include only lagged variables that are known 
at time t - 1 or earlier. Nonetheless we write i t  ash, for notational simplicity and generality. 



A. I .  Linear Instrumental Variabb 
530 Appendix 531 

With these convergence assumptions, (A.1 .9) implies that exactly the same coefficient estimate (A.1.17) is implied. Note that under 
this alternative interpretation, the second-stage regression asymptotically 
has an R' statistic of unity because the error term in (A.1 .1 ) is orthogonal to 
the instruments and therefore has a fitted value of zero when projected on 

where the instruments. This implies that asymptotically, if (A.l.l) and the orthogu 
nality conditions hold, then the coefficient estimates should not depend on 
which variable is chosen to be the dependent variable in (A.1 .1 ) and which 
are chosen to be regressors. Asymptotically, the same coefficient estimates 
will be obtained (up to a normalization) whichever way the regression is 
written. 

and MHX=  MhI. The variancecovariance matrix of 2SLS coefficient estimates, V*, can 
be estimated by substituting consistent estimates of the various moment 

Optimal Weighting Matrix matrices into (A.l.15) to obtain 
We have now shown that the estimator eTi s consistent and asymptotically 
normal. The final step of the analysis is to pick a weighting matrix W that 
minimizes the asymptotic variance matrix V and hence delivers an asymp 
totically efficient estimator. It turns out that V is minimized by picking where 8' is a consistent estimate of the variance of the equation error. This 
W equal to any positive scalar times S-'. Recall that S is the asymptotic formula is valid for just-identified IV and OLS coefficient estimates as well. 
variance-covariance matrix of the sample average orthogonality conditions In place of the weighting matrix W* defined above, it is always possible 
gr(9). Intuitively, one wants to downweight noisy orthogonality conditions to use kW where k is any positive scalar. Similarly, in place of the weighting 
and place more weight on orthogonality conditions that are precisely mea- matrix WT one can use ~TWTw,h ere kT is any positive scalar that converges 
sured. Since here S-' = a-'M,L, it is convenient to set W equal to to k. This rescaling does not affect the formula for the instrumentalvariables 

estimator (A.1 .17). One possible choice for the scalar k is a-', the reciprocal 
of the variance of the equation error 6,; this makes the weighting matrix 
equal to S-I. The corresponding choice for the scalar kT is some consistent 

the formula for V then simplifies to 
estimate I?' of a-*. Hansen (1982) has shown that with this scaling, T times 
the minimized value of the objective function is asymptotically distributed 
x 'w ith (NH-  Nx) degrees of freedom under the null hypothesis that (A.1 . l )  

In practice one can choose a weighting matrix holds and the instruments are orthogonal to the equation error. 
Hansen's test of the null hypothesis is related to the intuition that under 

the null, the residual from the IV regression equation should be uncorre- 
lated with the instruments and a regression of the residual on the instru- 

As T increases, W; will converge to W*. ments should have a "small" R2 statistic. TO understand this, note that when 
With this weighting matrix the formula for e~ becomes WT = (8' T-'HIH)-I, the minimized objective function is 

[T-'E(~~)'H](~'T-'H'H)[-T'- 'H'E(~;)]. (A.1 .19) 

where x r H(HIH)-'H'x is the predicted value of X in a regression of X ^ * 
Now consider regressing the residual ~ ( 9on~ th)e  instruments H. The fitted 

on H. This is the well-known twustage least squares (2SLS) estimator. It value is H(H'H)-'H'E(~;), and the R~ statistic of the regression converges 
can be thought of as a two-stage procedure in which one first regresses X on to the same limit as (A.1  .l9). Thus Hansen's result implies that T times the 
H, then regresses y on the fitted value from the first stage to estimate the R2 in a regression of the residual on the instruments is asymptotically X 2  
parameter vector 8,. with (NH-  Nx) degrees of freedom. This is a standard test of overidentifying 

Alternatively, one can think of 2SLS as regressing both X and y on H in restrictions in two-stage least squares (Engle [1984]). 
the first stage, and then regressing the fitted value ofy on the fittedvalue ofX; 



Appendix A.2. Generalized Method of Moments 533 

A2 Generalized Method of Moments Asymptotic Distn'bution Theory 
The asymptotic distribution of the coefficient estimate eTi s 

The Generalized Method of Moments (Hansen [1982] )c an be understood 
as an extension of the linear IV regression we have discussed. Suppose now 
that we have a model which defines a vector et = E(x , ,O ), where xt now 
includes all the data relevant for the model (that is, we have dropped the where 

distinction between yt and x , ) ,  and 8  is a vector of Ng coefficients. This v = (D~WD~)-'D~WSWD~(D~WD~)-('A..2 .8) 
formulation generalizes the linear IV regression in three ways. First, ~ ( x 8, ,)  These expressions are directly analogous to (A .1 .12)a nd (A .1 .13)f or the lin- 
can be a column vector with N, elements rather than a scalar. Second, ear instrumental variables case. Do is a generalization of M H Xi n those equa- 
E(x, ,8 ) c an be a nonlinear rather than a linear function of the data and the tions and is defined by Do = E [ a f ( x , ,B o)/a801. D r ( 8 )c onverges asymptot- 
parameters. Third, E(x, ,8 ) c an be heteroskedastic and serially correlated ically to Do. S  is defined as in (A .1 . 1 1 )  by 
rather than homoskedastic white noise. Our model tells us only that there 
is some true set of parameters O0 for which ~ ( x00~)i, s o rthogonal to a set 
of instruments; as before these are written in an (NHx  1 )  column vector ht. 

By analogy with (A.1.2)w e define 
Optimal Weighting Matrix 
Just as in the linear IV case, the optimal weighting matrix that minimizes 
V is any positive scalar times S-'. With an optimal weighting matrix the 

The notation @ denotes the Kroneckerpoduct of the two vectors. That is, f r * 
is a vector containing the cross-product of each instrument in h with each asymptotic variance of T1I2t imes the coefficient estimate 8 ,  is 
element of E .  f is therefore a column vector with Nf = N,NH elements, and 
the model implies by analogy with (A.1 .3) that 

Also, when the weighting matrix S-' is used, T times the minimized objective 
function is distributed X 2  with (Nf - Ng) degrees of freedom, where Nf is the 
number of orthogonality conditions and Ng is the number of parameters to 

Just as in (A.1 .4) ,w e define a vector gT(8)c ontaining the sample aver- 
be estimated. 

ages corresponding to the elements off  in (A .1 .19):  
In practice, of course, S and the other quantities in (A.2.8) must be 

estimated. To do this, one starts with an arbitrary weighting matrix W T ;t his 
could be the identity matrix or could be chosen using some prior informa- 
tion about the relative variances of the different orthogonality conditions. 
Using W T ,o ne minimizes (A.2.4)t o get an initial consistent estimate 8 ~ .  

By analogy with (A.1 .5),G MM minimizes the quadratic form 
To estimate V in (A.2.8) ,o ne replaces its elements by consistent estimates. 
Do can be replaced by D T ( e T ) ,W  can be replaced by W T ,a nd S can be 
replaced by a consistent estimate ST(OT).G iven these estimates, one can 

Since the problem is now nonlinear, this minimization must be performed construct a new weighting matrix WT = sT(eT)-aln d minimize (A.2.4) 
numerically. The first-order condition is * 

again to get a second-stage estimate OT. The asymptotic variance of T'I2 
times the second-stage estimate can be estimated as 

where D T ( 8 )i s a matrix of partial derivatives defined by 
and the secondstage minimized objective function is distributed x 2  with 
(Nf - Ng) degrees of freedom. Although a two-stage procedure is asymp- 
totically efficient, it is also possible to iterate the procedure further until 

The ( i , j )  element of D T ( ~is )a gTi (8 ) /ap j .  



534 Appendix A.3. Serially Correlated and Heteroskedastic E m s  535 

the parameter estimates and minimized objective function converge. This of autocovariances; and to get a consistent estimator of S one cannot allow 
eliminates any dependence of the estimator on the initial weighting matrix, the number of estimated autocovariances to increase too rapidly with the 
and it appears to improve the finite-sample performance of GMM when the sample size. Second, there is no guarantee that an estimator of S formed by 
number of parameters is large (Ferson and Foerster [1994]) .  substituting (A.3.3)i nto (A.3.1)w ill be positive definite. To handle these 

two problems Newey and West (1987) suggested the following estimator: 

A.3 Serially Correlated and Heteroskedastic Errors 

One of the most important steps in implementing GMM estimation is esti- 
mating the matrix S. From (A.2.9),  where q increases with the sample size but not too rapidly. q-1 is the max- 

imum lag length that receives a nonzero weight in the Newey and West 
(1987) estimator. The estimator guarantees positive definiteness by down- 
weighting higher-order autocovariances, and it is consistent because the 
downweighting disappears asymptotically. 

In models where autocovariances are known to be zero beyond lag K- 1 ,  
it is tempting to use the Newey and West (1987) estimator with q = K. 
This is legitimate when K = l ,  so that only the variance r o , T ( e T )  appears in 

(A.3 .2) the estimator; but when K> 1 this approach can severely downweight some 
nonzero autocovariances; depending on the sample size, it may be better to 

is the jth autocovariance matrix of f t ( O O ) .  The matrix S is the variance- use q>K in this situation. 
covariance matrix of the time-average off , (Oo);  equivalently,i t is the spectral Although the Newey and West (1987) weighting scheme is the most 
density matrix of f t ( O O )a t frequency zero. It can be written as an infinite commonly used, there are several alternative estimators in the literature 
sum of autocovariance matrices of f t ( O o ) .  including those of Andrews (1991),A ndrews and Monahan (1992),a nd 

If the autocovariances of f t ( O O )a re zero beyond some lag, then one can Gallant (1987).H amilton (1994) provides a useful overview. 
simplify the formula (A.3.1)f or S by dropping the zero autocovariances. The 
autocovariances of f t ( O o )  are zero if the corresponding autocovariances of The Linear Instrumental Variables Case 
e(xt ,6 , )  are zero. In the linear lV case with serially uncorrelated errors The general formulas given here apply in both nonlinear and linear models, 
discussed earlier, for example, e(xt ,0 0)i s white noise and so f , ( O o )  is white but they can be understood more simply in linear IV regression models. 
noise; in this case one can drop all the autocovariances rjf or j > 0 and S Return to the linear model of Section A.l ,  but allow the error term ~ ~ ( 6 0 )  
is just ro,th e variance of f t ( O O ) .T he same result holds in the consumption to be serially correlated and heteroskedastic. Equation (A.l. lO)b ecomes 
CAPM with one-period returns studied in Chapter 8. However in regressions 
with K-period returns, like those studied in Chapter 7 , K - 1 autocovariances S = lim ~ a r [ T - ' / ~ ~ ' e (=f 3 ~lim) ]  T - ' H ' ~ ( B O ) H ,  (A.3.5) 

T+ m T-t  oo 
of f t ( O O )  are nonzero and the expression for S is correspondingly more 
complicated. where n(OOis)  the variancecovariance matrix of ~ ( 0 0 )T. h is can be esti- 

mated by 
The Newey-W est Estimator s T ( ~ T ) =  T - ~ H ' ~ ~ ~ ( ~ T ) H ,  (A.3.6) 
To estimate S it would seem natural to replace the true autocovariances of 
f (00) ,r l(OOw)it,h  sample autocovariances where n T ( e T )  is an estimator of f l ( O O ) .  Equation (A.2.11)n ow becomes 

T = (T-'x'H(H'o~( e T ) ~ ) - I ~ ' x ) - l .  (A.3 .7) 
~ ] , T ( Q T )  T-' f t ( e ~ f) t - , ( e ~ ) ' ~  (A.3.3) 

k]+l  In the homo~kedastic~whinteo ise case considered earlier, = a21T so 
we used an estimate O T ( @ T=)  G 2 1w~h ere G 2  = T-' 

and substitute into (A.3.1). However there are two difficulties that must c:(OT).S ubsti- 
tuting this into ( ~ . 3 . 7gi)v es (A.l .18) .  

be faced. First, in a finite sample one can estimate only a finite number 



536 Appendix A.4. GMM and Maximum Likelihood 537 

When the error term is serially uncorrelated but heteroskedastic, then x,+l conditional on the history of xt and a parameter vector 6 :  
C? is a diagonal matrix with different variances on each element of the main 
diagonal. One can construct a sample equivalent of each element as follows. Lt(x t+~6,  )  = L(x t+~I  X,,xt-l ,  ... , 0 ) .  (A.4.1) 
For each element on the main diagonal of the matrix, n T , t t ( e T ) = ~ t ( e T ) ' ,  We use the notation C t  for the log of L,, the conditional log likelihood: 
while each off-diagonal element = 0 for s#t .  Substituting the re- 
sulting matrix a T ( e T ) i nto (A.3.6)o ne gets a consistent estimator of  ST(^), Ct(xt+l, 8 )  = 1% Lt(xt+l,6 ) .  (A.4.2) 
and substituting it into (A.3.7)o ne gets a consistent estimator %. This is The log likelihood C of the whole data set x l ,  . . . , X T  is just the sum of the 
true even though the matrix a ( e ~is n)o t itself a consistent estimator of conditional log likelihoods: 
because the number of elements of Cl that must be estimated equals the 
sample size. 

When the error term is serially correlated and homoskedastic, then one 
can construct each element of the matrix f lT(eT)a s follows: Since Lt is a conditional density, it must integrate to 1: 

(A.3.8) Given certain regularity conditions, it follows that the partial derivative of 
where the Newey and West (1987) weighting scheme with maximum lag Lt with respect to 6 must integrate to zero. A series of simple manipulations 
length q is used. Alternatively, one can replace the triangular weights ( q  - then shows that 
l ) / qw ith unit weights to get the estimator of Hansen and Hodrick (1980) ,  
but this is not guaranteed to be positive definite. 

When the error term is serially correlated and heteroskedastic, then 
one can construct C ~ T ( ~as:T )  

The partial derivative of the conditional log likelihood with respect to 
where the Newey and West (1987) weighting scheme is used. Again one the parameter vector, a e , ( ~ , +€~J,) / Mi,s a vector with Ng elements. It is 
can replace the triangular weights with unit weights to get the estimator of known as the score vector. From (A.4.5),i t has conditional expectation zero 

Hansen and Hodrick (1980) .  In each case substituting O ( e T )i nto (A.3.6) when the data are generated by (A.4.1).I t also has unconditional expecta- 

gives a consistent estim,a. te of S, and substituting it into equation (A.3.7) tion zero and thus plays a role analogous to the vector of orthogonality con- 
g ives 

ditions f t  in GMM analysis. The sample average ( 1 /  T )z L1a l , ( x , + ~@,  / a 0  
a consistent estimator V*T,e ven though the matrix a(eTis) n ot itself a consis- 

plays a role analogous to gT(8)i n GMM analysis. 
tent estimator of fl because the number of nonzero elements increases too 

The maximum likelihood estimate of the parameters is just the solution 
rapidly with the sample size. White (1984)g ives a comprehensive treatment 
of the linear model with serially correlated and heteroskedastic errors. 6 to MaxC(8) = ELl e l .  The first-order condition for the maximization 

can be written as 
71 

A.4 GMM and Maximum Likelihood 

Following Hamilton (1994) ,w e now show how some well-known properties which also characterizes the GMM parameter estimate for a just-identified 
of Maximum Likelihood estimators (MLE) can be understood in relation model. Thus the MLE is the same as GMM based on the orthogonality 
to GMM. We first lay out some notation. We use L, to denote the density of conditions in (A.4.5).  



538 Appendix A. 4. GMM and Maximum Likelihood 539 

Asymptotic Distribution Theory This is known as the information-matrix equality, and implies that the expec- 
The asymptotic distribution of ML parameter estimates is given by the fol- tations to which the sample averages faa nd f b  converge are equal. The 
lowing result: information matrix equality holds under the assumption that the data are 

(A.4.7) generated by (A.4.1). 
GMM analysis gives an alternative formula for the distribution O ~ M L  

where Z given by: parameter estimates. Recall from (A.2.11) that the GMM estimator is asymp- 
totically normal with asymptotic variance estimator 

Z(0) = lim -E 
n+ 00 i.t, = (D7 

and is known as the information matrix. Z can be estimated by the sample In this case 
counterpart: 

The information matrix gives us a measure of the sensitivity of the value of while 
the likelihood is to the values of the parameters in the neighborhood of 
the maximum. If small changes in the parameters produce large changes 
in likelihood near the maximum, then the parameters can be precisely es- 

since the score vector is serially uncorrelated so S can be estimated from its 
timated. Since the likelihood function is flat at the maximum, the local 

sample variance. Therefore, the distribution of the GMM estimator can be 
sensitivity of the likelihood to the parameters is measured by the local cur- 

expressed as: 
vature (the second derivative) of likelihood with respect to the parameters, 
evaluated at the maximum. 

Infmation-Matrix Equality 
An alternative estimator of the information matrix, Zb, uses the average where Za and Zb are the limits of faa nd fba s T increases without bound, 
outer product or sample variance of the score vectors: evaluated at the true parameter vector 0,. 

When the model is correctly specified, faa nd f b  both converge to the 
information matrixZ, hence (A.4.14) simplifies in the limit to (22-'I)-' = 
Z-I which reduces to the conventional expression for the asymptotic vari- 
ance in (A.4.7). Therefore, either fao r fb( or both) can be used to estimate 

To see why Zb converges to the same limit as Z,, differentiate the third Z in this case. 
equality of equation (A.4.5) with respect to 8' to get However, when the model is misspecified, faa nd f b  converge to dif- 

ferent limits in general; this has been used as the basis for a specification 
test by White (1982). But ML estimates of the misspecified model are still 
consistent provided that the orthogonality conditions hold, and one can 
use the general variance formula (A.4.14) provided that the score vector is 
serially uncorrelated. White (1982) suggests this approach, which is known 
as quasi-maximum likelihood estimation. 

Hypothesis Testing 
The asymptotic variances in (A.4.7)a nd (A.4.14) can be used in a straight- 
forward manner to construct Wald tests of restrictions on the parameters. 



540 Appendix 

T h e  idea of such tests is to  see whether the unrestricted parameter estimates References 
are significantly different from their restricted values, where the  variance of 

Abel, A., 1990, "Asset Prices under Habit Formation and Catching Up with the 
the unrestricted estimates is calculated without imposing the restrictions. 

Joneses," in A m ' c a n  Economic Review 80, Papers and Proceedings, 38-42. 
Alternatively, o n e  may want to test restrictions using estimates only of 

the restricted model. Once  restrictions are imposed, the minimized GMM , 1996, "Risk Premia and Term Premia in General Equilibrium," unpublished 

objective function is n o  longer identically zero. Instead, the Hansen (1982) paper, University of Pennsylvania. 

result is that T times the minimized objective function has a X 2  distribution Abel, A., N. G. Mankiw, L. Summers, and R. Zeckhauser, 1989, "Assessing Dynamic 
with degrees of freedom equal to the number  of restrictions. In  this case T Efficiency: Theory and Evidence," Review ofEconomic Studies, 56, 1-20. 
times the minimized objective function is just Acharya, S., 1988, "A Generalized Econometric Model and Tests of a Signalling 

Hypothesis with Two Discrete Signals,"J ournal ofFinance, 43, 413-429. 
, 1993, "Value of Latent Information: Alternative Event Study Methods," 

Journal of Finance, 48, 363-385. 
which is the Lagrange multiplier test statistic for a restricted model estimated Adams, K., and D. van Deventer, 1994, "FittingYield Curves and Forward Rate Curves 
by maximum likelihood. with Maximum Smoothness," Journal ofFixed Income, 4, 52-62. 

Admati, A., and P. Pfleiderer, 1988, "A Theory of Intraday Patterns: Volume and 
The Delta Method Price Variability," Revim of Financial Studies, 1, 3-40. 
More complicated inferences for arbitrary nonlinear functions of the es- , 1989, "Divide and Conquer: A Theory of Intraday and Day-of-the-Week 
timator e may be performed via Taylor's Theorem o r  the delta method. If 
n(e- Mean Effects," Review of Financial Studies, 2, 189-224. 

 80) 2. N(0,V s),t hen a nonlinear function f (e)h as the following Affleck-Graves, J., and B. McDonald, 1989, "Nonnormalities and Tests of Asset Pric- 
asymptotic distribution: ing Theories," Journal of Finance, 44, 889-908. 

Affleck-Graves, J., S. Hegde, and R. Miller, 1994, "Trading Mechanisms and the 
Components of the Bid-Ask Spread," Journal ofFinance, 49, 1471-1488. 

Ainslie, G., 1992, Picoeconomics, Cambridge University Press, Cambridge. 
which follows from a first-order Taylor series approximation for f (0)a round Aitchison, J., and S. Silvey, 1957, "The Generalization of Probit Analysis to the Case 
80. Higher-order terms converge to  zero faster than 1 1 0 h ence only the of Multiple Responses," Biometrika, 44, 131-140. 
first term of the expansion matters for the asymptotic distribution o f f  (8) .  Ait-Sahalia, Y., 1993, "Nonparametric Functional Estimation with Applications to 

Financial Models," unpublished Ph.D. dissertation, Department of Eco- 
nomics, Massachusetts Institute of Technology. 

1996a," Nonparametric Pricing of Interest Rate Derivative Securities," Econe 
metrica, 64,527-560. 

, 199613, "Testing Continuous-Time Models of the Spot Interest Rate," Review 
ofFinancia1 Studies, 9, 385-426. 

Ait-Sahalia, Y , a nd A. Lo, 1996, "Nonparametric Estimation of State-Price Densi- 
ties Implicit in Financial Asset Prices", Working Paper LFE-1015-96, MIT 
Laboratory for Financial Engineering. 

Aiyagari, S., and M. Gertler, 1991, "Asset Returns with Transaction Costs and Unin- 
sured Individual Risk: A Stage I11 Exercise," Journal ofMonetary Economics, 
27,309-331. 

Aldous, D., 1989, Probability Appoximations via thePoisson ClumpingHeurktic, Springer- 
Verlag, New York. 

Aldous, D., and P. Diaconis, 1986, "Shuffling Cards and Stopping Times," American 
Mathematical Monthly, 8, 333-348. 



References 

Alexander, S., 1961, "Price Movements in Speculative Markets: Trends or Random Backus, D., A. Gregory, and S. Zin, 1989, "Risk Premiums in the Term Structure: 
Walks," Industrial Managemnt Review, 2, 7-26. Evidence from Artificial Economies,"J ournal ofMonetaq Economics, 24, 371- 

, 399. 
1964, "Price Movements in Speculative Markets: Trends or  Random Walks, 
No. 2," in P. Cootner (ed.), The Random Character of Stock Market Prices, Badrinath, S., Kale, J., and T. Noe, 1995, "Of Shepherds, Sheep, and the Cross- 
Massachusetts Institute of Technology Press, Cambridge, MA. Autocorrelations in Equity Returns," h i e w o f f i a n c i a  Studies, 8, 401430. 

Amihud, Y., and H. Mendelson, 1980, "Dealership Markets: Market Making with Bagehot, W., (a.k.a.J ack Treynor), 1971, "The Only Game in Town," FinancialAnalysts 
Uncertainty," Journal of Financial Economics, 8, 31 -54. Journal, 22, 12-14. 

, 1986, "Asset Pricing and the Bid-Ask Spread," Journal of Financial Economics, Baker, C., 1956, "Effective Stock Splits," Harvard Business Ratiew, 34(1), January- 
17,223-250. February, 101-106. 

,1987, "Trading Mechanisms and Stock Returns: An Empirical Investigation," , 1957, "Stock Splits in a Bull Market," Harvard Business Review, 35(3), May- 
Journal ofFinance, 42, 533-553. June, 72-79. 

Amin, K., and V. Ng, 1993, "Option Valuation with Systematic Stochastic Volatility," , 1958, "Evaluation of Stock Dividends," Harvard Business h i m , 3 6(4),J uly- 
Journal of Finance, 48, 881-910. August, 99-1 14. 

Anderson, T., 1984, A n  Introduction to Multivariate Statistical Analysis (2nd ed.),J ohn Bakshi, G., and Z. Chen, 1996, "The Spirit of Capitalism and Stock Market Prices," 
Wiley and Sons, New York. American Economic Review, 86,133-157. 

Andrews, D., 1991, "Heteroskedasticity and Autocorrelation Consistent Covariance Balduzzi, P., G. Bertola, and S. Foresi, 1993, "A Model of Target Changes and the 
Matrix Estimation," Econometrica, 59, 81  7-858. Term Structure of Interest Rates," Working Paper 4347, NBER, Cambridge, 

Andrews, D., and J. Monahan, 1992, "An Improved Heteroskedasticity and Autocor- MA. 
relation Consistent Covariance Matrix Estimator," Economtrica, 60, 953- Ball, C., 1988, "Estimation Bias Induced by Discrete Security Prices," Journal ofFi- 
966. nance, 43,841-865. 

Arnold, L., 1974, Stochastic D i f m t i a l  Equations: Themy and Applications, John Wiley Ball, C., and A. Roma, 1994, "Stochastic Volatility Option Pricing," Journal ofFinancia1 
and Sons, New York. and Quantitative Analysis, 29, 589-607. 

Arrow, K., 1964, "The Role of Securities in the Optimal Allocation of Risk Bearing," Ball, C., and W. Torous, 1983, "A Simplified Jump Process for Common Stock Re- 
Review ofEconomic Studies, 31,91-96. turns," Journal offinancial and Quantitative Analysis, 18, 53-65. 

Aschauer, D., 1985, "Fiscal Policy and Aggregate Demand," Amoican Economic Review, ,1985, "On Jumps in Common Stock Prices and Their Impact on Call Option 
75,117-127. 

Pricing," Journal offinance, 40, 155-1 74. 
Ashford, J., 1959, "An Approach to the Analysis of Data for Semi-Quanta1 Responses , 1988, "Investigating Security-Price Performance in the Presence of Event- 

in Biological Response," Biomtrics, 26,535-581. 
Date Uncertainty," Journal of financial Economics, 22, 123-154. 

Ashley, J., 1962, "Stock Prices and Changes in Earnings and Dividends: Some Em- 
Ball, R., and P. Brown, 1968, "An Empirical Evaluation of Accounting Income Num- 

pirical Results," Journal of Political Economy, 82-85. 
bers," Journal ofAccounting Research, 159-178. 

Asquith, P., and D. Mullins, 1986, "Equity Issues and Offering Dilution," Journal of 
Financial Economics, 15, 61-89. Ball, C., W. Torous, and A. Tschoegl, 1985, "The Degree of Price Resolution: The 

Case of the Gold Market," Journal ofFutures Markets, 5 ,2943 .  
Atchison, M., K. Butler, and R. Simonds, 1987, "Nonsynchronous Security Trading 

and Market Index Autocorrelation," Journal ofFinance, 42, 111 -1 18. Banz, R., 1981, "The Relation between Return and Marketvalue of Common Stocks," 
Journal of Financial Economics, 9, 3-18. 

Bachelier, L., 1900, "Theory of Speculation," in Cootner, P. (ed.), The Random Char- 
acterofStock Market Prices, Massachusetts Institute of Technology Press, Cam- Banz, R., and M. Miller, 1978, "Prices for State-Contingent Claims: Some Estimates 
bridge, MA, 1964; Reprint. and Applications," Journal of Business, 51, 655672. 

Backus, D., 1993, "Cox-Ingersoll-Ross in Discrete Time," unpublished paper, New Barberis, N., A. Shleifer, and R. Vishny, 1996, "A Model of Investor Sentiment with 
York University. both Underreaction and Overreaction," unpublished paper, University of 

Chicago and Harvard University. 
Backus, D., and S. Zin, 1994, "Reverse Engineering the Yield Curve," Working Paper 

4676, NBER, Cambridge, MA. Barclay, M., and R. Litzenberger, 1988, "Announcement Effects of New Equity Issues 
and the Use of Intraday Price Data," Journal ofFinancia1 Economics, 21, 71- 

Backus, D., S. Foresi, and S. Zin, 1996, "Arbitrage Opportunities in Arbitrage-Free 100. 
Models of Bond Pricing," Working Paper 5638, NBER, Cambridge, MA. 



544 References References 545 

Barr, D., and J. Campbell, 1995, "Inflation, Real Interest Rates, and the Bond Mar- Bierwag, G., G. Kaufman, and A. Toevs (eds.), 1983, Innovations in Bond Portfolio 
ket: A Study of UK Nominal and Index-Linked Government Bond Prices," Management: Duration Analysis and Immunization, JAI Press, Westport, CT. 
Discussion Paper 1732, Harvard Institute of Economic Research, Harvard Biggans, J., and C. Cannings, 1987, "Markov Renewal Processes, Counters and Re- 
University. peated Sequences in Markov Chains," Advances in Applied Probability, 19, 

Barron, A., 1993, "Universal Approximation Bounds for Superpositions of a Sig- 521-545. 
moidal Function," I- Trans. Info. Theory, 39,930-945. Billingsley, P., 1968, Convergence of Pmbability Measures, John Wiley and Sons, New 

, 1994, "Approximation and Estimation Bounds for Artificial Neural Net- York. 
works," Machine Learning, 14, 115-133. Black, F., 1971, "Toward a Fully Automated Stock Exchange," Financial Analysts Jour- 

Barron, A., and R. Barron, 1988, "Statistical Learning Networks: A Unifymg View," in nal, July-August, 2944.  
20th Symposium on the Interface: Computing Science and Statistics, pp. 192-203, , F., 1972, "Capital Market Equilibrium with Restricted Borrowing," Journal of 
Reston, Virginia. 

Business, 45, July, 444-454. 
Barsky, R., and J. De Long, 1993, "Why Does the Stock Market Fluctuate?," Quarterly 

Journal of Economics, 108, 291-31 1.  , 1976, "Studies of Stock Price Volatility Changes," in Pmceedings of the 1976 
Meetings of the Business and Economic Statistics Section, American Statistical As- 

Basu, S., 1977, "The Investment Performance of Common Stocks in Relation to sociation, pp. 177-181. 
Their Price to Earnings Ratios: A Test of the Efficient Market Hypothesis," 
Journal of Finance, 32,663-682. , 1993, "Return and Beta," Journal ofPmtfolio Management, 20,&18. 

Beckers, S., 1980, "The Constant Elasticity of Variance Model and Its Implications Black, F., and P. Karasinski, 1991, "Bond and Option Pricing when Short Rates Are 
for Option Pricing," Journal ofFinance, 35, 661-673. Lognormal," Financial Analysts Journal, July-August, 52-59. 

,1983, "Variances of Security Price Returns Based on High, Low, and Closing Black, F., and M. Scholes, 1972, "The Valuation of Option Contracts and a Test of 
Prices," Journal of Business, 56,97-112. Market Efficiency," Journal ofFinance, 27, 399-418. 

Benartzi, S., and R. Thaler, 1995, "Myopic Loss Aversion and the Equity Premium , 1973, "The Pricing of Options and Corporate Liabilities," Journal ofPolitical 
Puzzle," Quarterly Journal of Economics, 110, 73-92. Economy, 81,637-654. 

Benston, G., and R. Hagerman, 1974, "Determinants of Bid-Asked Spreads in the Black, F., E. Derman, and W. Toy, 1990, "A One-Factor Model of Interest Rates 
Over-the-Counter Market," Journal of Financial Economics, 1,353-364. and Its Application to Treasury Bond Options," Financial Analysts Journal, 

Benveneto, R., 1984, "The Occurrence of Sequence Patterns in Ergodic Markov January-February, 33-39. 
Chains," Stochastic Processes and Their Applications, 17, 369-373. Black, F., M. Jensen, and M. Scholes, 1972, "The Capital Asset Pricing Model: Some 

Bernard, V., 1987, "Cross-Sectional Dependence and Problems in Inference in Empirical Tests," in Jensen, M. (ed.), Studies in  the Themy ofCapital Markets, 
Market-Based Accounting Research," Journal of Accounting Research, 25, 1- Praeger, New York. 
48. Blanchard, O., and M. Watson, 1982, "Bubbles, Rational Expectations and Financial 

Berndt, E., B. Hall, R. Hall, and J. Hausman, 1974, "Estimation and Inference in Markets," in P. Wachtel (ed.), Crises in the Economic and Financial Structure: 
Nonlinear Structural Models," Annals ofEconomic and Social Measurement, 3, Bubbles, Bursts, and Shock, Lexington, Lexington, MA. 
653-665. Blattberg, R., and N. Gonedes, 1974, "A Comparison of Stable and Student Distribu- 

Bernstein, P., 1992, Capital Ideas: The Improbable Origins of Modern Wall Street, Free tions as Statistical Models for Stock Prices," Journal ofBusiness, 47,244-280. 
Press, New York. Blume, M., and I. Friend, 1973, "A New Look at the Capital Asset Pricing Model," 

Bertsimas, D., and A. Lo, 1996, "Optimal Control of Execution Costs," Working Journal of Finance, 28, 19-33. 
Paper LFE-1025-96, Massachusetts Institute of Technology Laboratory for 
Financial Engineering, Cambridge, MA. , 1978, The Changing Rob of the Individual Investm, John Wiley and Sons, New 

York. 
Bertsimas, D., L. Kogan, and A. Lo, 1996, "When Is Time Continuous?," Working 

Paper LFE-1045-96, Blume, M., and R. Stambaugh, 1983, "Biases in Computed Returns: An Application 
Massachusetts Institute of Technology Laboratory for 

Financial Engineering. to the Size Effect," Journal of Financial Economics, 12,387-404. 

Bhattacharya, M., 1983, "Transactions Data Tests of Efficiency of the Chicago Board Blume, L., D. Easley, and M. O'Hara, 1994, "Market Statistics and T&hnical Analysis: 

Options Exchange," Journal offinancial Economics, 12, 181-185. The Role of Volume," Jo,urnal of Finance, 49, 153-181. 

Bick, A., 1990, "On Viable Diffusion Price Processes of the Market ~ortfolio,J"o urnal Blume, M., C. MacKinlay, and B. Terker, 1989, "Order Imbalances and Stock Price 
of Finance, 45, 673-689. Movements on October 19 and 20, 1987,"J ournal ofFinance, 44,827448. 



References 

Boehmer, E., J. Musumeci, and A. Poulsen, 1991, "Event-Study Methodolog) under , 1988, "A Lattice Framework for Option Pricing with Two State Variables," 
Conditions of Event Induced Variance," Journal offinancial Economics, 30, Journal offinancial and Quantitative Ana<ysis, 1-12. 
25.3-272. Brainard, W., W. Nelson, and M. Shapiro, 1991, "The Consumption Beta Explains 

Boldrin, M., and M. Woodford, 1990, "Equilibrium Models Displaying Endogenous Expected Returns at  Long Horizons," unpublished paper, Yale University 
Fluctuations and Chaos: A Survey,",]ournal ofMonetar3, Economicc, 25, 189- and University of Michigan. 
222. Branch, B., and W. Freed, 1977, "Bid-Asked Spreads on the AMEX and the Big 

Bollerslev, T., 1986, "Generalized Autoregressive Conditional Heteroskedasticity," Board," Journal of Finance, 32, 159-1 63. 
Journal ofEconometrics, 31, 307-327. Braun, P., D. Nelson, and A. Sunier, 1995, "Good News, Bad News, Volatility, and 

, 1987, "A Conditional Heteroskedastic Time Series Model for Speculative Betas," Journal of Finance, 50, 1575-1603. 
Prices and Rates of Return," h i m o fEconomics and Statistics, 69, 542-547. Breeden, D., 1979, "An Intertemporal Asset Pricing Model with Stochastic Con- 

, 1990, "Modelling the Coherence in Short-Run Nominal Exchange Rates: A sumption and Investment Opportunities," Journal of AnancialEconomics, 7, 
Multivariate Generalized ARCH Approach," h i m o fEconomics and Statis- 265-296. 
tics, 72, 498-505. Breeden, D., and R. Litzenberger, 1978, "Prices of State-Contingent Claims Implicit 

3ollerslev, T., and R. Engle, 1993, "Common Persistence in Conditional Variances," in Option Prices," Journal $Businas, 51,621-651. 
Econometn'ca, 61, 1 66-1 87. Breeden, D., M. Gibbons, and  R. Litzenberger, 1989, "Empirical Tests of the 

3ollerslev. T., andJ . Wooldridge, 1992, "Quasi-Maximum Likelihood Estimation and Consumption-Oriented CAPM,"J ournal offinance, 44,231-262. 
Inference in Dynamic Models with Time Varying Covariances," Econometric 

Breen, W., and R. Korajczyk, 1993, "On Selection Biases in Book-to-Market Based 
Reoims, 1 1 ,  143-1 72. 

Tests of Asset Pricing Models," Working Paper 167, Northwestern Univer- 
3ollerslev, T., R. Chou, and K. Kroner, 1992, "ARCH Modelling in Finance: A Review sity, Evanston, IL. 

of the Theory and Empirical Evidence," Journal of Economrtncs, 5 2 , 5 4 9 .  
Brennan, M., 1979, "The Pricing of Contingent Claims in Discrete-Time Models," 

3ollerslev, T., R. Engle, and D. Nelson, 1994, "ARCH Models," in R. Engle and Journal of Finance, 34, 53-68. 
D. McFadden (eds.), Handbook ofki:ronometncs,V ol. IV, Elsevier, Amsterdam. 

Brennan, M., and T. Copeland, 1988, "Stock Splits, Stock Prices, and Transaction 
iollerslev, T., R. Engle, and J. Wooldridge, 1988, "A Capital Asset Pricing Model with Costs," Journal of Financial Economics, 22, 83-1 0 1. 

Time Varying Covariances," Journal of Political Gonomy, 96, 116-131. 
Brennan, M., and E. Schwartz, 1977a, "Convertible Bonds: Valuation and Optimal 

onomo,  Marco, and Renk Garcia, 1993, "Disappointment Aversion as a Solution Strategies for Call and Conversion," Journal ofFinance, 32, 1699-1715. 
to the Equity Premium and the Risk-Free Rate Puzzles," CRDE Discussion 
Paper 2793. University of Montreal. , 1977b, "The Valuation of American Put Options," Journal ofFznance, 32, 

449462.  
~ u d o u k hJ.,,  and M. Richardson, 1994, "The Statistics of Long-Horizon Regressions 

Revisited," Mathematical Finance, 4, 103-1 19. , 1978, "Finite Difference Methods and Jump Processes Arising in the Pricing 
of Contingent Claims: A Synthesis," Journal of Financial and Quantitative 

~ u d o u k hJ,. , Richardson, M., Stanton, R. and R. Whitelaw, 1995, "Pricing Mortgage- Analysis, 13, 461-474. 
Backed Securities in a Multifactor Interest Rate Environment: A Multivari- 
ate Density Estimation Approach," working paper, Stern School of Business, , 1979, "A Continuous-Time Approach to the Pricing of Bonds," Journal of 
New York University. Banking and finance, 3,133-155. 

udoukh, J., M. Richardson, and R. Whitelaw, 1994, "A Tale of Three Schools: Brennan, M., N.J egadeesh, and B. Swaminathan, 1993, "Investment Analysis and  the 
Insight? on Autocorrelations of Short-Horizon Stock Returns," Reuim of Adjustment of Stock Prices to Common Information," h i e w  of Financial 
Antmrial Studi~s7, ,  539-573. Studies, 6 ,  799-824. 

n, G., and D. Cox, 1964, "An Analysis of Transformations," Journal of the w ( i l  Brenner, R., R. H ar jes, and K. Kroner, 1996, "Another Look at Alternative Models of 
Statistical Society, Series B(26), 21 1-243. the Short-Term Interest Rate," Journal ofFinancia1 and QuantitativeAnalysis, 

31,85-107. 
r, G., and D. Pierce, 1970, "Distribution of Residual Autocorrelations in 

Autoregressive-Integrated Moving Average Time Series Models," Jor~rnal Brock, W., 1986, "Distinguishing Random and Deterministic Systems: Abridged 
of the Ammican Statistical A~sociation6, 5, 1509-1 526. Version," JournalofEconomic Theory, 40, 168-195. 

le, P.. 1977, "Options: A Monte Carlo Approach," Journal of Financial ~conomics, Brock, W., and C. Sayers, 1988, "Is T h e  Business Cycle Characterized By Deterministic 
4,323-338. Chaos?," Journal of Monetary Economics, 22, 71-90. 



References 

Brock, W., W. Dechert, andJ. Scheinkman, 1987, "ATest for Independence Based on , 1993b, "Why Long Horizons? A Study of Power against Persistent Alterna- 
the Correlation Dimension," unpublished paper, University of Wisconsin tives," unpublished paper, Princeton University. 
at Madison, University of Houston, and University of Chicago. , 1995, "Some Lessons from the Meld Curve," Journal ofEconomic Perspec:tives, 

Brock, W., J. Lakonishok, and B. LeBaron, 1992, "Simple Technical Trading Rules 9(3), 129-152. 
and the Stochastic Properties of Stock Returns,"J ournal ofFinance, 47,1731- , 1996a, "Understanding Risk and Return," Journal ofPolitica1 Economy, 104, 
1764. 298-345. 

Brodsky, B., 1993, Nonparametric Methods in Change-Point Problemr, Kluwer Academic , 1996b, "Consumption and the Stock Market: Interpreting International 
Publishers, Boston. Evidence," NBER Working Paper 5610, National Bureau of Economic Re- 

Bronfman, C., 1991, "From Trades to Orders on the NYSE: Pitfalls in Inference Us- search, Cambridge, MA. 
ing Transactions Data," working paper, Department of Finance and Real Campbell, J., and J. Arnmer, 1993, "What Moves the Stock and Bond Markets? A 
Estate, College of Business and Public Administration, University of Ari- Variance Decomposition for Long-Term Asset Returns," Journal of Finance, 
zona, Tucson, AZ. 48, 3-37. 

Broomhead, D., and D. Lowe, 1988, "Multivariable Functional Interpolation and Campbell, J., and J. Cochrane, 1995, "By Force of Habit: A Consumption-Based Ex- 
Adaptive Networks," Complex System, 2, 321-355. planation of Aggregate Stock Market Behavior," unpublished paper, Har- 

vard University and University of Chicago. 
Brown, S., and P. Dybvig, 1986, "The Empirical Implications of the Cox, Ingersoll, 

Ross Theory of the Term Structure of Interest Rates," Journal ofFinance, 41, Campbell,J ., and L. Hentschel, 1992, "No News Is Good News: An Asymmetric Model 
617-632. of Changing Volatility in Stock Returns," Journal ofFinancial Economics, 31, 

281-318. 
Brown, D., and R. Jennings, 1989, "On Technical Analysis," Review ofFinancial Studies, 

2,527-552. Campbell, J., and H. Koo, 1996, "A Comparison of Numerical and Analytical A p  
proximate Solutions to an Intertemporal Consumption Choice Problem," 

Brown, R., and S. Schaefer, 1991, "Interest Rate Volatility and the Term Structure," Journal of Economic Dynamics and Control, forthcoming. 
unpublished paper, London Business School. 

Campbell, J., and A. Kyle, 1993, "Smart Money, Noise Trading, and Stock Price 
, 1994, "The Term Structure of Real Interest Rates and the Cox, Ingersoll, Behavior," Review of Economic Studies, 60, 1-34. 

and Ross Model," Journal of Financial Economics, 35, 3-42. 
Campbell, J., and N. G. Mankiw, 1987, "Are Output Fluctuations Transitory?," Quar- 

Brown, S., and J. Warner, 1980, "Measuring Security Price Performance," Journal of terly Journal of Economics, 102, 857-880. 
Financial Economics, 8, 205-258. , 1990, "Permanent Income, Current Income, and Consumption," Journal of 

, 1985, "Using Daily Stock Returns: The Case of Event Studies," Journal of Business and Economic Statistics, 8, 265-278. 
Financial Economics, 14, 3-31. Campbell, J., and P. Perron, 1991, "Pitfalls and Opportunities: What Macroe- 

Brown, S., and M. Weinstein, 1985, "Derived Factors in Event Studies," Journal oJ conomists Should Know about Unit Roots," NBER Macroeconomics Annual, 
Financial Economics, 14, 491-495. 6,141-201. 

Brown, S., W. Goetzmann, and S. Ross, 1995, "Survival," Journal ofFinance, 50, 853- Campbell,J ., and R. Shiller, 1987, "Cointegration and Tests of Presentvalue Models," 
873. Journal of Political Economy, 95, 1062-1087. 

Burnside, C., 1994, "Hansen-Jagannathan Bounds as Classical Tests of Asset Pricing 1988a," The Dividend-Price Ratio and Expectations of Future Dividends and 
Models," Journal of Business and Economic Statistics, 12, 57-79. Discount Factors," Review ofFinancia1 Studies, 1, 195-227. 

Campbell, C., and C. Wasley, 1993, "Measuring Security Price Performance Using , 1988b, "Stock Prices, Earnings, and Expected Dividends,"J ournal ofFinance, 
Daily NASDAQ Returns," Journal of Financial Economics, 33, 73-92. 43,661-676. 

Campbell,J ., 1986, "Bond and Stock Returns in a Simple Exchange Model," Quarterly , 1991, 'Yleld Spreads and Interest Rate Movements: A Bird's Eye View," 
Journal of Economics, 101, 785-803. Review of Economic Studies, 58, 495-514. 

, 1987, "Stock Returns and the Term Structure," Journal ofFinancia1 Economics, , 1996, "A Scorecard for Indexed Government Debt," NBER Macroeconomics 
18,373-399. Annual, forthcoming. 

, 1991, "A Variance Decomposition for Stock Returns," EconomicJ ournal, 101, Campbell, J., S. Crossman, and J. Wang, 1993, "Trading Volume and Serial Correla- 
157-179. tion in Stock Returns," QuarterZyJournal oflclconomics, 108, 905-939. 

Carlstein, E., H. Muller, and D. Siegmund, eds., 1994, Change-PointProblas, Institute 
, 1993a, "Intertemporal Asset Pricing without Consumption Data," Ama'can of Mathematical Statistics, Hayward, CA. 

Economic Review, 83,487-51 2. 



References 55 1 

Carleton, W., and I. Cooper, 1976, "Estimation and Uses of the Term Structure of Chen, N., R. Rolf, and S. Ross, 1986, "Economic Forces and the Stock Market," 
Interest Rates," Journal of Finance, 31, 1067-1083. Journal of Business, 59, 383-403. 

Carverhill, A., 1988, "Numerical Methods in Pricing," Preprint 88/1, Financial O p  Chiras, D., and S. Manaster, 1978, "The Information Content of Option Prices and 

tions Research Centre, University of Warwick, UK. a Test of Market Efficiency," Journal offinancial Economics, 6, 213-234. 
Cho, D., and E. Frees, 1988, "Estimating the Volatility of Discrete Stock Prices," 

Carverhill, A,, and N. Webber, 1988, "American Options: Validation of Numerical 
Journal of Finance, 43, 451-466. 

Methods," Preprint 88/2, Financial Options Research Centre, University 
of Warwick, UK. Choi, J., D. Salandro, and K. Shastri, 1988, "On the Estimation of Bid-Ask Spreads: 

Theory and Evidence," Journal of Financial and Quantitative Analysis, 23, 
Cecchetti, S., P. Lam, and N. Mark, 1990, "Mean Reversion in Equilibrium Asset 219-230. 

Prices," American Economic Review, 80, 398-418. 
Christie, A., 1982, "The Stochastic Behavior of Common Stock Variances: Value, 

, 1994, "Testing Volatility Restrictions on Intertemporal Marginal Rates of Leverage, and Interest Rate Effects,"J ournal ofFinancialEconomics, 10,407- 
Substitution Implied by Euler Equations and Asset Returns," Journal of Fi- 432. 
nance, 49,123-152. Christie, W., and P. Schultz, 1994, "Why Do NASDAQ Market Makers Avoid Odd- 

Chamberlain, G., 1983a, "Funds, Factors, and Diversification in Arbitrage Pricing Eighth Quotes?,"J ournal ofFinance, 49, 1813-1840. 
Models," Econonzetn'ca, 51, 1305-1323. Christie, W., J. Harris, and P. Schultz, 1994, "Why Did NASDAQMarket Makers Stop 

, 1983b, "A Characterization of the Distributions that Imply Mean-Variance Avoiding Odd-Eighth Quotes?," Journal ofFinance, 49, 1841-1860. 
Utility Functions," Journal of Economic Theory, 29,1985-201. Chung, C., and A. Goldberger, 1984, "Proportional Projects in Limited Dependent 

Chamberlain, G., and M. Rothschild, 1983, "Arbitrage, Factor Structure, and Mean- Variables Models," Econometrica, 52, 531-534. 
Variance Analysis on Large Asset Markets," Econometnca, 51, 1281-1304. Chung, K., and R. Williams, 1990, Introduction to Stochastic Integration (2d ed.), 

Chan, K., 1988, "On the Contrarian Investment Strategy," Journal of Business, 61, Birkhsuser, Boston, MA. 
147-163. Clark, P., 1973, "A Subordinated Stochastic Process Model with Finite Variance for 

Chan, K., N. Chen, and D. Hsieh, 1985, "An Exploratory Investigation of the Firm Speculative Prices," Econometrica, 41, 135-156. 
Size Effect," Journal ofFinancialEconomics, 14, 451-472. Clewlow, L., 1990, "Finite Difference Techniques for One and Two Dimensional 

Chan, K., W. Christie, and P. Schultz, 1995, "Market Structure and the Intraday Option Valuation Problems," Preprint 90/10, Financial Options Research 

Evolution of Bid-Ask Spreads for NASDAQ Securities," Journal ofBusiness, Centre, University of Warwick, UK. 

68,35-60. Cochrane, J., 1988, "How Big Is the Random Walk in GNP?," Journal of Political 
Econon~y9, 6,893-920. 

Chan, K., G. Karolyi, F. Longstaff, and A. Sanders, 1992, "An Empirical Comparison 
of Alternative Models of the Short-Term Interest Rate," Journal of Finance, , 1991, "Volatility Tests and Efficient Markets: A Review Essay," Journal of 
47,1209-1227. Monetary Economics, 27,463-487. 

Cochrane, J., and L. Hansen, 1992, "Asset Pricing Explorations for Macroecc- 
Chan, L., and J. Lakonishok, 1993a, "Are the Reports of Beta's Death Premature?," 

Journal of Portjolio Management, 19, 51-62. nomics," in NBER Macroeconomics Annual 1992, Massachusetts Institute of 
Technology Press, Cambridge, MA, 115-165. 

, 1993b, "Institutional Trades and Intra-Day Stock Price Behavior," Journal of Cohen, K., G. Hawawini, S. Maier, R. Schwartz, and D. Whitcomb, 1983a, "Estimating 
Financial Economics, 33, 173-199. and Adjusting for the Intervalling-Effect Bias in Beta," Management Science, 

, 1995, "The Behavior of Stock Prices around Institutional Trades," Journal of 29, 135-148. 
Finance, 50, 1147-11 74. , 1983b, "Friction in the Trading Process and the Estimation of Systematic 

Chen, H., 1991a, "Estimation of a Projection-Pursuit Type Regression Model," Annals Risk," Journal ofFinancial Economics, 12, 263-278. 
of Statistics, 19, 142-157. Cohen, K., S. Maier, R. Schwartz, and D. Whitcomb, 1978, "The Returns Genera- 

Chen, N., 1983, "Some Empirical Tests of Arbitrage Pricing," Journal ofFinance, 38, tion Process, Returns Variance, and the Effect of Thinness in Securities 
1393-1414. Markets," Journal offinance, 33, 149-167. 

Chen, N., 1991b, "Financial Investment Opportunities and the Macroeconomy," , 1979, "On the Existence of Serial Correlation in an Efficient Securities 
Journal of Finance, 46, 529-554. Market," TZMS Studies in  the M a n a g e m t  Sciences, 11, 151-168. 

, 1981, "Transaction Costs, Order Placement Strategy and Existence of the 
Chen, N., and J. Ingersoll, 1983, "Exact Pricing in Linear Factor Models with Finitely Bid-Ask Spread," Journal of Political Economy, 89, 287-305. 

Many Assets," Journal of Finance, 38,985-988. 



552 References References 

, 1986, The Microstructure of Securities Markets, Prentice-Hall, Englewood Cliffs, Cox, J., 1975, "Notes on Option Pricing I: Constant Elasticity of Variance Diffu- 

NJ. sions," unpublished lecture notes, Graduate School of Business, Stanford 
University. 

Collins, D., and W. Dent, 1984, "A Comparison of Alternative Testing Methodologies 
Used In Capital Market Research," Journal of Accounting Research, 22,48444. Cox, J., and S. Ross, 1976, "The Valuation of Options for Alternative Stochastic 

Connor, G., 1984, "A Unified Beta Pricing Theory," Journal of Economic Theory, 34, Processes," Journal of Financial Economics, 3, 145-166. 
13-31. Cox, J., and M. Rubinstein, 1985, Options Markets, Prentice-Hall, Englewood Cliffs, 

Connor, G., and R. Korajczyk, 1986, "Performance Measurement with the Arbitrage New Jersey. 
Pricing Theory: A New Framework for Analysis," Journal ofFinancial Ece Cox,J ., J. Ingersoll, and S. Ross, 1981a, "A Reexamination of Traditional Hypotheses 
nomics, 15, 373-394. about the Term Structure of Interest Rates,"J ournal ofFinance, 36,769-799. 

, 1988, "Risk and Return in an Equilibrium APT: Application of a New Test , 1981b, "The Relation between Forward Prices and Futures Prices," Journal 
Methodology," Journal of Financial Economics, 21, 255-290. of Financial Economics, 9, 321 -346. 

,1993, "ATest for the Number of Factors in an Approximate Factor Structure," , 1985a, "A Theory of the Term Structure of Interest Rates," Econometnca, 53, 
Journal of Finance, 48, 1263-1 291. 385-408. 

Conrad, J., and G. Kaul, 1993, "Long-Term Market Overreaction or Biases in Com- , 1985b, "An Intertemporal General Equilibrium Model of Asset Prices," 
puted Returns?,"J ournal ofFinance, 48, 39-63. Econometrica, 53,363-384. 

Conrad, J., G. Kaul, and M. Nimalendran, 1991, "Components of Short-Horizon Cox, J., S. Ross, and M. Rubinstein, 1979, "Option Pricing: A Simplified Approach," 
Individual Security Return," Journal of Financial Economics, 29,365-384. Journal of Financial Economics, 7, 229-264. 

Constantinides, G., 1982, "Intertemporal Asset Pricing with Heterogeneous Con- Crack, T. and 0. Ledoit, 1996, "Robust Structure Without Predictability: The 'Com- 
sumers and without Demand Aggregation," Journal ofBusiness, 55,253-268. pass Rose' Pattern of the Stock Market," Journal ofFinance, 51, 751-762. 

,1986," Capital Market Equilibrium with Transaction Costs,"J ournalofPolitical Craig, S., J. Kohlase, and D. Papell, 1991, "Chaos Theory and Microeconomics: 
Economy, 94,842-862. An Application to Model Specification and Hedonic Estimation," Reoiew of 

, 1990, "Habit Formation: A Resolution of the Equity Premium Puzzle," Jour- Economics and Statistics, 73, 208-215. 
nal of Political Economy, 98, 519-543. Cumby, R., and D. Modest, 1987, "Testing for Market Timing Ability: A Framework 

, 1992, "A Theory of the Nominal Term Structure of Interest Rates," Review for Forecast Evaluation," Journal of Financial Economics, 19, 169-190. 
ofFinancia1 Studies, 5,531-552. Cutler, D., J. Poterba, and L. Summers, 1991, "Speculative Dynamics," Review of 

Constantinides, G., and D. Duffie, 1996, "Asset Pricing with Heterogeneous Con- Economic Studies, 58, 529-546. 
sumers," Journal of Political Economy, 104, 219-240. Cybenko, G., 1989, "Approximation by Superpositions of a Sigmoidal Function," 

Cootner, P. (ed.), 1964, The Random Character of Stock Market Prices, Massachusetts Mathematics ofC ontrol, Siguals, and System, 2, 303-314. 
Institute of Technology Press, Cambridge, MA. Dann, L., and C. James, 1982, "An Analysis ofthe Impact of Deposit Rate Ceilings on 

Copeland, T., and D. Galai, 1983, "Information Effects on the Bid-Ask Spread," the Market Values of Thrift Institutions,"J ournal ofFinance, 37,1259-1275. 
Journal of Finance, 38, 1457-1469. David, F., and D. Barton, 1962, Cmbinatorial Chance, Hafner, New York. 

Corrado, C., 1989, "A Nonparametric Test for Abnormal Security Price Performance Davies, R., and D. Harte, 1987, "Tests for Hurst Effect," Biometda, 74,95-101. 
in Event Studies," Journal of Financial Economics, 23, 385-395. 

Davis, D., and C. Holt, 1993, Experimental Economics, Princeton University Press, 
Courtadon, G., 1982, "A More Accurate Finite Difference Approximation for the 

Valuation of Options," Journal of Financial and Quantitative Analysis, 17, Princeton, NJ. 

697-703. Davis, M., and A. Norman, 1990, "Portfolio Selection with Transaction Costs," Math- 
ematics of Operatimu Research, 15, 676713. 

Cowles, A., 1933, "Can Stock Market Forecasters Forecast?," Econometnca, 1,309-324. 
, 1960, Day, R., 1983, "The Emergence of Chaos from Classical Economic Growth," Quarter4 

"A Revision of Previous Conclusions Regarding Stock Price Behavior," 
Econometnca, 28,909-915. Journal ofEconomics, 98, 201-214. 

Deaton, A., and M. Irish, 1984, "Statistical Models for Zero Expenditures in House- 
Cowles, A., and H. Jones, 1937, "Some A Posteriori Probabilities in Stock Market 

Action," Econometrica, 5, 280-294. hold Budgets,"Journal of Public Economics, 23, 59-80. 
DeBondt, W., and R. Thaler, 1985, "Does the Stock Market Overreact?," Journal of 

Cox, D., and H. Miller, 1965, The Theory of Stochastic Processes, Chapman and   all, 
London. Finance, 40, 793-805. 



554 References References 555 

, 1987, "Further Evidence on Investor Overreaction and Stock Market Sea- Dufour, J., 1981, "Rank Tests for Serial Dependence," Journal of Time Series Analysis, 
sonality,"J ournal of Finance, 42, 557-582. 2,117-128. 

Debreu, G., 1959, Theory of Value, John Wiley and Sons, NewYork. Dufour, J., and R. Roy, 1985, "Some Robust Exact Results on Sample Autocorrelations 
DeLong, B., A. Shleifer, L. Summers, and R. Waldmann, 1990a, "Positive Feedback and Tests of Randomness," Journal ofEconometrics, 29, 257-273. 

Investment Strategies and Destabilizing Speculation," Journal ofFinance, 45, Dunn, K., and K. Singleton, 1986, "Modelling the Term Structure of Interest Rates 
379-396. under Habit Formation and Durability of Goods," Journal of Financial E c ~  

, 1990b, "Noise Trader Risk in Financial Markets," Journal of Political Economy, nomics, 17, 27-55. 
98,703-738. Dupire, B., 1994, "Pricing with a Smile," RISK 7, January, 18-20. 

Demsetz, H., 1968, "The Cost of Transacting," Quarterly Journal of Economics, 82, Durlauf, S., and R. Hall, 1989, "Measuring Noise in Stock Prices," unpublished paper, 
33-53. Stanford University. 

Derman, E., and I. Kani, 1994, "Riding on the Smile," RISK, 7, February, 32-39. Durlauf, S., and P. Phillips, 1988, "Trends versus Random Walks in Time Series 
Dhrymes, P., I. Friend, B. Gultekin, and M. Gultekin, 1984, "ACritical Reexamination Analysis," Economtrica, 56, 1333-1357. 

of the Empirical Evidence on the Arbitrage Pricing Theory," Journal of Dybvig, P., 1985, "An Explicit Bound on Individual Assets' Deviations from APT 
Finance, 39, 323-346. Pricing in a Finite Economy," Journal ofFinancia1 Economics, 12, 483-496. 

Diaconis, P., 1988, Group Rep-esentatwns in  Probability and Statistics, Institute of Math- , 1989, "Bond and Bond Option Pricing Based on the Current Term Struc- 
ematical Statistics, Hayward, CA. ture," unpublished paper, Washington University. 

Diaconis, P., and M. Shahshahani, 1984, "On Nonlinear Functions of Linear Combi- Dybvig, P., and J. Ingersoll, 1982, "Mean-Variance Theory in Complete Markets," 
nations," SZAMJournal on Scientijic and Statistical Computing, 5(1),1 75-191. Journal of Business, 55, 233-252. 

Diamond, P., 1965, "National Debt in a Neoclassical Growth Model," Amaican Ece Dybvig, P., and S. Ross, 1985, 'Yes, The APT Is Testable," Journal of Finance, 40, 
nomic Review, 55,1126-11 50. 1173-1 188. 

Diba, B., and H. Grossman, 1988, "The Theory of Rational Bubbles in Stock Prices," Dybvig, P., J. Ingersoll, Jr., and S. Ross, 1996, "Long Forward and Zero-Coupon Rates 
Economic Journal, 98,746-757. Can Never Fall," Journal of Business, 69, 1-25. 

Dickey, D., and W. Fuller, 1979, "Distribution of the Estimators for Autoregressive Easley, D., and M. O'Hara, 1987, "Price, Trade Size, and Information in Securities 
Time Series with a Unit Root," Journal of the Ama'can Statistical Association, Markets," Journal of Financial Economics, 19, 69-90. 
74,427-431. ,1992, "Time and the Process of Security Price Adjustment,"J ournalofFinance, 

Dimson, E., 1979, "Risk Measurement When Shares Are Subject to Infrequent Trad- 47,577-605. 
ing," Journal of Financial Economics, 7, 197-226. 

Eberlein, E., and M. Taqqu, 1986, Dependence in  Probability and Statistics: A Survqr 
Ding, Z., C. Granger, and R. Engle, 1993, "A Long Memory Property of Stock Returns of Recent Result, Progress in Probability and Statistics, Vol. 11 ,  BirkhBuser, 

and a New Model," Journal ofEmpirica1 Finance, 1 ,  83-106. Boston. 
Dolley, J., 1933, "Characteristics and Procedure of Common Stock Split-Ups," Har- Eckbo, B., 1983, "Horizontal Mergers, Collusion, and Stockholder Wealth," Journal 

vard Business Review, 316-326. of Financial Economics, 11, 241-276. 
Donaldson, R., and M. Kamstra, 1996, "A New Dividend Forecasting Procedure that Eckbo, E., V. Maksimovic, and J. Williams, 1990, "Consistent Estimation of Cross- 

Rejects Bubbles in Asset Prices: The Case of 1929's Stock Crash," Rmiew of Sectional Models in Event-Studies," Review of Financial Studies, 3 (3), 343- 
Financial Studies, 9, 333-383. 365. 

Donoho, D., and I.J ohnstone, 1989, "Projection-Based Approximation and ADuality Edwards, R., and J. Magee, 1966, Technical Analysis of Stock Tmds (revised 5th ed.), 
with Kernel Methods," Annals of Statistics, 17, 58-106. John Magee, Boston. 

Duffie, D., 1992, Dynamic Asset Pricing Themy, Princeton University Press, Princeton, Eichenbaum, M., L. Hansen, and K. Singleton, 1988, "A Time Series Analysis of 
NJ. Representative Agent Models of Consumption and Leisure Choice under 

Duffie, D., and C. Huang, 1985, "Implementing Arrow-Debreu Equilibria by Contin- Uncertainty," Quarterly Journal ofEconomics, 103,51-78. 
uous Trading of Few Long-Lived Securities,"-Economtrica, 53,1337-1356. Eikeboom, A., 1993, "The Dynamics of the Bid-Ask Spread," working paper, Sloan 

Duffie, D., and R. Kan, 1993, "AYield-Factor Model of Interest ~ a t e s ,u"n published School of Management, Massachusetts Institute of Technology, Cambridge, 
paper, Stanford University. MA. 



References 

Einstein, A., 1905, "Ueber die von der molekular-kinetischen Theorie der ,1984, "The Information in the Term Structure,"J o~rnalofFinancialEco~ 
Wirme geforderte Bewegungvon in ruhenden Flfissigkeiten suspendierten 13,509-521. 
Teilchen," A n n a h  der Physik, 17,549-560. , 1990, "Term-Structure Forecasts of Interest Rates, Inflation, and Rea 

Engle, R., 1982, "Autoregressive Conditional Heteroskedasticity with Estimates of turns," Journal o f Monetary Economics, 25, 59-76. 
the Variance of UK Inflation," Econometrica, 50,987-1008. , 1991, "Efficient Capital Markets: 11," Journal of Finance, 46, 1575-1618. 

, 1984, "Wald, Likelihood Ratio, and Lagrange Multiplier Tests in Economet- , 1993, "Multifactor Portfolio Efficiency and Multifactor Asset Pricing b 
rics," in Z. Griliches and M. Intriligator (eds.), Handbook of Econometrics, els," working paper, CRSP, University of Chicago, Chicago, IL. 
Volume II, North-Holland, Amsterdam, chap. 13. Fama, E., and R Bliss, 1987, "The Information in Long-Maturity Forward Rat, 

Engle, R., and G. Conzalez-Rivera, 1991, "Semiparametric ARCH Models," Journal American Economic Review, 77,680-692. 
of Business and Economic Statistics, 9, 345-359. Fama, E., and M. Blume, 1966, "Filter Rules and Stock Market Trading Profit 

Engle, R., and C. Granger, 1987, "Cointegration and Error-Correction: Representa- Journal of Business, 39,22&241. 
tion, Estimation, and Testing," Econometrica, 55,251-276. Fama, E., and K. French, 1988a, "Dividend Yields and Expected Stock Return: 

Engle, R., and K. Kroner, 1995, "Multivariate Simultaneous Generalized ARCH," Journal ofFinancial Economics, 22, 3-27. 
Econometric Thory, 1 1,122-1 50. , 1988b, "Permanent and Temporary Components of Stock Prices,"J ournal 

Engle, R., and V. Ng, 1993, "Measuring and Testing the Impact of News on Volatility," Political Economy, 96, 246273. 
Journal of Finance, 48, 1749-1778. , 1989, "Business Conditions and Expected Returns on Stocks and Bonds, 

Engle, R., D. Lilien, and R Robins, 1987, "Estimating Time-Varying Risk Premia in Journal of Financial Economics, 25, 23-49. 
the Term Structure: The ARCH-M Model," Econometrica, 55,391-407. , 1992, "The Cross-Section of Expected Stock Returns," Journal ofFinance, 47, 

Engle, R., V. Ng, and M. Rothschild, 1990, "Asset Pricing with a Factor ARCH Covari- 427-465. 
ance Structure: Empirical Estimates for Treasury Bills,"J ournal offconomet- , 1993, "Common Risk Factors in the Returns on Stocks and Bonds," Journal 
rics, 45, 213-238. of Financial Economics, 33, 3-56. 

Epstein, L., and S. Zin, 1989, "Substitution, Risk Aversion, and the Temporal Be- , 1996a, "Multifactor Explanations of Asset Pricing Anomalies," Journal of 
havior of Consumption and Asset Returns: A Theoretical Framework," Finance, 51, 55-84. 
Econometrica, 57,937-968. , 1996b, "The CAPM Is Wanted, Dead or Alive," Journal ofFinance, forthcom- 

, 1990, "First-Order Risk Aversion and the Equity Premium Puzzle,"J ournal of ing. 
Monetary Economics, 26, 387-407. Fama, E., and J. MacBeth, 1973, "Risk, Return, and Equilibrium: Empirical Tests," 

,1991, "Substitution, Risk Aversion, and the Temporal Behavior of Consump Journal of Political Economy, 71,607-636. 
tion and Asset Returns: An Empirical Investigation," Journal of Political Fama, E., and R. Roll, 1971, "Parameter Estimates for Symmetric Stable Distribu- 
Economy, 99,263-286. tions," Journal of the American Statistical Association, 66, 331-338. 

Tstrella, A., and G. Hardouvelis, 1991, "The Term Structure as a Predictor of Real Fama, E., L. Fisher, M. Jensen, and R. Roll, 1969, "The Adjustment of Stock Prices 
Economic Activity," Journal ofFinance, 46,555-576. to New Information," Intaational Economic Review, 10, 1-21. 

lytan, T., and G. Harpaz, 1986, "The Pricing of Futures and Options Contracts on Fang, K., and Y Wang, 1994, Number-Themetic Methods in Statistics, Chapman and Hall, 
the Value Line Index," Journal ofFinance, 41, 843-856. London. 

ibozzi, F., 1996, Bond Markets, Analysis and Strategies (3rd ed.), Prentice-Hall, Upper Faust, J., 1992, "When Are Variance Ratio Tests For Serial Dependence Optimal?," 
Saddle River, NJ. Econometrica, 60,1215 -1 226. 

bozzi, F., and T, Fabozzi (eds.), 1995, The Handbook @Fixed-Income Securities (4th Feller, W., 1968, An Introduction to Probability Theory and Its Applications, John Wiley 
ed.), Irwin, Burr Ridge, IL. and Sons, New York. 

ma, E., 1965, "The Behavior of Stock Market Prices,"J ournal ofRusiness, 38,34-105. Ferson, W., and G. Constantinides, 1991, "Habit Persistence and Durability in Ag- 
-, 1970, "Efficient Capital Markets: A Review of Theory and Empirical Work," gregate Consumption: Empirical Tests," Journal of Financial Economics, 29, 

Journal ofFinance, 25, 383-41 7. 199-240. 
-, 1975, "Short-Term Interest Rates as Predictors of Inflation," American Eco- 

nomic Review, 65,269-282. 
- -. - . . .  . ., t , . . I .  



558 References References 559 

Fielitz, B., 1976, "Further Results on Asymmetric Stable Distributions of Stock Prices , 1978, "The Pricing of Supershares," Journal of Financial Economics, 6, 3-10. 
Changes," Journal of Financial and Quantitative Analysis, 11, 39-55. Garman, M., and M. Klass, 1980, "On the Estimation of Security Price Volatilities 

Fielitz, B., and J. Rozell, 1983, "Stable Distributions and Mixtures of Distributions from Historical Data," Journal of Business, 53, 67-78. 
Hypotheses for Common Stock Returns," Journal of the American Statistical George, T., G. Kaul, and M. Nimalendran, 1991, "Estimation of the Bid-Ask Spread 
Association, 78, 28-36. and Its Components: A New Approach," Reuim ofFinancia1 Studies, 4 2 3 -  

Fisher, L., 1966, "Some New Stock Market Indexes," Journal of Business, 39, 191-225. 656. 
Fishman, G., 1996, Monte Carlo: Concepts, Algorithms, and Applications, Springer- Gerber, H., and S. Li, 1981, "The Occurrence of Sequence Patterns in Repeated 

Verlag, New York. Experiments and Hitting Times in a Markov Chain," Stochastic Processes and 
Flavin, M., 1983, "Excess Volatility in the Financial Markets: A Reassessment of the Their Applications, 11, 101-108. 

Empirical Evidence," Journal of Political Economy, 91,929-956. Geske, R., and K. Shastri, 1985, "Valuation by Approximation: A Comparison of 
Frankel, J., G. Galli, and A. Giovannini, eds., 1996, The Microstructure of Fureign Ex- Option Valuation Techniques," Journal ofFinancial and QuantitativeAnalysis, 

change Markets (NBER Conference Report), University of Chicago Press, 20,45-72. 
Chicago, IL. Gibbons, M., 1982, "Multivariate Tests of Financial Models: A New Approach,"J our- 

French, R,a nd R. Roll, 1986, "Stock Return Variances: The Arrival of Information nal of Financial Economics, 10, 3-27. 
and the Reaction of Traders," Journal ofFinancia1 Economics, 17, 5-26. Gibbons, M., and W. Ferson, 1985, "Testing Asset Pricing Models with Changing 

French, K., G. Schwert, and R. Starnbaugh, 1987, "Expected Stock Returns and Expectations and an Unobservable Market Portfolio," Journal of Financial 
Volatility," Journal of Financial Economics, 19, 3-30. Economics, 14, 21 7-236. 

Friedman, J., and W. Stuetzle, 1981, "Projection Pursuit Regression," Journal of the Gibbons, M., and K. Ramaswamy, 1993, "A Test of the Cox, Ingersoll, and Ross Model 
American Statistical Association, 76 (376), December. of the Term Structure," Reuiew ofFinancial Studies, 6, 619-658. 

Friend, I., and M. Blume, 1975, "The Demand for Risky Assets," Amaican Economic Gibbons, M., S. Ross, and J. Shanken, 1989, "A Test of the Efficiency of a Given 
Review, 65,900-922. Portfolio," Econometrics, 57, 11 21-1 152. 

Froot, R, 1989, "New Hope for the Expectations Hypothesis of the Term Structure Gilles, C., and S. LeRoy, 1991, "Econometric Aspects of the Variance Bounds Tests: 
of Interest Rates," Journal of Finance, 44, 283-305. A Survey," Review ofFinancia1 Studies, 4, 753-791. 

Froot, K., and M. Obstfeld, 1991, "Intrinsic Bubbles: The Case of Stock Prices," Giovannini, A., and P. Weil, 1989,'"Risk Aversion and Intertemporal Substitution in 
A m ' c a n  Economic Review, 81,1189-1 21 7. the Capital Asset Pricing Model," Working Paper 2824, NBER, Cambridge, 

Fuller, W., 1976, Introduction to Statistical Time Series, Wiley, New York. MA. 
Furbush, D., andJ . Smith, 1996, "Quoting Behavior on NASDAQ: The Determinants Gleick,J ., 1987, Chaos: Making a New Science, Viking Penguin Inc., New York. 

of Clustering and Relative Spreads," Economists Inc., Washington, D.C. Glosten, L., 1987, "Components of the Bid-Ask Spread and the Statistical Properties 
Galai, D., 1977, "Tests of Market Efficiency of the Chicago Board Options Exchange," of Transaction Prices," Journal ofFinance, 42, 1293-1307. 

Journal of Business, 50, 167-197. Glosten, L., and L. Harris, 1988, "Estimating the Components of the Bid/Ask 
, 1978, "Empirical Tests of Boundary Conditions for CBOE Options," Journal Spread," Journal ofFinancial Economics, 21, 123-142. 

of Financial Economics, 6, 187-21 1. Glosten, L., and P. Milgrom, 1985, "Bid, Ask and Transaction Prices in a Special- 
Gallant, A., 1987, Nonlinear Statistical Models,J ohn Wiley and Sons, New York. ist Market with Heterogeneously Informed Traders," Journal of Financial 
Gallant, A., and H. White, 1992, "On Learning the Derivatives of an Unknown Economics, 14, 71-100. 

Mapping with Multilayer Feedforward Networks," Neural Networks, 5, 128- Glosten, L., R. Jagannathan, and D. Runkle, 1993, "On the Relation Between the 
138. Expected Value and the Volatility of the Nominal Excess Return on Stocks," 

Gallant, R., P. Rossi, and G. Tauchen, 1991, "Stock Prices and Volume," Review of Journal ofFinance, 48, 1779-18 01. 
Financial Studies, 5, 199-242. Godek, P., 1996, "Why NASDAQ Market Makers Avoid Odd-Eighth Quotes,"J ournal 

Garber, P., 1989, "Tulipmania," Journal of Political Economy, 97,535-560. ofFinancia1 Economics, 41, 465-474. 

Garman, M., 1976a, "Market Microstructure," Journal of Financial Economics, 3, 257- Goldenberg, D., 1991, "A Unified Method for Pricing Options on Diffusion Pro- 
275. cesses,"J ournal of Financial Economics, 29, 3-34. 

, 1976b, "A General Theory of Asset Valuation under Diffusion State Pro- Goldman, B., H. Sosin, and M. Gatto, 1979, "Path Dependent Options: 'Buy at the 
cesses," Working Paper 50, University of California, Berkeley. Low, Sell at the High,"'Jmrnal of Finance, 34, 111 1-1 127. 



References 561 

Goldman, B., H. Sosin, and L. Shepp, 1979, "On Contingent Claims that Insure Grossman, S., and R. Shiller, 1981, "The Determinants of the Variability of Stock 
Ex-Post Optimal Stock Market Timing," Journal of Finance, 34,401414 . Market Prices," Ammian Economic Review, 71,222-227. 

Goldstein, M., 1993, Bid-Ask Spreads on U S .  Equity Markets, Unpublished Ph.D. dis- , 1982, "Consumption Correlatedness and Risk Measurement in Economies 
sertation, Wharton School, University of Pennsylvania. with Non-Traded Assets and Heterogeneous Information,"J ournal ofFinan- 

Goodhart, C., and R. Curcio, 1990, "Asset Price Discovery and Price Clustering in the cia1 Economics, 10, 195-210 . 
Foreign Exchange Market," unpublished working paper, London School Grossman, S., and J. Stiglitz, 1980, "On the Impossibility of Informationally Efficient 
of Economics. Markets," American Economic Review, 70,393-408. 

Gordon, M., 1962, The Investment, Financing, and Valuation of the Corporation, Irwin, Grossman, S., and Z. Zhou, 1996, "Equilibrium Analysis of Portfolio Insurance," 
Homewood, IL. Journal of Finance, 51, 1379-1 403. 

Gottlieb, G., and A. Kalay, 1985, "Implications of the Discreteness of Observed Stock Grossman, S., A. Melino, and R. Shiller, 1987, "Estimating the Continuous Time 
Prices," Journal ofFinance, 40, 135-154. Consumption Based Asset Pricing Model," Journal of Business and Economic 

Gourieroux, C., A. Monfort, and A. Trognon, 1985, "A General Approach to Serial Statistics, 5, 315-328. 
Correlation," Econometric Theory, 1,315-340. Grossman, S., M. Miller, D. Fischel, K. Cone, and D. Ross, 1995, "Clustering and 

Gourlay, A., and S. McKee, 1977, "The Construction of Hopscotch Methods for Competition in Asset Markets," Lexecon Inc. Report. 
Parabolic and Elliptic Equations in Two Space Dimensions with a Mixed Grundy, B., and M. McNichols, 1989, "Trade and Revelation of Information through 
Derivative,"J ournal of Computational and Applied Mathematics, 201-206. Prices and Direct Disclosure," Review of Financial Studies, 2,495-526. 

Graham, R., D. Knuth, and 0. Patashnik, 1989, Concrete Mathematics: A Foundation Gultekin, N., R. Rogalski, and S. Tini~1,  982, "Option Pricing Model Estimates: 
for Computer Science, Addison-Wesley, Reading, MA. Some Empirical Results," Financial Management, ll,58-69. 

Grandmont, J., and P. Malgrange, 1986, "Introduction to Nonlinear Economic Dy- Gurland, J., T. Lee, and P. Dahm, 1960, "Polychotomous Quanta1 Response in Bio- 
namics,"J ournal of Economic T h e q , 4 0, 3-1 2. logical Assay," Biometries, 16, 382-398. 

Granger, C., 1966, "The Typical Spectral Shape of an Economic Variable," Economet- Hagerman, R., 1978, "More Evidence on the Distribution of Security Returns,"J our- 
rica, 34, 150-161. nal of Finance, 33, 1213 -1 220. 

, 1969, "Investigating Causal Relations by Econometric Models and Cross- Hakansson, N., 1976, "Purchasing Power Funds: A New Kind of Financial Interme- 
Spectral Methods," Econometrica, 37,424-438. diary," Financial Analysts Journal, 32,49-59. 

, 1980, "Long Memory Relationships and the Aggregation of Dynamic Mod- , 1977, "The Superfund: Efficient Paths towards Efficient Capital Markets 
els," Journal ofEconometrics,1 4, 227-238. in Large and Small Countries," in H. Levy and M. Sarnat (eds.), Financial 

Granger, C., and A. Andersen, 1978, An Introduction to Bilinear Time Series Models, Decision Making Under Uncertainty, Academic Press, New York. 
Vandenhoeck and Ruprecht, GGttingen. Hald, A., 1990, A H i s t q  of Probability and Statistics and Their Applications befme 1750, 

Granger, C., and R. Joyeux, 1980, "An Introduction to Long Memory Time Series John Wiley and Sons, New York. 
Models and Fractional Differencing,"J ournalof TimeSaaesAnalysi,1 ,15-29. Hall, A., 1992, "Some Aspects of Generalized Method of Moments Estimation," in 

Granger, C., and 0. Morgenstern, 1963, "Spectral Analysis of NewYork Stock Market G. Maddala, C. Rao, and H. Vinod (eds.), Handbook of Statistics, Volume 11: 
Prices," Kyklos, 16, 1-27. Econometrics, North-Holland, Amsterdam. 

Granger, C., and 0. Morgenstern, 1970, Predictability of Stock Market Prices, Heath- Hall, R., 1988, "Intertemporal Substitution in Consumption," Journal ofPoliticalEcon- 
Lexington, Lexington, MA. omy, 96,221-273. 

Granito, M., 1984, Bond Portfolio Immunization, Lexington, Lexington, MA. Halpern, P., and S. Turnbull, 1985, "Empirical Tests of Boundary Conditions for 
Grassberger, P., and I. Procaccia, 1983, "Measuring the Strangeness of Strange At- Toronto Stock Exchange Options," Journal ofFinance, 40,481-500. 

tractors," Physica, 9D, 189-208. Hamilton, J., 1989, "A New Approach to the Economic Analysis of Nonstationary 
Gray, S., 1996, "Modeling the Conditional Distribution of Interest Rates as a Regime- Time Series and the Business Cycle," Econometrica, 57, 357-384. 

Switching Process," Journal of Financial Economics, 42, 27--62. , 1990, "Analysis of Time Series Subject to Changes in Regime," Journal of 
Grinblatt, M., and S. Titman, 1985, "Factor Pricing in a Finite Economy," ~ournaolf  Econometrics, 45,39-70. 

Financial Economics, 12, 497-507. , 1993, "Estimation, Inference, and Forecasting of Time Series Subject to 
Grossman, S., 1989, The I n f m t i o n a l  file of Prices, Massachusetts Institute of Tech- Changes in Regime," in G. Maddala, C. Rao, and H. Vinod, eds., Handbook 

nology Press, Cambridge, MA. ofStatistics, Volume 11, North-Holland, New York. 



Hamilton, J., 1994, Time Series Analysis, Princeton University Press, Princeton, NJ. Harris, L., G. Sofianos, and J. Shapiro, 1994, "Program Trading and IntradayVolatil- 
ity," Review of Financial Studies, 7,653-685. 

Hammersley,J ., and D. Handscomb, 1964, Mon,te Carlo Methods, Chapman and Hall, 
London. Harrison, M., 1985, Brownian Motion and StochasticI;lowS ystems,J ohn Wiley and Sons, 

New York. 
Hammersley, J., and J. Mauldon, 1956, "General Principles of Antithetic Variates," 

Proceedings ofthe Cambridge Philosophical Society, 52, 476-481. Harrison, M., and D. Kreps, 1979, "Martingales and Arbitrage in Multiperiod Secu- 
rities Markets,"J ournal ofEconomic Themy, 20,381408. 

Hampel, F., 1986, Robust Statistics: The Appoach Based on Injluace Functions, John 
Wiley and Sons, New York. Harrison, M., and S. Pliska, 1981, "Martingales and Stochastic Integrals in the Theory 

of Continuous Trading," Stochastic Processes and Their Applicatiom, 11, 215- 
Hansen, L., 1982, "Large Sample Properties of Generalized Method of Moments 260. 

Estimators," Econometrica, 50, 1029-1054. 
Harvey, A., E. Ruiz, and N. Shephard, 1994, "Multivariate Stochastic Variance Mod- 

, 1985, "A Method for Calculating Bounds on the Asymptotic Covariance els," Review of Economic Studies, 61, 247-264. 
Matrices of Generalized Method of Moments Estimators," Journal ofEcono- 

Harvey, C., 1988, "The Real Term Structure and Consumption Growth," Journal of 
metrics, 30, 203-238. 

Financial Economics, 22, 305-334. 
Hansen, L., and R. Hodrick, 1980, "Forward Exchange Rates as Optimal Predictors 

, 1989, "Time-Varying Conditional Covariances in Tests of Asset Pricing Mod- 
of Future Spot Rates: An Econometric Analysis,"J ournal ofPoliticalEconomy, 

els," Journal of Financial Economics, 24, 289-31 7. 
88,829-853. 

, , 1991, "The World Price of Covariance Risk,"J ournal ofFinance, 46, 111 -157. 
1983, "Risk Averse Speculation in the Forward Foreign Exchange Market: 
An Econometric Analysis of Linear Models," in J. Frenkel (ed.), Exchange Harvey, C., and G. Zhou, 1990, "Bayesian Inference in Asset Pricing Tests,"Journal 
Rates and International Mameconomics, University of Chicago Press, Chicago, of Financial Economics, 26, 221-254. 
IL. Hasbrouck, J., 1988, "Trades, Quotes, Inventories, and Information," Journal of Fi- 

Hansen, L., and R. Jagannathan, 1991, "Restrictions on Intertemporal Marginal nancial Economics, 22, 229-252. 
Rates of Substitution Implied by Asset Returns," Journal of Political Economy, , 1991a, "Measuring the Information Content of Stock Trades," Journal of 
99,225-262. Finance, 46, 179-208. 

Hansen, L., and S. Richard, 1987, "The Role of Conditioning Information in De- , 1991b, "The Summary Informativeness of Stock Trades: An Econometric 
ducing Testable Restrictions Implied by Dynamic Asset Pricing Models," Analysis," Reuiew ofFinancial Studies, 4, 571-595. 
Economtrica, 55,587-613 . Hasbrouck, J., and T. Ho, 1987, "Order Arrival, Quote Behavior, and the Return- 

Hansen, L., and J. Scheinkman, 1995, "Back to the Future: Generating Moment Im- Generating Process,"J ournal ofFinance, 42, 1035-1048. 
plications for Continuous-Time Markov Processes," Econometrica, 63, 767- Hausman, J., 1978, "Specification Tests in Econometrics," Econometrica, 46, 1251- 
804. 1271. 

Hansen, L., and K. Singleton, 1982, "Generalized Instrumental Variables Estimation Hausman, J., A. Lo, and C. MacKinlay, 1992, "An Ordered Probit Analysis of Trans- 
of Nonlinear Rational Expectations Models," Econometrica, 50,1269-1288. action Stock Prices," Journal ofFinancial Economics, 31,319-379. 

, 1983, "Stochastic Consumption, Risk Aversion and the Temporal Behavior He, H., and H. Leland, 1993, "On Equilibrium Asset Price Processes," Review of 
of Asset Returns," Journal of Political Economy, 91, 249-268. Financial Studies, 6, 593-61 7. 

Hansen, L., J. Heaton, and E. Luttmer, 1995, "Econometric Evaluation of Asset He, H., and D. Modest, 1995, "Market Frictions and Consumption-Based Asset Pric- 
Pricing Models," Review ofFinancia1 Studies, 8, 237-274. ing," Journal of Political Economy, 103 ,9411 7. 

Hansen, L., J. Heaton, and M. Ogaki, 1988, "Efficiency Bounds Implied by Multi- Heath, D., R. Jarrow, and A. Morton, 1992, "Bond Pricing and the Term Structure 
period Conditional Moment Restrictions," Journal of the American Statistical of Interest Rates: A New Methodology for Contingent Claims Valuation," 
Associatim, 83,863-871. Econometrica, 60, 77-105. 

Hiirdle, W., 1990, Applied Nonparametric Regression, Cambridge University Press, Cam- Heaton, J., 1995, "An Empirical Investigation of Asset Pricing with Temporally De- 
bridge, UK. pendent Preference Specifications," Economtrica, 681-717. 

Harris, L., 1990, "Estimation of Stock Variances and Serial Covariancesf rom Discrete Heaton, J., and D. Lucas, 1996, "Evaluating the Effects of Incomplete Markets on 
Observations," Journal of Financial and Quantitative Analysis, 25, 291-306. Risk Sharing and Asset Pricing," Journal of Political Economy, 104, 668-712. 

, 1991, "Stock Price Clustering and Discreteness," Revieu of Financial Studies, Helson, H., and D. Sarason, 1967, "Past and Future," Mathematica Scandinavia, 21, 
4, 389-415. 5-1 6. 



564 References References 

Henriksson, R., and R. Merton, 1981, "On Market Timing and Investment Perfor- , 1993, "Implications of Nonlinear Dynamics for Financial Risk Management," 
mance, 11: Statistical Procedures for Evaluating Forecasting Skills," Journal Journal ofFinancia1 and Quantitative Analysis, 1993, 28, 41-64. 
of Business, 54, 513-533. Hsu, D., R. Miller, and D. Wichern, 1974, "On the Stable Paretian Behavior of Stock 

Hentschel, L., 1995, "All in the Family: Nesting Symmetric and Asymmetric GARCH Market Prices," Journal of the American Statistical Association, 69, 108-1 13. 
Models," Journal of Financial Economics, 39, 71-104. Huang, C., 1992, Lecture Notes on the Theory ofFinancia1 Markets in  Continuous Time. 

Herrndorf, N., 1984, "A Functional Central Limit Theorem for Weakly Dependent Huang, C., and R. Litzenberger, 1988, Foundations for Financial Economics, North- 
Sequences of Random Variables," Annals of Probability, 12,141-153. Holland, New York. 

Hertz, J., A. Krogh, and R. Palmer, 1991, Introduction to the Theory of Neural Computa- Huang, R., and H. Stoll, 1995a, "The Components of the Bid-Ask Spread: A Gen- 
tion, Addison-Wesley Publishing Company, Reading, MA. eral Approach," Financial Markets Research Center Working Paper 94-33, 

Heston, S., 1992, "Testing Continuous-Time Models of the Term Structure of Interest Owen Graduate School of Management, Vanderbilt University. 
Rates," unpublished paper, Yale University. 1995b, "Dealer Versus Auction Markets: A Paired Comparison of Execution 

, 1993, "A Closed-Form Solution for Options with Stochatic Volatility with Costs on NASDAQ and the NYSE," Financial Markets Research Center 
Applications to Bond and Currency Options," Review ofFinancia1 Studies, 6, Working Paper 95-1 6, Owen Graduate School of Management, Vanderbilt 
327-343. University. 

Hicks, J., 1946, Value and Capital (2d ed.), Oxford University Press, Oxford. Huber, P., 1985, "Projection Pursuit," Annals ofStatistics, 13(2),4 35-525. 
Huberman, G., 1982, "A Simple Approach to Arbitrage Pricing Theory," Journal of 

Hinich, M., and D. Patterson, 1985, "Evidence of Nonlinearity in Daily Stock Re- 
turns," Journal of Business and Economic Statistics, 3, 69-77. Economic Theory, 28, 183-191. 

Huberman, G., and S. Kandel, 1987, "Mean-Variance Spanning," Journal ofFinance, 
Ho, T., and S. Lee, 1986, "Term Structure Movements and Pricing Interest Rate 

Contingent Claims," Journal of Finance, 41, 101 1-1029. 42(4), 873-888. 
Hull, J., 1993, Options, Futures, and OtherDerivative Securities (2d ed.), Prentice-Hall, 

Ho, T., and H. Stoll, 1980, "On Dealership Markets under Competition," Journal of 
Finance, 35, 259-267. Englewood Cliffs, New Jersey. 

Hull, J., and A. White, 1987, "The Pricing of Options on Assets with Stochastic 
,1981," Optimal Dealer Pricing under Transactions and Return Uncertainty," 

Volatilities,"J ournal ofFinance, 42, 281-300. 
Journal of Financial Economics, 9,47-73. 

, 1990a, "Pricing Interest-Rate-Derivative Securities," Review of Financial Stud- 
Hodrick, R., 1992, "Dividend Yields and Expected Stock Returns: Alternative Pro- ies, 3, 573-592. 

cedures for Inference and Measurement," Review of Financial Studies, 5, 
357-386. , 1990b, "Valuing Derivative Securities Using the Explicit Finite Difference 

Method," Journal ofFinancia1 and Quantitative Analysis, 25, 87-100. 
Hoel, P., S. Port, and C. Stone, 1972, Introduction to Stochastic Processes, Houghton 

Mifflin, Boston, MA. Hurst, H., 1951, "Long Term Storage Capacity of Reservoirs," Transactions of the 
American Society of Civil Engineers, 116, 770-799. 

Hofmann, N., E. Platen, and M. Schweizer, 1992, "Option Pricing under Incom- 
pleteness and Stochastic Volatility," Mathematical Finance, 2, 153-187. Hutchinson, J., A. Lo, and T. Poggio, 1994, "A Nonparametric Approach to the Pric- 

ing and Hedging of Derivative Securities Via Learning Networks," Journal 
Hogarth, R., and M. Reder, eds., 1987, Rational Choice: The Contrast Between Economics ofFinance, 49,851-889. 

and Psychology, University of Chicago Press, Chicago, IL. 
Ingersoll, J., 1987, T h e q o fFinancialDecision Making, Rowman & Littlefield, Totowa, 

Holden, A., ed., 1986, Chaos, Princeton University Press, Princeton, NJ. 
NJ. 

Hornik, K., 1989, "Multilayer Feedforward Networks Are Universal Approximators," Ingersoll, J. Jr., J. Skelton, and R. Weil, 1978, "Duration Forty Years Later," Journal of 
Neural Netwmks, 2(5), 359-366. Financial and Quantitative Analysis, 13, 627-650. 

Hornik, K., M. Stinchcombe, and H. White, 1990, "Universal Approximation of an It6, K, 1951, "On Stochastic Differential Equations," Memoirs of the American Mathe- 
Unknown Mapping and Its Derivatives," Neural Netwmks, 3,551-560. matical Society, 4, 1-5 1.  

Hosking, J., 1981, "Fractional Differencing," Biometrika, 68, 165-176. Jackwerth, J., and M. Rubinstein, 1995, "Recovering Probability Distributions from 
Hsieh, D., 1989, "Testing for Nonlinear Dependence in Daily Foreign Exchange Contemporary Security Prices," working paper, Haas School of Business, 

Rates," Journal of Business, 62, 339-368. University of California at Berkeley. 

, 1991, "Chaos and Nonlinear Dynamics: Application to Financial Markets," Jacquier, E., N. Polson, and P. Rossi, 1994, "Bayesian Analysis of Stochastic Volatility 
Journal ofFinance, 46, 1839-1877. Models," Journal of Business and Economic Statistics, 12, 371-389. 



566 References References 

Jagannathan, R., and Z. Wang, 1996, "The Conditional CAPM and the Cross-Section Kales( M., and P. Whitlock, 1986, Monte Carlo Methods, V o b I:~ Ba sics, John Wiley 
of Expected Returns," Journal of Finance, 51,3 -53. and Sons, New York. 

Jain, P., 1986, "Analyses of the Distribution of Security Market Model Prediction Kandel, E., and L. Marx, 1996, "NASDAQ Market Structure and Spread Patterns," 
Errors for Daily Returns Data," Journal ofAccounting Research, 24, 7696. unpublished working paper, Simon Graduate School of Business, Univer- 

Jamshidian, F., 1989, "An Exact Bond Option Formula," Journal ofFinance, 44, 205- sity of Rochester. 

209. Kandel, S., 1984, "The Likelihood Ratio Test of Mean-Variance Efficiency without a 

Jarrell, G., and A. Poulsen, 1989," The Returns to Acquiring Firms in Tender Offers: Riskless Asset," Journal of Financial Economics, 13, 575-592. 
Evidence from Three Decades," Financial Management, 18, 12-19. Kandel, S., R. McCulloch, and R. Stambaugh, 1995, "Bayesian Inference and Port- 

Jarrell, G., J. Brickley, and J. Netter, 1988, "The Market for Corporate Control: The folio Efficiency," Review ofFinancia1 Studies, 8, 1-53. 
Empirical Evidence Since 1980,"J ournal ofEconomic Perspectives, 2,639-658. Kandel, S., and R. Stambaugh, 1987, "On Correlations and Inferences about Mean- 

Jarrow, R., and A. Rudd, 1982," Approximate Option Valuation for Arbitrary Stochas- Variance Efficiency," Journal ofFinancia1 Economics, 18, 61-90. 
tic Processes," Journal of Financial Economics, 10, 347-369. , 1989, "Modelling Expected Stock Returns for Long and Short Horizons," 

Jegadeesh, N., 1990," Evidence of Predictable Behavior of Security Returns," Journal Working Paper 42-88,R odney L. White Center, Wharton School, University 
of Finance, 45, 881-898. of Pennsylvania. 

, 1991, "Seasonality in Stock Price Mean Reversion: Evidence from the U.S. ,1990," A Mean-Variance Framework for Tests ofAsset Pricing Models," Review 
and the U.K.," Journal ofFinance, 46, 1427-1444. of Financial Studies, 2, 125-1 56. 

Jegadeesh, N., and S. Titman, 1993," Returns to Buying Winners and Selling Losers: , 1991, "Asset Returns and Intertemporal Preferences," Journal of Monetary 
Implications for Stock Market Efficiency,"J ournal ofFinance, 48,6 5-91. Economics, 27, 39-71. 

, 1995, "Overreaction, Delayed Reaction, and Contrarian Profits," Review of , 1995, "Portfolio Inefficiency and the Cross-Section of Expected Returns," 
Financial Studies, 8,9 73-993. Journal of Finance, 50, 157-184. 

Jensen, M., and R. Ruback, 1983," The Market for Corporate Control: The Scientific Kane, E., 1970, "The Term Structure of Interest Rates: An Attempt to Reconcile 
Evidence," Journal of Financial Economics, ll,5-50. Teaching with Practice," Journal ofFinance, 25, May, 361-374. 

Jerison, D., I. Singer, and D. Stroock (eds.), 1996, The 1994 Wiener Symposium Proceed- , 1983, "Nested Tests of Alternative Term Structure Theories," Review ofEco- 
ings, American Mathematical Society, Providence, RI. nomics and Statistics, 65, 1 15-1 23. 

Jobson, D., and R. Korkie, 1982, "Potential Performance and Tests of Portfolio Effi- Kane, E., and H. Unal, 1988, "Change in Assessments of Deposit Institution Riski- 
ciency," Journal of Financial Economics, 10, 433-466. ness," Journal of Financial Seruices Research, 1, 207-229. 

, 1985, "Some Tests of Linear Asset Pricing with Multivariate Normality," Karpoff,J ., 1986, "A Theory of Trading Volume," Journal ofFinance, 41, 1069-1088. 
Canadian Journal of Administrative Sciences, 2, 114-138. ,1987," The Relation between Price Changes and Trading Volume: A Survey," 

Johnson, H., 1983," An Analytic Approximation of the American Put Price," Journal Journal of Financial and Quantitative Analysis, 22, 109-1 26. 
of Financial and Quantitative Analysis, 18, 141-148. Keim, D., 1989, "Trading Patterns, Bid-Ask Spreads, and Estimated Security Returns: 

Johnson, H., and D. Shanno, 1987," Option Pricing when the Variance Is Changing," The Case of Common Stocks at Calendar Turning Points," Journal ofFinan- 
Journal of Financial and Quantitative Analysis, 22, 143-1 5 1. cia1 Economics, 25, 75-97. 

Jones, L., 1987," On a Conjecture of Huber Concerning the Convergence of Projec- Keim, D., and A. Madhavan, 1995a, "Anatomy of the Trading Process: Empirical 
tion Pursuit Regression," Annals of Statistics, 15(2),8 80-882. Evidence on the Behavior of Institutional Traders," Journal ofFinancia1E m  

Joreskog, R,1 967, "Some Contributions to Maximum Likelihood Factor Analysis," nomics, 37, 371-398. 
Psychometrika, 34, 183-202. , 1995b," Execution Costs and Investment Performance: An Empirical Anal- 

Judge, G., W. Griffiths, C. Hill, H. Likepohl, and T. Lee, 1985, The Theory and Practice ysis of Institutional Equity Trades," working paper, School of Business Ad- 
ofEconometrics,J ohn Wiley and Sons, New York. ministration, University of Southern California. 

Kagel, J., and A. Roth, eds., 1995, Handbook of Exp mental Economics, Princeton , 1996, "The Upstairs Market for Large-Block Transactions: Analysis and 
University Press, Princeton, NJ. Measurement of Price Effects," Review of Financial Studies, 9, 1-36. 

Kahneman, D., and A. Tversky, 1979, "Prospect Theory: An Analysis of Decision Keim, D., and R. Stambaugh, 1986," Predicting Returns in Stock and Bond Markets," 
Under Risk," Econometrics, 47, 26.3-291. Journal of Financial Economics, 17, 357-390. 

eri



568 References Refaences 569 

Kendall, M., 1953, "The Analysis of Economic Time Series-Part I: Prices," Journal Lee, C., and M. Ready, 1991, "Inferring Trade Direction from Intraday Data," Journal 
of the Royal Statistical Society, 96, 11-25. ' of Finance, 46, 733-746. 

, 1954, "Note on Bias in the Estimation of Autocorrelation," Biometrilza, 41, Lee, S., and B. Hansen, 1994, "Asymptotic Theory for the GARCH(1,i) Quasi- 
403-404. Maximum Likelihood Estimator," Econometric T h q , 1 0, 29-52. 

Kennan, D., and M. O'Brien, 1993, "Competition, Collusion, and Chaos," Journal of Lehmann, B., 1987, "Orthogonal Frontiers and Alternative Mean Variance Efficiency 
Economic Dynamics and Control, 17,327-353. Tests," Journal of Finance, 42,601-619 . 

Kennedy, D., 1976, "The Distribution of the Maximum Brownian Excursion," Journal , 1990, "Fads, Martingales, and Market Efficiency," Quarter4 Journal of ECO- 
@Applied Probability, 13, 371 -376. nomics, 105, 1-28. 

Kim, M., C. Nelson, and R. Startz, 1988, "Mean Reversion in Stock Prices? A Reap , 1991, "Earnings, Dividend Policy, and Present Value Relations: Building 
praisal of the Empirical Evidence," Technical Report 2795, NBER, Cam- Blocks of Dividend Policy Invariant Cash Flows," Working Paper 3676, 
bridge, MA; to appear in Review ofEconomic Studies. NBER, Cambridge, MA. 

Kindleberger, C., 1989, Manias, Panics, and Crashes: A History of Financial Crises, , "Empirical Testing of Asset Pricing Models," in P. Newman, M. Milgate, and 
revised ed., Basic Books, New York. J. Eatwell (eds.), The New Palgrave Dictionary of Monty and Finance, Stockton 

Press, New York, pp. 749-759. 
Kleidon, A., 1986, "Variance Bounds Tests and Stock Price Valuation Models,"J ournal 

of Political Economy, 94, 953-1 001. Lehmann, B., and D. Modest, 1988, "The Empirical Foundations of the Arbitrage 
Pricing Theory," Journal of Financial Economics, 21, 21 3-254. 

Kleidon, A., and R. Willig, 1995, "Why Do Christie and Schultz Infer Collusion from 
Their Data?," unpublished working paper. Leroy, S., 1973, "Risk Aversion and the Martingale Property of Stock Returns," Znw- 

national Economic Review, 14, 436-446. 
Kocherlakota, N., 1996, "The Equity Premium: It's Still a Puzzle," Journal ofEconomic 

Literature, 34, 42-71. LeRoy, S., 1989, "Efficient Capital Markets and Martingales," Journal of Economic 
Literature, 27, 1583-1 621. 

Korajczyk, R., and C. Viallet, 1989, "An Empirical Investigation of International Asset 
Pricing," Review of Financial Studies, 2, 553-586. LeRoy, S., and R. Porter, 1981, "The Present Value Relation: Tests Based on Variance 

Bounds," Econometrica, 49,555-577. 
Kothari, S., J. Shanken, and R. Sloan, 1995, "Another Look at the Cross-Section of 

Expected Returns," Journal of Finance, 50, 185-224. LeRoy, S., and D. Steigerwald, 1992, "Volatility," Working Paper 6-92, Department 
of Economics, University of California Santa Barbara. 

Kreps, D., and E. Porteus, 1978, "Temporal Resolution of Uncertainty and Dynamic 
Choice Theory," Econometrica, 46,185-200. Levy, H., 1985, "Upper and Lower Bounds of Put and Call Option Values: Stochastic 

Dominance Approach," Journal of Finance, 40,1197-1218. 
Kreps, D., 1988, Notes on the Theory of Choice, Westview Press, Boulder, CO. 

Lkvy, P., 1924, "ThCorie des Erreurs. La Loi de Gauss et Les Lois Exceptionelles," 
Kroner, K., and V. Ng, 1993, "Modelling the Time Varying Comovement of As- Bull. Soc. Math., 52, 49-85. 

set Returns," unpublished paper, University of Arizona and International , 1925, Calcul des Pmbabilitis, Gauthier-Villers, Paris. 
Monetary Fund. 

Li, S., 1980, "A Martingale Approach to the Study of Occurrence of Sequence Pat- 
Kyle, A., 1985, "Continuous Auctions and Insider Trading," Econometrica, 53, 1 31 5- terns in Repeated Experiments," Annals @Probability, 8,1171-11 76. 

1335. 
Lintner, J., 1965a, "Security Prices, Risk and Maximal Gains from Diversification," 

Laibson, D., 1996, "Hyperbolic Discount Functions, Undersaving, and Savings Pol- Journal of Finance, 20, 587-61 5. 
icy," Working Paper 5635, NBER, Cambridge, MA. 

1965b, "The Valuation of Risky Assets and the Selection of Risky Investments 
Lakonishok,J ., A. Shleifer, and R. Vishny, 1994, "Contrarian Investment, Extrapola- in Stock Portfolios and Capital Budgets," Review ofEconomics and Statistics, 

tion, and Risk," Journal of Finance, 49, 1541-1578. 47, 13-37. 
Lanen, W., and R. Thompson, 1988, "Stock Price Reactions as Surrogates for the Net Litzenberger, R., and K. Ramaswamy, 1979, "The Effect of Personal Taxes and Div- 

Cashflow Effects of Corporate Financial Decisions," Journal of Accounting idends on Capital Asset Prices: Theory and Evidence," Journal ofFinancial 
and Economics, 10, 31 1-334. Economics, 7, 163-196. 

Lang, S., 1973, Calculus of Several Variables, Addison-Wesley, Reading, MA. Liuenberger, R., and J. Rolfo, 1984, "An International Study of Tax Effects on Gov- 
Leamer, E., 1978, Spec$caiion Searches, John Wiley and Sons, New York. ernment Bonds," Journul of Finance, 39, 1-22. 
LeBaron, B., 1996, "Technical Trading Rule Profitability and Foreign Exchange Liu, C., and J. He, 1991, "A Variance-Ratio Test of Random Walks in Foreign Ex- 

Intervention," Working Paper 5505, NBER, Cambridge, MA. change Rates,"J ournal of Finance, 46, 777-786. 



References 571 

,jung, G., and G. Box, 1978. "On a Measure of Lack of Fit in Time Series Models," Lorenz, E., 1963, "Deterministic Aperiodic Flow," Journal ofAtmocphac Scwncq 20, 
Biometrika, 66,67-72. 130-141. 

,lung, I,., and T. Siiderstrbm, 1986, Il 'heq and Practice of Recursive Identification, Lucas, R., Jr., 1978, "Asset Prices in an Exchange Economy," Econom~tricu,4 6, 1429- 
Massachusetts Institute of Technology Press, Cambridge, MA. 1446. 

Lo, A,, 1986, "Statistical Tests of Contingent Claims Asset-Pricing Models: A New Lumsdaine, R., 1995, "Finite-Sample Properties of the Maximum Likelihood Esti- 
Methodology," Journal of financial Economics, 17, 14.3-173. mator in GARCH(1,l) and IGARCH(1,l) Models: A Monte Carlo Investi- 

, 1987, "Semiparametric Upper Bounds for Option Prices and Expected Pay- gation," Journal of Business and Economic Statistics, 13, 1-10. 
offs," Journal ofFinancia1 Economics, 19, 37.3-388. Luttmer, E., 1994, "Asset Pricing in Economies with Frictions," unpublished paper, 

, 1988, "Maximum Likelihood Estimation of Generalized It6 Processes with Kellogg Graduate School of Management, Northwestern University. 
Discretely Sampled Data," Econometric Theory, 4, 231-247. Lutz, F., 1940, "The Structure of Interest Rates," Quarterly Journal of Economics, 55, 

, 1991, "Long Term Memory in Stock Market Prices," Econometrics, 59,1279- 36-63. 
1313. Macaulay, F., 1938, Some Theoretical Problems Suggested hy the Movements oflnterest Rates, 

, ed., 1995, Thp Industrial Organization and Regulation of the Securities Industry Bond Yields, and Stock Prices i n  the United States Since 1856, National Bureau 
(NBER Conference Report), University of Chicago Press, Chicago, IL. of Economic Research, New York. 

, ed., 1996, Market Efjciency: Stock Market Behaviour i n  T h e q  and Practice, MacBeth, J., and L. Merville, 1979, "An Empirical Examination of the Black-Scholes 
Edward Elgar Publishing, Ltd., London. Call Option Pricing Model," Journal ofFinance, 34, 1173-1 186. 

Lo, A,, and A. C. MacKinlay, 1988, "Stock Market Prices Do Not Follow Random , 1980, "Tests of the Black-Scholes and Cox Call Option Valuation Models," 
Walks: Evidence from a Simple Specification Test," Review ofFinancia1 Stud- Journal of Finance, 35, 285-300. 
its, 1 , 41 -66. Mackay, C., 1852, Memoirs ofExtrmdinary Popular Delusions and the Madness ofcrowds, 

, 1989, "The Size and Power of the Variance Ratio Test in Finite Samples: A 2nd ed., Office Nat. Illustrated Library, London. 
Monte Carlo Investigation," Journal ofEconometrics, 40, 203-238. MacKinlay, A. C., 1987, "On Multivariate Tests of the CAPM," Journul of Financial 

, l990a, "An Econometric Analysis of Nonsynchronous-Trading," Journal of Economics, 18, 341-372. 
Econometrics, 45, 181-212. , 1995, "Multifactor Models Do Not Explain Deviations from the CAPM," 

, lWOb, "Data-Snooping Biases in Tests of Financial Asset Pricing Models," Journal of Financial Economics, 38, 3-28. 
R m i m  of Financial S tud iq  3,431-468. MacKinlay, A. C., and M. Richardson, 1991, "Using Generalized Methods of Moments 

, 1990b, "When Are Contrarian Profits Due to Stock Market Overreaction?," to Test Mean-Variance Efficiency," Journal of Finance, 46, 51 1-527. 
Re71ieccr of Financial Studies, 3, 175-208. Maddala, G., 1983, Limited-Dependent and Quulitative Variables i n  Econometrics, Cam- 

, 1996, "Maximizing Predictability in the Stock and Bond Markets," Working bridge University Press, Cambridge, UK. 
Paper LFE-1019-96, MIT Laboratory for Financial Engineering. Madhavan, A., and S. Smidt, 1991, "A Bayesian Model of Intraday Specialist Pricing," 

Lo, A,, andJ. Wang, 1995, "Implementing Option Pricing Modelswhen Asset Returns Journal ofFinancia1 Economics, 30, 99-1 34. 
Are Predictable," Journal ofFinance, 50,87-129. Magnus, J., and H. Neudecker, 1988, Matrix Dfferential Calculus, John Wiley and 

Loewenstein. G., and D. Prelec, 1992, "Anomalies in Intertemporal Choice: Evi- Sons, New York. 
dence and an Interpretation," Quarterly Journal of Economics, 107,573-598. Malatesta, P., and R. Thompson, 1985, "Partially Anticipated Events: A Model of 

Longstaff, F., 1989, "A Nonlinear General Equilibrium Model of the Term Structure Stock Price Reactions with an Application to Corporate Acquisitions," Jour- 
of Interest Rates," Journal of Financial Economics, 23, 195-224. nal of Financial Economics, 14, 237-250. 

, 1992, "Multiple Equilibria and Term Structure Models," Journal of Financial Malkiel, B., 1992, "Efficient Market Hypothesis," in Newrnan, P., M. Milgate, and 
Economics, 32, 333-344. J. Eatwell (eds.), New Palgrave Dictionary of Money and Finance, Macmillan, 

London. 
, 1995, "Option Pricing and the Martingale Restriction," Review of Financial 

Sturlips, 8 ,  1091-1 124. Mandelbrot, B., 1963, "The Variation of Certain Speculative Prices," Journal of Busi- 
ness, 36, 394-419. 

Longstaff, F., and E. Schwartz, 1992, "Interest Rate Volatility and the Term Structure: 
A Two-Factor General Equilibrium Model," Jounull of finance, 47, 1259- , 1967, "The Variation of Certain Speculative Prices," Journal of Business, 36, 
1282. 394-419. 



572 References References 573 

, 1971, "When Can Price Be Arbitraged Efficiently? A Limit to the Validity of Mason, S., R. Merton, A. Perold, and P. Tufano, 1995, Cases in Financial Engznehng: 
the Random Walk and Martingale Models," Review ofEconomics and Statistics, Applied Studies ofFinancial Innovation, Prentice-Hall, Englewood Cliffs, NJ. 
53,225-236. Mayers, D., 1972, "Nonmarketable Assets and Capital Market Equilibrium under 

, 1972, "Statistical Methodology for Non-Periodic Cycles: From the Covari- Uncertainty," in Jensen, M. (ed.), Studies in the T h e q  of Capital Markets, 
ance to R/S Analysis," Annals ofEconomic and SocialMeczsuremat, 1,259-290. Praeger, New York, 223-248. 

, 1975, "Limit Theorems on the Self-Normalized Range for Weakly and McCallum, B., 1994, "Monetary Policy and the Term Structure of Interest Rates," 
Strongly Dependent Processes," Z. Wahrscheinlichkeitstheoriev enu., Gebiete Working Paper 4938, NBER, Cambridge, MA. 
31,271-285. McCullagh, P., 1980," Regression Models for Ordinal Data," Journal of the Royal Sta- 

Mandelbrot, B., and M. Taqqu, 1979, "Robust R/S Analysis of Long Run Serial tistical Society, Series B(42), 109-142. 
Correlation," Bulletin of the International Statistical Institute, 48(Book 2), 59- McCulloch, J., 1971, "Measuring the Term Structure of Interest Rates," Journal of 
104. Business, 44, 19-31. 

Mandelbrot, B., and H. Taylor, 1967, "On the Distribution of Stock Price Differ- , 1975," The Tax-Adjusted Yield Curve," Journal of Finance, 30, 81 1-830. 
ences," Operations Research, 15, 1057-1062. , 1993, "A Reexamination of Traditional Hypotheses About the Term Struc- 

Mandelbrot, B., and J. Van Ness, 1968, "Fractional Brownian Motion, Fractional ture: A Comment," Journal of Finance, 48,7 79-789. 
Noises and Applications," S.I.A.M. Review, 10,422-437. 

McCulloch, J., and H. Kwon, 1993, "US Term Structure Data, 1947-1991,"W orking 
Mandelbrot, B., and J. Wallis, 1968, "Noah, Joseph and Operational Hydrology," Paper 93-6, Ohio State University. 

Water Resources Research, 4, 909-918. 
I McCulloch, W., and W. Pitts, 1943," ALogical Calculus of Ideas Immanent in Nervous 

, 1969a," Computer Experiments with Fractional Gaussian Noises. Parts 1, 2, Activity," Bulletin of Mathematical Biophysics, 5, 1 15-1 33. 
3," Water Resources Research, 5, 228-267. 

McQueen, G., and V. Roley, 1993, "Stock Prices, News, and Business Conditions," 
1969b," Some Long Run Properties of Geophysical Records," WaterResources Reuim of Financial Studies, 6,683-707. 

Research, 5, 321-340. 
Mech, T., 1993, "Portfolio Return Autocorrelation," Journal of Financial Economics, 

Mankiw, N. G., 1986, "The Equity Premium and the Concentration of Aggregate 34,307-344. 
Shocks," Journal of Financial Economics, 17, 21 1-219. 

Mehra, R., and E. Prescott, 1985," The Equity Premium: APuzzle,"J ournalofMonetary 
Mankiw, N. G., and J. Miron, 1986, "The Changing Behavior of the Term Structure Economzcs, 15, 145-1 61. 

of Interest Rates," QuarterlyJ ournal ofEconomics, 101, 211-221. 
Mei, J., 1993, "A Semiautoregression Approach to the Arbitrage Pricing Theory," 

Mankiw, N. G., and M. Shapiro, 1986, "Do We Reject Too Often? Small Sample Journal of Finance, 48, 599-620. 
Properties of Tests of Rational Expectations Models," Economics Letters, 20, 
139-1 45. Melino, A., 1988, "The Term Structure of Interest Rates: Evidence and Theory," 

Journal of Economic Suruqrs, 2, 335-366. 
Mankiw, N. G., and S. Zeldes, 1991, "The Consumption of Stockholders and Non- 

' 
Stockholders," Journal of Financial Economics, 29,97-112. Melino, A., and S. Turnbull, 1990," Pricing Foreign Currency Options with Stochastic 

Volatility,"J ournal of Econometrics, 45, 239-265. 
Mankiw, N. G., D. Romer, and M. Shapiro, 1985, "An Unbiased Reexamination of 

Stock Market Volatility,"J ournal of Finance, 40,677-687. Merton, R., 1969," Lifetime Portfolio Selection under Uncertainty: The Continuous 
Time Case," Review ofEconomics and Statistics, 51, 247-257. 

Manne, H., 1965," Mergers and the Market for Corporate Control," Journal ofPolitical 
Economy, 73,110-120. , 1972, "An Analytic Derivation of the Efficient Portfolio Frontier," Journal of 

Financial and Quantitative Analysis, 7, 1851-1872. 
Mark, N., 1995, "Exchange Rates and Fundamentals: Evidence on Long-Horizon 

Predictability," American Economic Review, 85, 201-218. , 1973a, "An Intertemporal Capital Asset Pricing Model," Econometn'ca, 41, 
867-887. 

Markowitz, H., 1959, Pwtfolio Setkction: EfJient Diversification of Investments, John 
Wiley, New York. , 1973b," Rational Theory of Option Pricing," Bell Journal of Economics and 

Marsh, T., and R. Merton, 1986, "Dividend Variability and Variance Bounds Tests Management Science, 4, 141-183. 

for the Rationality of Stock Market Prices," Amen'can Economic Review, 76, , 1976a," The Impact on Option Pricing of Specification Error in the Under- 
48-98. / lying Stock Price Distribution," Journal ofFinance, 31, 333-350. 

Marsh, T., and E. Rosenfeld, 1986, "Non-Trading, Market Making, and Estimates of 1976b," Option Pricing when Underlying Stock Returns Are Discontinuous," 
Stock Price Volatility,"J ournal ofFinancial Economics, 15, 359-372. Journal of financral Economzcs, 3, 125-1 44. 

I 



References References 5 75 

, 1980, "On Estimating the Expected Return on the Market: An Exploratory Myers, S., and N. Majluf, 1984, "Corporate Financing and Investment Decisions 
Investigation," Journal of Financial Economics, 8, 32.3-361. When Firms Have Information that Investors Do Not Have," Journal of Fi- 

, 1981, "On Market Timing and Investment Performance, I: An Equilibrium nancial Economics, 13, 187-221. 
Theory of Value for Market Forecasts," Journal of Business, 54, 363-406. Naik, V., and M. Lee, 1994, "The Meld Curve and Bond Option Prices with Dis- 

, 1990, Continuous-Time finance, Blackwell Publishers, Cambridge, MA. crete Shifts in Economic Regimes," unpublished paper, University of British 
Columbia and University of Saskatchewan. 

Miccheli, C., 1986, "Interpolation of Scattered Data: Distance Matrices and Condi- 
tionally Positive Definite Functions," Constructive Approximation, 2, 11-22. Neftci, S., 1991, "Naive Trading Rules in Financial Markets and Wiener-Kolmogorov 

Prediction Theory: A Study of 'Technical Analysis',"J ournal of Business, 64, 
Mikkelson, W., and M. Partch, 1986, "Valuation Effects of Security Offerings and the 549-572. 

Issuance Process," Journal of Financial Economics, 15, 31-60. 
Nelson, D., 1990, "Stationarity and Persistence in the GARCH(1,l) Model," Econ& 

Mishkin, F., 1988, "The Information in the Tenn Structure: Some Further Results," metric Themy, 6,318-334. 
Journal of Applied bkonometrics, 3,307-314. 

, 1991, "Conditional Heteroskedasticity in Asset Returns: A New Approach," 
, 1990a, "The Information in the Longer-Maturity Term Structure about Fu- Econometrica, 59,347-370. 

ture Inflation," QuarterlyJ ournal of Economics, 105,815421. 
, 1992, "Filtering and Forecasting with Misspecified ARCH Models I: Getting 

, 1990b, "What Does the Term Structure Tell Us about Future Inflation?," the Right Variance with the Wrong Model," Journal ofEconometrics, 52,61- 
Journal of Monetary Economics, 25, 77-95. 90. 

Mitchell, M., and J. Netter, 1994, "The Role of Financial Economics in Securities , 1996, "AsymptoticallyO ptimal Smoothingwith ARCH Models," Econometrica, 
Fraud Cases: Applications at the Securities and Exchange Commission," 64,561-573. 
The Business Laruyer, 49, 545-590. Nelson, C., and M. Kim, 1993, "Predictable Stock Returns: The Role of Small Sample 

Modest, D., and M. Sundaresan, 1983, "The Relationship between Spot and Fu- Bias," Journal ofFinance, 48,641-661. 
tures Prices in Stock Index Futures Markets: Some Preliminary Evidence," Nelson, C., and A. Siegel, 1987, "Parsimonious Modelling of Meld Curves," Journal 
Journal ofFutuw Markets, 3, 15-42. of Business. 

Modigliani, F., and R. Sutch, 1966, "Innovations in Interest Rate Policy," Ammican Nelson, C., and R. Startz, 1990, "The Distribution of the Instrumental Variables 
Economic Revim,5 6,178-197. Estimator and Its t-Ratio when the Instrument Is a Poor One," Journal of 

Mood, A., 1940, "The Distribution Theory of Runs," Annals ofMathematica1 Statistics, Business, 63, S125-S140. 
11,367-392. Nelson, D., and D. Foster, 1994, "Asymptotic Filtering Theory for Univariate ARCH 

Moody, J., and C. Darken, 1989, "Fast Learning in Networks of Locally Tuned Pro- Models," Econometnca, 62, 1- 41.  
cessing Units," Neural Computations, I ,  281-294. Nelson, D., and K Ramaswamy, 1990, "Simple Binomial Processes as Diffusion A p  

Morrison, D., 1990, Multivariate Statistical Methods, McGraw Hill, New York. proximations in Financial Models," Rmim ofFinancia1 Studies, 3,393-430. 
Morse, D., 1984, "An Econometric Analysis of the Choice of Daily Versus Monthly N m  Yurk Stock ExchangeFact Book: 199.2 Data, 1993, New York Stock Exchange, April. 

Returns in Tests of Information Content," Journal ofAccountingResearch, 22, Newey, W., 1985, "Semiparametric Estimation of Limited Dependent Variable Mod- 
605-623. els with Endogenous Explanatory Variables," Annalesde L'lnsee, 59/60,219- 

Mossin,J ., 1966, "Equilibrium in a Capital Asset Market," Econometnca, 35,768-783. 237. 
Muirhead, R., 1983, Aspects ofMultivariateStatistical7?zeory, John Wiley and Sons, New Newey, W., and K West, 1987, "A Simple, Positive Semi-Definite, Heteroscedasticity 

York. and Autocorrelation Consistent Covariance Matrix," Econometnca, 55,703- 
Murphy, J., 1986, Technical Analysis of the Futures Markets, New York Institute of Fi- 708. 

nance, New York. Niederhoffer, V., 1965, "Clustering of Stock Prices," Operations Research, 13, 258-265. 
Muth, J., 1960, "Optimal Properties of Exponentially Weighted Forecasts," Journal of , 1966, "A New Look at Clustering of Stock Prices," Journal of Business, 39, 

the Amoican Statistical Association, 55, 299-306. 309-31 3. 
Muthuswamy,J ., 1988, "Asynchronous Closing Prices and Spurious Autocorrelations Niederhoffer, V., and M. Osborne, 1966, "Market Making and Reversal on the Stock 

in Portfolio Returns," working paper, Graduate School of Business, Univer- Exchange," Journal of the American Statistical Association, 61,897-916. 
sity of Chicago. Niyogi, P., and F. Girosi, 1996, "On the Relationship between Generalization Er- 

Myers,J ., and A. Bakay, 1948, 'Influence of Stock Split-Ups on Market Price," Haward ror, Hypothesis Complexity, and Sample Complexity for Radial Basis Func- 
Business Review, 25 1-265. tions," Neural Computation, 8,819-842. 



576 References R e f m c e s  577 

Officer, R., 1972," The Distribution of Stock Returns,"J ournalof theAmerican Statistical Petersen, M., and S. Umlauf, 1990, "An Empirical Examination of the Intraday 
Association, 67, 807-81 2. Behavior of the NYSE Specialist," working paper, Massachusetts Institute 

,1973," of Technology, Cambridge, MA. 
The Variability of the Market Factor of the NewYork Stock Exchange," 

Journal of Business, 46,434-453. Phillips, S., and C. Smith, 1980, "Trading Costs for Listed Options: The Implications 
for Market Efficiency,"J ournal ofFinancia1 Economics, 8, 179-201. 

Ogaki, M., 1992," Generalized Method of Moments: Econometric Applications," in 
G. Maddala, C. Rao, and H. Vinod (eds.), Handbook of Statistics, Volume 11: Poggio, T., and F. Girosi, 1990, "Networks for Approximation and Learning," h 
Econometrics, North-Holland, Amsterdam. ceedings ofthe IEEE, special issue: Neural Neborkc I: Theory and Modeling, 78, 

1481-1497. 
O'Hara, M., 1995, M a h t  Microstructure Theory, Blackwell Publishers, Cambridge, 

MA. Poterba, J., and L. Summers, 1986, "The Persistence of Volatility and Stock Market 
Fluctuations," American Economic Review, 76,1142-11 51. 

Oldfield, G., R. Rogalski, and R. Jarrow, 1977," An Autoregressive Jump Process for , 1988," Mean Reversion in Stock Returns: Evidence and Implications," Jour- 
Common Stock Returns," Journal ofFinancial Economics, 6,3 89-418. nal of Financial Economics, 22, 27-60. 

Osborne, M., 1959, "Brownian Motion in the Stock Market," Operations Research, 7, Powell, M., 1987," Radial Basis Functions for Multivariable Interpolation: A Review," 
145-1 73. inJ . Mason and M. Cox (eds.), AlgmonthmsfmAp~ximatioCnla, rendon Press, 

,1962," Periodic Structure in the Brownian Motion of Stock Prices," Operations Oxford, UK. 
Research, 10, 345-379. Prabhala, N., 1995, "Conditional Methods in Event-Studies and An Equilibrium 

Pagan, A., and G. Schwert, 1990, "Alternative Models for Conditional Stock Volatil- Justification for Using Standard Event-Study Procedures," School of Man- 
ity," Journal of Econometrics,4 5, 267-290. agement, Yale University, New Haven, CT. 

Parker, D., 1985, "Learning Logic," Working Paper 47, Center for Computational Praetz, P., 1972, "The Distribution of Share Price Changes," Journal of Business, 45, 
Research in Economics and Management Science, Massachusetts Institute 49-55. 
of Technology. Priestley, M., 1988, Non-Linear and NonStationary Time Series Analysis, Academic Press, 

Parkinson, M., 1980," The Extreme Value Method for Estimating the Variance of the San Diego. 
Rate of Return," Journal of Business, 53, 61-65. Radner, R., 1982, "Equilibrium Under Uncertainty," in K. Arrow and M. Intriligator 

(eds.), Handbook of Mathematical Economics, Volume 11, North-Holland, New 
Paskov, S., and J. Traub, 1995, "Faster Valuation of Financial Derivatives," Journal of 

Portfolio Managemat, 22, 113-120. York, 923-1006. 
Rady, S., 1994, "State Prices Implicit in Valuation Formulae for Derivative Securities: 

Pau, L., 1991, "Technical Analysis for Portfolio Trading by Syntactic Pattern Recog- A Martingale Approach," Discussion Paper 181, LSE Financial Markets 
nition," Journal ofEconomic Dynamics and Control, 15,715-730. Group, London, UK. 

Pearson, N., and T. Sun, 1994," Exploiting the Conditional Density in Estimating the Randles, R., and D. Wolfe, 1979, Introduction to the Theory ofNonparamtric Statistics, 
Term Structure: An Application to the Cox, Ingersoll, and Ross Model," John Wiley and Sons, New York. 
Journal ofFinance, 49, 1279-1304. 

Reddington, F., 1952, "Review of the Principle of Life-Office Valuations," Journal of 
Pennacchi, G., 1991, "Identifjmg the Dynamics of Real Interest Rates and Inflation: the Institute of Actuark, 78, 286340. 

Evidence Using Survey Data," Review ofFinancial Studies, 4, 53-86. Reinsch, C., 1967," Smoothing by Spline Functions," N u w M ath, 10,177-183. 
Perrakis, S., 1986, "Option Bounds in Discrete Time: Extensions and the Price of Restoy, F., and P. Weil, 1993," Approximate Equilibrium Asset Pricing," unpublished 

the American Put," Journal of Business, 59, 119-142. paper, Bank of Spain and ECARE. 
Perrakis, S., and P. Ryan, 1984, "Option Pricing Bounds in Discrete Time," Journal Richard, S., 1978, "An Arbitrage Model of the Term Structure of Interest Rates," 

of Finance, 39, 519-525. Journal of Financial Economics, 7, 38-58. 
Perron, P., 1989," The Great Crash, the Oil Price Shock, and the Unit Root Hypoth- Richardson, M., 1993, "Temporary Components of Stock Prices: A Skeptic's View," 

esis," Econometrica, 57, 1361-1402. Journal of Business and Economic Statistics, 11, 199-207. 
, 1991, "Test Consistency With Varying Sampling Frequency," Econometric The- Richardson, M., and T. Smith, 1991," Tests of Financial Models with the Presence of 

ory, 7,341-368. Overlapping Observations," Review of Financial Studies, 4, 227-254. 

Pesaran, M., and S. Potter, 1992, "Nonlinear Dynamics and Econometrics: An Intro- , 1994, "A Unified Approach to Testing for Serial Correlation in Stock Re- 
duction,"J ournal of Applied Econometrics, 7(Supp), S1-S8. turns," Journal of Business, 67, 371-399. 



References 579 

Richardson, M., and J. Stock, 1989, "Drawing Inferences from Statistics Based on , 1977, "Risk, Return, and Arbitrage," in I. Friend and J. Bicksler (eds.), ~ i s k  

Multi-Year Asset Returns," Journal ofFinancial Economics, 25, 32.3-348. and Return in  Finance I, Ballinger, Cambridge, MA. 

Rietz, T., 1988, "The Equity Risk Premium: A Solution?," Journal of Monetary EC(F Rubin, D., and D. Thayer, 1982, "EM Algorithms for ML Factor Analysis," Psychom- 

nomics, 21, 117-1 32. trika, 57,69-76. 
Rubinstein, M., 1976, "The Valuation of Uncertain Income Streams and the Pricing 

Ritchken, P., 1985, "On Option Pricing Bounds," Journal ofFinance, 40, 1219-1233. 
of Options," Bell Journal ofEconomics, 7, 407-425. 

Ritter, J., 1990, "Long-Run Performance of Initial Public Offerings," Journal of Fi- , 1985, "Nonparametric Tests of Alternative Option Pricing Models Using All 
nance. 

Reported Trades and Quotes on the 30 Most Active CBOE Option Classes 
Robbins, H., and S. Monro, 1951a, "A Stochastic Approximation Method," Annals of from August 23, 1976 through August 31, 1978," Journal ofFinance, 40, 

Mathematical Statistics, 25, 737-744. 455-480. 
, 1951 b, "A Stochastic Approximation Model," Annals ofMathematica1 Statistics, , 1994, "Implied Binomial Trees," Journal of Finance, 49, 771-818. 

22,400-407. 
Rudebusch, G., 1995, "Federal Reserve Interest Rate Targeting, Rational Expecta- 

Roberds, W., D. Runkle, and C. Whiteman, 1996, "A Daily View of Yield Spreads and tions, and the Term Structure," Journal of Monetary Economics, 35, 245-274. 
Short-Term Interest Rate Movements," Journal of Monq! Credit and Banking, Rumelhart, D., G. Hinton, and R. Williams, 1986, "Learning Internal Representation 
28, 34-53. by Error Propagation," in D. Rumelhart and J. McClelland (eds.), Parallel 

Roberts, H., 1959, "Stock-Market 'Patterns' and Financial Analysis: Methodological Distributed Processing: Explorations in  the Microstructure ofcognition, Volume 1: 
Suggestions," Journal ofFinance, 14, 1-10. Foundations, Massachusetts Institute of Technology Press, Cambridge, MA, 

, 1967, "Statistical versus Clinical Prediction of the Stock Market," unpub- chap. 8. 
lished manuscript, Center for Research in Security Prices, University of Ruud, P., 1983, "Sufficient Conditions for the Consistency of Maximum Likelihood 
Chicago, May. Estimation Despite Misspecification of Distribution in Multinomial Dis 

Robinson, M., 1988, "Block Trades on the Major Canadian and U.S. Stock Exchanges: Crete Choice Models," Econometrica, 51, 225-228. 
A Study of Pricing Behavior and Market Efficiency," unpublished Ph.D. Samuelson, P., 1965, "Proof that Properly Anticipated Prices Fluctuate Randomly," 
dissertation, School of Business Administration, University of Western On- Industrial Management Review, 6,41-49. 
tario, Ontario, Canada. , 1967, "Efficient Portfolio Selection for Pareto-LCvy Investments," Journal of 

Robinson, P., 1979, "The Estimation of a Non-Linear Moving Average Model," Financial and Quantitative Analysis, 2, 107-122. 
Stochastic Processes and Their Applications, 5, 81-90. , 1969, "Lifetime Portfolio Selection by Dynamic Stochastic Programming," 

Roll, R., 1977, "A Critique of the Asset Pricing Theory's Tests: Part I," Journal of Review of Economics and Statistics, 51, 239-246. 
Financial Economics, 4, 129-1 76. ,1972, "Mathematics of Speculative Price," in Day, R., and S. Robinson (eds.), 

, 1980, "Orthogonal Portfolios," Journal of Financial and Quantitative Analysis, Mathematical Topics inEconomic Themy and Computation, Society for Industrial 
15,1005-1023. and Applied Mathematics, Philadelphia, PA. 

, 1984, "A Simple Implicit Measure of the Effective Bid-Ask Spread in an , 1973, "Proof that Properly Discounted Present Values of Assets Vibrate Ran- 
Efficient Market," Journal ofFinance, 39, 1127-1 140. domly," BellJournal of Economics and Management Science, 4, 369-374. 

Roll, R., and S. Ross, 1980, "An Empirical Investigation of the Arbitrage Pricing , 1976, "Limited Liability, Short Selling, Bounded Utility, and Infinite- 
Theory," Journal ofFinance, 35, 1073-1 103. Variance Stable Distributions,"J ournal ofFinancia1 and Quantitative Analysis, 

,1984, "A Critical Reexamination of the Empirical Evidence on the Arbitrage 485-503. 
Pricing Theory: A Reply," Journal of Finance, 39, 347-350. Schaefer, S., 1981, "Measuring a Tax-Specific Term Structure of Interest Rates in the 

, 1994, "On the CrossSectional Relation between Expected Returns and Be- Market for British Government Securities," Economic Journal, 91,415431. 

tas," Journal ofFinance, 49, 101-122. , 1982, "Tax-Induced Clientele Effects in the Market for British Government 
Securities," Journal of Financial Economics. 

Romano, J., and L. Thombs, 1996, "Inference for Autocorrelations under Weak 
Assumptions," Journal ofthe American Statistical Association, 91, 590-600. Scheinkman, J., and B. LeBaron, 1989, "Nonlinear Dynamics and Stock Returns," 

Journal of Business, 62, 31 1-338. 
Rosenblatt, F., 1962, Principles of Neurodynamics, Spartan Books, New York. 

Scheinkman, J., and M. Woodford, 1994, "Self-Organized Criticality and Economic 
Ross, S., 1976, "The Arbitrage Theory of Capital Asset Pricing," Journal of Economic Fluctuations," Amen'can Economic Review, 84,417-421. 

Themy, 13, 341-360. 



580 References References 58 1 

Schipper, K., and R. Thompson, 1983, "The Impact of Merger-Related Regulations , 1987a, "Multivariate Proxies and Asset Pricing Relations: Living with the 
on the Shareholders of Acquiring Firms,"J ournal ofAccounting Research, 21, Roll Critique,"J ournal ofFinancial Economics, 18, 91 -1 10. 
184-22 1. , 1987b. "Nonsynchronous Data and the Covariance-Factor Structure of Re- 

, 1985, "The Impact of Merger-Related Regulations Using Exact Test Statis- turns," Journal ofFinance, 42, 221-232. 
tics," Journal ofAccounting Research, 23, 408-415. , 1987c, "A Bayesian Approach to Testing Portfolio Efficiency," Journal of 

Scholes, M., and J. Williams, 1977, "Estimating Betas from Nonsynchronous Data," Financial Economics, 19, 195-21 5. 
Journal ofFinancia1 Economics, 5, 309-328. , 1992a, "The Current State of the Arbitrage Pricing Theory," Journal ofFi- 

Schuss, Z., 1980, TheMy and Application of Stochastic Dzfferential Equations, John Wiley nance, 47, 1569-1574. 
and Sons, 'New York. , 1992b, "On the Estimation of Beta-Pricing Models," Reuzew ofFznancia1 Stud- 

Schwartz, E., 1977, "The Valuation of Warrants: Implementing a New Approach," ies, 5, 1-34. 
Journal ofFinancial Economics, 4, 79-93. Sharpe, W., 1964, "Capital Asset Prices: A Theory of Market Equilibrium under 

Schwert, G., 1981, "Using Financial Data to Measure the Effects of Regulation," Conditions of Risk," Journal ofFinance, 19, 425-442. 
Journal of Law and Economics, 24, 121-1 57. , 1970, Pmtjolzo T h e q  and Capital Markets, McGraw-Hill, New York, NY. 

, 1989, "Why Does Stock Market Volatility Change Over Time?," Journal of Sharpe, W., G. Alexander, and J. Bailey, 1995, Investments, Fifth Edition, Prentice- 
Finance, 44, 11  15-1 153. Hall, Englewood Cliffs, NJ. 

i 

Schwert, W., 1990, "Stock Market Volatility," Financial Analysts Journal, May-June, Shea, G., 1984, "Pitfalls in Smoothing Interest Rate Term Structure Data: Equilib- 
23-34. rium Models and Spline Approximations," Journal ofFinancia1 and @anti- 

Sclove, S., 1983a, "Time-Series Segmentation: A Model and a Method," I n f m t i o n  tative Analysis, 19, 253-269. 
Sciences, 29, 7-25. , 1985, "Interest Rate Term Structure Estimation with Exponential Splines: A 

, 1983b, "On Segmentation of Time Series," in S. Karlin, T. Amemiya, and Note," Journal ofFinance, 40, 319-325. 
L. Goodman, eds., Studies in Econometrics, Time Series, and Multivariate Statis- Shefrin, H., and M. Statman, 1985, "The Disposition to Ride Winners Too Long 
tics, Academic Press, New York. and Sell Lowers Too Soon: Theory and Evidence," Journal of Finance, 41, 

Scott, L., 1985, "The Present Value Model of Stock Prices: Regression Tests and 774-790. 
Monte Carlo Results," Reuimu ofEconomics and Statistics, 67,599-607. , Shephard, N., and S. Kim, 1994, "Stochastic Volatility: Likelihood Inference and 

, 1987, "Option Pricing when the Variance Changes Randomly: Theory, Esti- Comparison with ARCH Models," unpublished paper, Nuffield College, 
mation, and an Application," Journal ofFinancial and Quuntitative Analysis, Oxford, and Princeton University. 
22,419-438. Shiller, R., 1981, "Do Stock Prices Move Too Much to Be Justified by Subsequent 

Securities and Exchange Commission, 1994, Market 2000: An Fxamination of Current Changes in Dividends?," American Economic Reuieu, 71,421-436. 
Equity Ma* Developments, US Government Printing Office, Washington, , 1984, "Stock Prices and Social Dynamics," Bmokings Papers on Economic Activ- 
DC. ity, 2,457-498. 

Sentana, E., 1991, "Quadratic ARCH Models: A Potential Reinterpretation of ARCH , 1989, Market Volatility, Massachusetts Institute of Technology Press, Cam- 
Models as Second-Order Taylor Approximations," unpublished paper, Lon- bridge, MA. 
don School of Economics. , 1990, "The Term Structure of Interest Rates," in B. Friedman and F. Hahn 

Serfling, R., 1980, Approximation T h e m o fMathematical Statistics, John Wiley and (eds.), Handbook ofMonetary Economics, North-Holland, Amsterdam. 
Sons, New York. Shiller, R., and P. Perron, 1985, "Testing the Random Walk Hypothesis: Power Versus 

Shanken,J ., 1982, "The Arbitrage Pricing Theory: Is It Testable?,"J ournal ofFinance, Frequency of Observation," Economics Letters, 18,381-386. 
37,1129-1140. Shiller, R., J. Campbell, and K. Schoenholtz, 1983, "Forward Rates and Future Pol- 

, 1985a, "Multi-Beta CAPM or Equilibrium APT? A Reply," Journal ofFinance, icy: Interpreting the Term Structure of Interest Rates," Bmkings Papers on 
40,1189-1 196. Economic Activity, 1, 173-21 7. 

, 1985b, "Multivariate Tests of the Zero-Beta CAPM," Journal ofFinancial Ece Shimko, D., 1991, "Beyond Implied Volatility: Probability Distributions and Hedge 
nomics, 14, 327-348. Ratios Implied by Option Prices," working paper, University of Southern 

California. 
, 1986, "Testing Portfolio Efficiency When the Zero-Beta Rate Is Unknown," 

Journal ofFinance, 41,269-276. , 1993, "Bounds of Probability," RISK 6,33-37. 



382 References References 5: 

Shorack, G.,a nd J. Wellner, 1986, Empirical Processvs with Applications to Statisttcr,J ohn Stoll, H., and R. Whaley, 1990, "Stock Market Structure and Volatility," Reuiau 
Wiley and Sons, New York. Financial Studies, 3, 37-71. 

Sias, R., and L. Starks, 1994, "Institutions, Individuals and Return Autocorrelations," Strang, C., 1976, Linear Algebra and Its Applications, Academic Press, New York. 
Working Paper, University of Texas, Austin. 

Stuart, A., and K. Ord, 1987, Kendull's Adr~anred7 k e qo JStatistics, Vols. 1-111, Oxfor 
Siege],J ., 1994, Stocks for the Long Run, Norton, New York. University Press, New York. 
Silvey, S., 1975, Stati~ticaIln fcmce, Chapman and Hall, London. Stutzer, M., 1995, "A Simple Nonparametric Approach to Derivative Security Valu 
Simkowitz, M., and W. Beedles, 1980, "Asymmetric Stable Distributed Security Re- ation," working paper, Carlson School of Management, University of Min 

turns," Journal of tht American Statistical Association, 75, 3 0 6 31 2. nesota. 

Sims, C., 1974, "Output and Labor Input in Manufacturing," Brookings Papers on Subba Rao, T., and M. Gabr, 1984, An Introduction to Bispectral Analysis and Bilineaj 
Economic Activity, 3, 695-728. Time Series Models, Springer-Verlag, Berlin. 

, 1977, "Exogeneity and Causal Ordering in Macroeconomic Models," in Suits, D., A. Mason, and L. Chan, 1978, "Spline Functions Fitted by Standard Regres- 
New Methodr in  Business Cyck Research: Proceedingsfrom a Conferace, Federal sion Methods," Reviau oJEconomics and Statistics, 60, 132-139. 
Reserve Bank of Minneapolis. Summers, L., 1986, "Does the Stock Market Rationally Reflect Fundamental Values?," 

Singleton, K., 1990, "Specification and Estimation of Intertemporal Asset Pricing Journal offinance, 41,59 1600. 
Models," in B. Friedman and F. Hahn (eds.), Handbook of Monetary Ecp Sun, T., 1992, "Real and Nominal Interest Rates: A Discrete-Time Model and Its 
nomics, North-Holland, Amsterdam. Continuous-Time Limit," Review of Financial Studies, 5, 581-61 1. 

Smith, A., 1968, The Monpy Game, Random House, NewYork. Sundaresan, S., 1989, "Intertemporally Dependent Preferences and the Volatility of 

Smith, C., 1976, "Option Pricing: A Review," Journal offinancialEconomics, 3, 3-51. Consumption and Wealth," R m i m  ofFinancia1 Studies, 2, 73-88. 
, 1996, Fixed-Incow Securitin, forthcoming. 

Stambaugh, R., 1982, "On the Exclusion of Assets from Tests of the Two Parameter 
Model," Journal ofFinancia1 Economics, 10, 235-268. Svensson, L., 1994, "Estimating and Interpreting Forward Interest Rates: Sweden 

1992-1994," Working Paper 4871, NBER, Cambridge, MA. 
, 1986, "Bias in Regressions with Lagged Stochastic Regressors," CRSP Work- 

ing Paper 156, University of Chicago. Taylor, S., 1986, ModellingFinancial Time Series,J ohn Wiley and Sons, London. 

, 1988, "The Information in Forward Rates: Implications for Models of the Taylor, M., and H. Allen, 1992, "The Use of Technical Analysis in the Foreign Ex- 
Term Structure," Journal ofFinancia1 Economics, 21,41-70. change Market," Journal ofIntmationa1 Monpy and Finance, 11,304-314. 

Teriisvirta, T., D. Tjostheirn, and C. Granger, 1994, "Aspects of Modelling Nonlinear 
Startz, R., 1989, "The Stochastic Behavior of Durable and Non-Durable Consump 

Time Series," in R. Engle and D. McFadden (eds.), Handbook ofEconometrics, 
tion," Review ofEconomics and Statistics, 71, 356363. 

Vol. IV,E lsevier, Amsterdam. 
Stein, E., and J. Stein, 1991, "Stock Price Distributions with Stochastic Volatility: An Thisted, R., 1991, "Assessing the Effect of Allergy Medications: Models for Paired 

Analytic Approach," Review offinancial Studies, 4, 727-753. 
Comparisons on Ordered Categories," Statistics of Medicine, forthcoming. 

Stoker, T., 1986, "Consistent Estimation of Scaled Coefficients," Econowtrica, 54, Thompson, J . ,  and H. Stewart, 1986, Nonlinear Dynamics and Chaos, John Wiley and 
1461-1481. Sons, New York. 

, 1991, "Equivalence of Direct, Indirect and Slope Estimators of Average T in i~S,. , 1972, "The Economics of Liquidity Services," Quarterly Journal ofEconomics, 
Derivatives," in W. Barnett, J. Powell, and G. Tauchen (eds.), Nonpamwt.ric 86,79-93. 
and Semiparametric Methodr in Econometrics and Statistics, Cambridge Univer- 

Tirole, J., 1982, "On the Possibility of Speculation under Rational Expectations," 
sity Press, Cambridge, UK. 

Econowtrica, 50, I 163-1 181. 
, 1992, 1,ectures on Semiparametric Econometrics, CORE Lecture Series, CORE , 1985, "Asset Bubbles and Overlapping Generations," Econometrica, 53, 1499- 

Foundation, Louvain-la-Neuve, Belgium. 1527. 
Stoll, H., 1978, "The Supply of Dealer Services in Securities Markets," Journal of Tong, H., 1983, Threshold Models in Nonlinear Timv Series Analysis, Springer-Verlag, 

finance, 33, 1133-1 151. New York. 
, 1985, The Stock Exchange Specialist System: An  Economic Analysis, Salomon , 1990, Non-linear Time Series: A Dynamic System Appoach, Oxford University 

Brothers Center, New York University, New York. Press, Oxford. 
, 1989, "Inferring the Components of the Bid-Ask Spread: Theory and Em- Treynor, J., and R. Ferguson, 1985, "In Defense of Technical Analysis," Journal of 

pirical Tests," Journal ofFinanr~4, 4, 115-134. finance, 40, 757-773. 



References 

Trzcinka, C., 1986, "On the Number of Factors in the Arbitrage Pricing Model," White, H., and I. Domowitz, 1984, "Nonlinear Regression with Dependent Observa- 
Journal of Finance, 41, 347-368. tions," Econometrica, 52, 143-162. 

Tsay, R., 1986, "Nonlinearity Tests for Time Series," Biometrika, 73,461-466. Widrow, B., and S. Stearns, 1985, AdaptiveSignalProcessing, Prentice-Hall, Englewood 

Tucker, A., 1992, "A Reexamination of Finite- and Infinite-Variance Distributions as Cliffs, NJ. 
Models of Daily Stock Returns," Journal of Business and Economic Statistics, Wiener, N., 1923, "Differential-Space," Journal of Mathematics and Physics, 2, 131-1 74. 
10, 73-81. Wiggins,J ., 1987, "Option Values Under Stochastic Volatility: Theory and Empirical 

Turnbull, S., and F. Milne, 1991, "A Simple Approach to Interest-Rate Option Pric- Estimates," Journal of Financial Economics, 19, 351-372. 
ing," Review of Financial Studies, 4,87-120. Wilcox, D., 1992, "The Construction of U.S. Consumption Data: Some Facts and 

Tversky, A,, and D. Kahneman, 1992, "Advances in Prospect Theory: Cumulative Their Implications for Empirical Work," Ama'can Economic Reoim, 82,922- 
Representation of Uncertainty," Journal of Risk and Uncertainty, 5, 297-323. 941. 

Wood, R., T. McInish, and R Ord, 1985, "An Investigation of Transactions Data for 
Vapnik, V., 1982, Estimation of Dependences Based on Empirical Data, Springer-Verlag, 

Berlin. NYSE Stocks," Journal ofAnance, 40, 723-738. 
Working, H., 1960, "Note on the Correlation of First Differences of Averages in a 

Vasicek, O., 1977, "An Equilibrium Characterization of the Term Structure," Journal 
Random Chain," Econometrica, 28,916918. 

of Financial Economics, 5, 177-1 88. 
Yaari, M., 1987, "The Dual Theory of Choice Under Risk," Econometrica, 55,95-115. 

Vasicek, O., and H. Fong, 1982, "Term Structure Modelling Using Exponential 
Splines," Journal ofFinance, 37, 339-341. Zarowin, P., 1989, "Short-Run Overreaction: Size and Seasonality Effect," Journal of 

Portfolio Management, 15, 2629. 
Vayanos, D., 1995, "Transaction Costs and Asset Prices: A Dynamic Equilibrium 

Zehna, P., 1966, "Invariance of Maximum Likelihood Estimation," Annals of Mathe- 
Model," unpublished paper, Stanford University. 

matical Statistics, 37, 744-744. 
Volterra, V., 1959, Theory ofFunctionals and of IntegreDifferential Equations, Dover, New 

York. 
Wahba, G., 1990, Spline Models for Obseruational Data, Regional Conference Series in 

Applied Mathematics, Vol. 59, SIAM Press, Philadelphia. 
Wallis, W., and H. Roberts, 1956, Statistics: A New Appoach, Free Press, NewYork. 
Wang, J., 1993, "A Model of Intertemporal Asset Prices Under Asymmetric Informa- 

tion," Review ofEconomic Studies, 60, 249-282. 
, 1994, "A Model of Competitive Stock Trading Volume," Journal ofPolitica1 

Economy, 102,127-168. 
Weil, P., 1989, "The Equity Premium Puzzle and the Risk-Free Rate Puzzle," Journal 

of Monetary Economics, 24, 401- 42 1. 
West, R, 1988a, "Bubbles, Fads, and Stock Price Volatility Tests: A Partial Evaluation," 

Journal of Finance, 43, 639-656. 
, 1988b, "Dividend Innovations and Stock Price Volatility," Econometrica, 56, 

37-61. 
Wheatley, S., 1988, "Some Tests of the Consumption-Based Asset Pricing Model," 

Journal of Monetary Economics, 22, 193-218. 
White, H., 1980, "A Heteroskedasticity-Consistent Covariance Matrix Estimator and 

a Direct Test for Heteroskedasticity," Economtrica, 48, 817-838. 
, 1982, "Maximum Likelihood Estimation of Misspecified Models," Economt- 

rica, 50, 1-25. 
, 1984, Asymptotic Theory for Econometricians, Academic Press, Orlando, FL. 
, 1992, Artijicial Neural Networks: Approximation and Learning Theory, Blackwell 

Publishers, Cambridge, MA. 



Author Index 

Ait-Sahalia, 340, 370, 392,451, 507, Bakay, 150 
510-512 Baker, 150 

Abel, 260, 327, 328 Balduzzi, 423 
Acharya, 175 Ball, C., 107-109, 117, 122, 123, 177, 
Adams, 411  379,392 
Admati, 99 Ball, R., 150 
Ameck-Graves,1 07,208 Banz, 21 1,509 
Ainslie, 334 Barberis, 333 
Aitchison, 123 Barclay, 176 
Aiyagari, 316 Barr, 443,445 
Aldous, 41 Barron, A,, 512 
Alexander, G., 155 Barron, R., 512 
Alexander, S., 42,65 Barsky, 283 
Allen, 44 Barton, 41 
Amihud, 103,104,107,316 Basu, 21 1 
Amin, 379, 381 Beckers, 379 
Ammer, 421,445 Beedles, 17 
Andersen, 472 Benartzi, 333 
Anderson, 192 Bernard, 167 
Andrews, 535 Bernstein, 3, 20, 339 
Arnold, 357 Bertola, 423 
Arrow, 507 Bertsimas, 99, 107, 365, 366 
Aschauer, 326 Bick, 507 
Ashford, 123 Bierwag, 406 
Ashley, 150 Billingsley, 341, 344 
Asquith, 174, 179 Black, 23, 182, 211, 339, 350, 351, 
Atchison. 85 354, 356, 367, 379, 442, 455,457, 

462,464,485,497,510,523 
Bachelier, 16, 20, 32, 341 Blanchard, 259 
Backus, 429, 435,454,455,457,465 Blattberg, 17,208, 379 
Bagehot, 103,107 Bliss, 418, 422 
Bailey, 155 Blume, L., 44 



Author Zndex Author Index 

Blume, M., 42, 43, 65, 100, 178, 21 1, Christie, A., 379, 497 Dickey, 65 Frees, 117, 121-123 
323,324 Christie, W., 107, 110 Dimson, 85 French, 72, 78, 79, 157, 211,212, 

Boehmer, 167 Chung, C., 504 Ding, 486 240,241,248,249,266268,274, 
Boldrin, 474 Chung, K., 6 Dolley, 149 366,421,497 
Bollerslev, 381,481,483,488, 489, Cochrane, 48,49,52,274,302, 327, Domowitz, 54 Friedman, 518 

491-494,496 330,332 Donaldson, 283 Friend, 21 1,220,323,324 
Bonomo, 334 Cohen, 84,85,88,104,107 Duffie, 8, 318, 380, 441 Froot, 259,288,333,424 
Boudoukh, 79,134 Collins, 167 Dufour, 55 Fuller, 45, 46, 65 
Box, 47,140 Cone, 11 0 Dunn, 327 Furbush, 110 
Brainard, 31 7 Connor, 221,237-241 Dupire, 507 
Braun, 494 Constantinides, 316, 318, 326,327, Durlauf, 276278 Gabr, 472 
Breeden, 221,298,306,317,508,509 330,442,507 Dybvig, 220,221,351,433,440,443, Galai, 103, 107 
Breen, 251 Cooper, 409 444,452,456,464 Gallant, A., 522, 535 
Brennan, 108,381,441,449 Cootner, 65 Galli, 84 
Brenner, 452,455 Copeland, 103, 107, 108 Easley, 44, 99, 103, 107, 140 Garber, 258 
BricMey, 179 Corrado, 172,173 Eckbo, B., 179 Garcia, 334 
Brieman, 7 Cowles, 20,35-37,65 Eckbo, E., 175 Garman, 380,481,507 
Brock, 44,470,474,478,479 Cox, D., 7, 140 Edwards, 43 Gatto, 385, 391 
Brodsky, 472 Cox, J., 31, 318, 340, 341, 349, 355, Eichenbaum, 326 George, 107,135 
Bronfman, 136 379,380, 382,414,429,435,436, Eikeboom, 102 Gertler, 316 
Broomhead, 516 440,441, 443, 444, 449,456, 458, Einstein, 16, 32 Gibbons, 193, 196, 199, 206, 245, 
Brown, D., 44 463,508,509 Engle, 54, 257, 381, 469, 481, 482, 246,298,317,445,446,448,455 
Brown, P., 150 Crack, 113 485,486,489,492,494,496,531 Gilles, 275 
Brown, R., 430,441,443,452,455 Craig, 474 Epstein, 305, 315, 319, 320, 334 Giovannini, 84,320 
Brown, S., 150, 154, 157, 171, 177, Curcio, 107, 109 Estrella, 424 Girosi, 512, 516, 517, 522 

311,443,452 Cutler, 317 Eytan, 92 Gleick, 473 
Burnside, 304 Glosten, 101-103, 106, 107, 135, 486, 
Butler, 85 Dahm, 123 Fabozzi, 396, 405,406 488,497 

Dann, 180 Fama, 20,22,41-43,54, 65, 78, 79, Godek, 110 
Campbell, C., 173 Darken, 516 150, 157, 208, 211, 212, 215, 221, Goetzmann, 311  
Campbell, J., 48,49,214,221,239, David, 41 240,241,248,249,266-268,274, Goldberger, 504 

255,257,258,261-264,268,273, Davis, D., 84 379,418,421,422,424 Goldenberg, 379 
274, 278, 281, 283, 285, 286, 311, Davis, M., 316 Fang, 387,388 Goldman, 385,391 
313-315,317,318,320,321, Day, 474 Faust, 48,49, 52 Goldstein, 102 
323-327,330,332,372,395,408, Deaton, 504 Feller, 7 Gonedes, 17,208,379 
415, 419, 421,422,435, 443, 445, DeBondt, 212 Ferguson, 44 Gonzalez-Rivera, 489 
448,494,497 Debreu, 507 Ferson, 327,448,534 Goodhart, 107, 109 

Carleton, 409 Dechert, 470,478,479 Fielitz, 17 Gordon, 256 
Carlstein, 472 De Long, 283 Fischel, 110 Gottlieb, 121-123 
Cecchetti, 304,310 DeLong, 317,333 Fisher, 85, 150 Graham, 114 
Chamberlain, 92,229, 238,504 Demsetz, 103,107 Fishman, 387 Grandmont, 474 
Chan, K., 110,449 Dent, 167 Flavin, 279 Granger, 17, 59, 60, 65,91, 257, 470, 
Chan, L., 107,410 Dennan, 442,455,457,464,507 Foerster, 534 472,486 
Chen, N., 239,240,424 Dhrymes, 220 Fong, 412 Granito, 406 
Cho, 117,121-123 Diaconis, 41 Foresi, 423,457 Grassberger, 47-78 
Choi, 102 Diamond, 260 Foster, 381, 485 Gray, 452, 455 
Chou, 381,481 Diba, 259 Frankel, 84 Gregory, 435 



Author Index Author Index 

Grinblatt, 221 Hodrick, 267-270, 274, 285, 286, Kahneman, 333 Lee, T., 123 
Grossman, H., 259 448,536 Kalay, 121-123 Lehmann, 236,238,240,241,246, 
Grossman, S., 15, 24, 110, 305, 306, Hoel, 7, 346 Kalos, 387 284 

308,317,318,335 Hofmann, 379 Kamstra, 283 Leland, 507 
Gultekin, B., 220 Hogarth, 332 Kan, 441 LeRoy, 23,256,275,276, 279,280 
Gultekin, M., 220 Holden, 474 Kandel, E., 110 Leroy, 31, 102,508 
Gultekin, N., 369 Holt, 84 Kandel, S., 200, 215, 217, 221, 231, Levy, 17 
Gurland, 123 Hornik, 522 286,309,310 Lilien, 494 

Hosking, 59, 60 Kane, 168,396 Lintner, 14, 156, 181 
Hagerman, 17 Hsieh, 474,475,479 Kani, 507 Litzenberger, 8, 176, 216, 298, 317, 
Hakansson, 507 Hsu, 17 Karasinski, 442 412,461,508,509 
Hald, 30 Huang, C., 8,380,391,461,508 Karolyi, 449 Ljung, G., 47 
Hall, A., 527 Huang, R., 107,110,135 Kaufman, 406 LO, 20,47-49,52-55, 63,64, 74, 76, 
Hall, R., 276, 278, 305, 311 Huberman, 221,231 Kaul, 107, 135 78, 79, 84, 85, 89,93-95,99, 107, 
Hamilton, 7, 257, 360, 372, 451, 472, Hull, 340, 349,378-381, 459,464, Keim, 100,107,267,421 109, 122-124, 128,130, 133, 

473,481,527,535,536 510 Kendall, 65, 273 136138,143,212,248,249,251, 
Hammersley, 386388 Hurst, 59, 62 Kennan, 475 340,359,365,366,370,371,374, 
Hampel, 523 Hutchinson, 340, 392,510,512,519, Kim, M., 79,274 375, 377, 379, 392, 472, 504, 507, 
Handscomb, 387 522 Kim, S., 490 510-512,519,522-524 
Hansen, B., 488 Kindleberger, 258 Loewenstein, 334 
Hansen, L., 269, 270, 292, 295, 296, Ingersoll, 8, 210, 295, 318, 351, 380, Klass, 481 Longstaff, 392, 429, 437-439, 442, 

301, 302,304, 306, 309,311, 314, 382,406,414,429,433,435,436, Kleidon, 110,277,278 444,449,455,507 
315,326,332,359-361,392,448, 440, 441, 443, 444, 449, 456, 458, Knuth, 114 Lorenz, 473 
527,531,536,540 463 Kocherlakota, 302, 310 Lowe, 516 

Hirdle, 501,502 Irish, 504 Kogan, 365,366 Lucas, D., 316, 318 
Hardouvelis, 424 It6, 348 Kohlase, 474 Lucas, R., Jr., 9, 31, 102, 508 
Ha jes, 452,455 Koo, 321 Lumsdaine, 488 
Harpaz, 92 Joreskog, 234 Korajczyk, 237-241,251 Luttmer, 304, 315,316 
Harris, J., 107, 110 Jackwerth, 370,507 Korkie, 196, 223, 224 Lutz, 413,418 
Harris, L., 103, 107-109, 121-123, Jacquier, 490 Kothari, 212,251 

135 Jagannathan, 214, 292, 296,301, Kreps, 31,319,332,355,380,508 Macaulay, 403,420 
Harrison, 31,355, 380,508 304,309,315,486,488,496,497 Krogh, 512 MacBeth, 211,215,369 
Harvey, A., 490,493 Jain, 177 Kroner, 381,452,455,481,491 Mackay, 258 
Harvey, C., 217, 314,494-497 James, 180 Kwon, 397,415,420,451,453 MacKinlay, A. C., 47-49, 52-55, 74, 
Hasbrouck, 107 Jamshidian, 463 Kyle, 99, 317 76, 78, 79,85,89,93-95,99, 130, 
Hausman, 50,51,109,122-124,128, Jarrell, 178, 179 133,193,210,212,245,248-251, 

136138, 143 Jarrow, 455, 457,458,464 Laibson, 334 366,379,472,504,510,523,524 
Hawawini, 85 Jegadeesh, 21 2,266,274 Lakonishok, 44,107,248,249 MacKinlay, C., 109, 122-124, 128, 
He, H., 315,316,507 Jennings, 44 Lam, 304,310 136138,143 
Heath, 455, 457,458,464 Jensen, 150,179,211 Lanen, 175 Maddala, 123 
Heaton, 304, 316, 318,327, 332, 360 Jerison, 348 Lang, 6 Madhavan, 107 
Hegde, 107 Jobson, 196,223,224 Leamer, 472,523 Magee, 43 
Hentschel, 485,488,497 Johnson, 379 LeBaron, 44,475,479 Magnus, 6 
Hertz, 512 Jones, H., 35-37,65 Ledoit, 113  Maier, 84, 85,88, 104, 107 
Heston, 379,446,448 Joyeux, 59,60 Lee, C., 136 Majluf, 179 
Hicks, 418 Lee, M., 452,455 Maksimovic, 175 
Ho, 103, 104, 107, 455-457, 464 Lee, S., 455-457,464,488 Malatesta, 167, 175 



Author Index Author Index 

Malgrange, 474 Modigliani, 418 Pau, 44 Rolfo, 412 
Malkiel, 20 Monahan, 535 Pearson, 438,441 Roll,17,72,1 01-103,106, 128, 134, 
Mandelbrot, 17, 54, 59, 62, 63, 65, Monro, 515 Pennacchi, 445 135, 143, 145, 150, 184, 213, 216, 

379 Mood, 39,41 Perold, 507 239,240,243,366 
Mankiw, L., 260 Moody, 516 Perron, 366,372,472 Roma, 379 
Mankiw, N. G., 48,49,278,289,313, Morganstern, 17,65 Pesaran, 475 Romano, 47 

317,318,320,326,337,422,423 Morrison, 235,238 Pfleiderer, 99 Romer, 278,289 
Manne, 178 Morse, 175 Phillips, P., 277 Rosenfeld, 121,123-125, 143 
Mark, 274, 304,310 Morton, 455, 457, 458, 464 Phillips, S., 100 Ross, D., 110 
Markowitz, 181 Mossin, 14 Pierce, 47 Ross,S.,31, 156, 193, 196,206,216, 
Marsh, 121, 123-125, 143,277, 278, Muirhead, 192, 193 Pitts, 512 219, 220, 239, 240, 245, 246, 311, 

289 Muller, 472 Platen, 379 318, 341, 355, 380, 382, 414,429, 
Marx, 110 Mullins, 174, 179 Poggio, 340, 392,510, 512, 516, 517, 433, 435,436, 440,441, 443, 444, 
Mason, A., 410 Musumeci, 167 519,522 449,456,458,463,508,509 
Mason, S., 507 Muth, 56 Polson, 490 Rossi, 490 
Mauldon, 388 Myers, J., 150 Port, 7, 346 Roth, 84 
Mayers, 214 Myers, S., 179 Porter, 275, 276, 280 Rothschild, 92,238,491 
McCallum, 425 Porteus, 319 Roy, 55 
McCullagh, 123 Naik, 452, 455 Poterba, 48,49, 78, 79, 266,317 Rozell, 17 
McCulloch,J ., 397, 410, 411, 414, Neftci, 44 Potter, 475 Ruback, 179 

415,417,420,451,453 Nelson, C., 79, 274, 313, 413 Poulsen, 167, 178 Rubin, 234 
McCulloch, R., 217 Nelson, D., 381,481,484-486,488, Powell, 516, 517 Rubinstein, 340, 341, 349, 351, 370, 
McCulloch, W., 512 489,494 Prabhala, 175 381,461,507,509 
McDonald, 208 Nelson, W., 317 Prelec, 334 Rudebusch, 423 
McQueen, 149 Netter, 149, 179 Prescott, 293, 302, 303, 307, 332, 334 Ruiz, 490, 493 
Mech, 134 Neudecker, 6 Priestley, 470, 471 Runkle, 423, 486, 488, 497 
Mehra, 293,302,303,307,332,334 Newey, 55,130,269,270,535,536 Procaccia, 476-478 Ruud, 504 
Mei, 237 Ng, 379,381,485,486,491 
Melino, 308, 317,396,489,490 Niederhoffer, 107, 109 Radner, 508 Salandro, 102 
Mendelson, 103,104,107,316 Nimalendran, 107, 135 Rady, 507 Samuelson, 17,20,23,30,2 56, 321 
Merton, 6,8,1 84, 219,221,277,278, Norman, 316 Ramaswamy, 216,381,445,446,455 Sanders, 449 

289, 315, 318, 322, 339, 340, 346, Ready, 136 Sayers, 474 
349-351,354,365,392,481,507, O'Brien, 475 Reddington, 405 Schaefer, 412,430,441,443,452,455 
510 O'Hara, 44,84,99, 103, 107, 140 Reder, 332 Scheinkman, 359-361,392,470,475, 

Merville, 369 Obstfeld, 259, 288 Reinsch, 522 478,479 
Miccheli, 517 Officer, 17,481 Richard, 295 Schipper, 167,180 
Mikkelson, 179 Ogaki, 360,527 Richardson, 47-49,58,79,134,210, Schoenholtz, 408,421,422 
Milgrom, 101-103,107 Ord, 7, 17 274,422 Scholes, 85, 88, 177, 211, 339, 350, 
Miller, H., 7 Osborne, 65,107,109 Rietz, 310, 311 351,354,356,367,462,510 
Miller, M., 110,509 Ritter, 156 Schultz, 107 
Miller, R., 17, 107 Pagan, 485 Robbins, 515 Schuss, 346 
Milne, 463 Palmer, 51 2 Roberds, 423 Schwartz, E., 429,438,439,441,442, 
Miron, 423 Papell, 474 Roberts, 22, 30, 65 444,449,455 
Mishkin, 424 Parkinson, 481 Robins, 494 Schwartz, R., 84,85,88, 104, 107 
Mitchell, 149, 179 Partch, 179 Robinson, P., 471 Schweizer, 379 
Modest, 92, 236, 238, 240, 241, 315, Paskov, 388 Rogalski, 369 Schwert, G., 149,485,497 

316 Patashnik, 114 Roley, 149 Sclove, 472 



Authm Index Author Index 

Scott, 276,379 Stinchcombe, 522 Unal, 168 White, A., 378-381, 464 
SEC, 84,108 Stock, 48,49,58, 79, 274,422 White, H., 54, 174, 489, 512, 515, 
Sentana, 497 Stoker, 504,505 van Deventer, 411  522,527,536,539 
Shanken, 85,193,196,199,200,206, Stoll, 103-105, 107, 110, 135 Vasicek, 412, 429, 432, 434, 441, Whitelaw, 134 

212,215-217,220,222,226,233, Stone, 7, 346 449 Whiteman, 423 
245,246,251 Strang, 6 Vayanos, 316 Whitlock, 387 

Shanno, 379 Stroock, 348 Vishny, 248,249, 333 Wichern, 17 
Shapiro, J., 107 Stuart, 7, 17 Volterra, 471 Wiener, 348 
Shapiro, M., 278, 289, 317, 320, 422 Stuetzle, 518 Wiggins, 378-381,489,490 

Wahba, 523 
Sharpe, 14, 155, 156, 181 Subba Rao, 472 Wilcox, 316 

Waldmann, 317, 333 
Shastri, 102 Suits, 410 Williams, J., 85, 88, 175, 177 

Wallis, J., 59, 63 
Shea, 412 Summers, 48, 49, 78, 79, 260, 265, Williams, R., 6 

Wang,J ., 92,99,371, 373-375, 377 
Shephard, 490,493 266,317,333 Willig, 11 0 

Wang, Y., 387,388 
Shiller, 255,257,258, 261-263,265, Sun, 429,435,438,441 Woodford, 474,475 

Wang, Z., 214, 496 
267, 275, 276, 278, 281, 283, 305, Sundaresan, M., 92 Wooldridge, 488,489,491,494,497 

Warner, 150,154,171,177 
306,308,317,318,366,395-397, Sundaresan, S., 326, 327, 330, 396 Working, 65 

Wasley, 173 
408,419,421,422,443,445 Sunier, 494 Watson, 259 

Shimko, 370,577 Sutch, 418 Yaari, 334 
Weil, P., 305, 310, 315, 319, 320 

Shleifer, 248, 249, 317, 333 Svensson, 413 Weil, R., 406 Zeckhauser, 260 
Sias, 134 Weinstein, 157 Zehna, 367 
Siegel, A., 413 Taqqu, 63 West, 55, 130, 258, 269, 270, 275, Zeldes, 31 7 
Siegel, J., 311  Taylor, H., 17 278,280,289,535,536 Zhou, G., 217 
Siegmund, 472 Taylor, M., 44 Whaley, 107 Zhou, Z., 335 
Silvey, 7, 123, 358 Taylor, S., 485 Wheatley, 317 Zin, 305, 315,319,320,334,435, 
Simkowitz, 17 Terkvirta, 470 Whitcomb, 84, 85, 88, 104, 107 454,455,457,465 
Simonds, 85 Thaler, 212, 333 
Sims, 91 Thayer, 234 
Singer, 348 Thisted, 123 
Singleton, 306, 311, 314, 326, 327, Thombs, 47 

332,418,429 Thompson, J., 474 
Skelton, 406 Thompson, R., 167, 175, 180 
Sloan, 21 2, 251 T in i~1,0 3, 107, 369 
Smidt, 107 Tirole, 259, 260 
Smith, A,, 23 Titrnan, 212,221 
Smith, C., 100 Tjostheim, 470 
Smith, J., 110 Toevs, 406 
Smith, T., 47, 79 Tong, 470,472 
Sofianos, 107 Torous, 107-109,177,392 
Sosin, 385, 391 Toy, 442,455,457,464 
Stambaugh, 100, 178, 214, 215, 217, Traub, 388 

267, 273, 286, 309, 310, 421, 422, Treynor, 44 
446,448,497 Tsay, 476 

Starks, 134 Tschoegl, 107-109 
Startz, 313,326 Tucker, 17 
Steigerwald, 279 Tufano, 507 
Stewart, 474 Turnbull, 463,489,490 
Stiglitz, 15, 24 Tversky, 333 



Subject Index 

absolute value GARCH model, 485 option pricing, 381 
activation function, 513 arithmetic Brownian motion, 32, 
affine-yield models of the term 344. See also Brownian motion 

structure, 428,441,445 Arrow-Debreu securities, 507 
aggregate consumption artificial neural network, 512. See also 

aggregation, 305 learning networks 
Consumption Capital Asset Asian options, 382 

Pricing Model, 316 ask price, 83 
American option, 349 asymptotic distribution 
amplitudedependent exponential GMM estimator, 533 

auroregression (EXPAR) models, N estimator, 529 
- 470 ML estimator, 350, 538 
antipersistence, 60 asymptotic order, 343 
antithetic variates method, 388 asymptotically efficient estimator, 
arbitrage opportunities, 339 358,530 

state price vector, 295 autocorrelation coefficients, 44, 66, 
bond excess returns, 414 145 
Merton's approach to option autocorrelation matrices, 75, 76, 

pricing, 351 131. See also cross-autocorrelation 
arbitrage portfolios, 351 autocovariance coefficients, 45 
Arbitrage Pricing Theory (APT), 8, autocovariance matrices, 74 

85,92, 219. See also Capital Asset Autoregressive Conditionally 
Pricing Model, muitifactor Heteroskedastic models, 469, 
models 482. See also GARCH models 

exact factor pricing, 221 average derivative estimators, 505 
factor risk exposure, 221 average rate options, 382,386 
pervasive factors, 221 
riskfree return, 220 backpropagation, 515  
welldiversified market portfolio, bandwidth, 500 

22 1 optimal bandwidth selection, 502 
ARCH models, 469, 482. See also barrier models, 121 

GARCH models barrier options, 391 



Subject Index Subject Index 

Bayesian inference, 7 estimator for o', 361 ceiling function, 11 4 correlation integral, 477 
BDS test, 479 geometric, 347 chaos theory, 473. See also cost of capital estimation, 183 
BEKK model, 491 properties, 344 deterministic nonlinear coupon bonds, 396,401 
benchmark portfolio, 298 bubbles, 258 dynamical systems convexity, 406 
Berkeley Options Database, 107 bullish vertical spread, 509 clientele effects, 41 2 coupon rate, 401 
Bernoulli distribution, 18 Butterfly Effect, 473 closeness indicator, 477 duration, 403 
beta, 155,182,496 CobbDouglas utility, 326 effective duration, 406 
bias call option, 349 coefficient functions, 356 forward rates, 408 

finite-sample bias in long-horizon callable bond, 395 cointegration, 257 immunization, 405 
regressions, 273 Capital Asset Pricing Model ( W M ) ,  the term structure of interest loglinear model, 406 

bid price, 83 14, 181. See also Arbitrage Pricing rates, 419 Macaulay's duration, 403 
bid-ask bounce, 101, 134 Theory, Intertemporal Capital complete asset markets, 295 modified duration, 405 
bid-ask spread, 99, 146, 147 Asset Pricing Model, compound options, 391 price, 401,409 

adverse-selection cost component, data-snooping biases, conditional volatility models. See yield to maturity, 401 
103 mean-variance efficient-set ARCH models, GARCH models covariance stationarity, 484 

estimating the effective bid-ask mathematics, multifactor models, connection strength, 513 Cowles-Jones ratio, 35. See also 
spread, 134 sample selection biases consistent and uniformly Random Walk 1 model 

inventory cost component, 103 anomalies, 211  asymptotically normal (CUAN) Cox, Ingersoll, and Ross model, 436 
order-processing cost component, applications, 183 estimators, 358 Cox-Ross option pricing technique, 

103 Black version, 182, 196 constant-correlation model, 492 390. See also risk-neutral 
bilinear model, 471 book-market effect conditional, constant-expected-return hypothesis, option-pricing method 
binary threshold model, 512 496 255 cross-autocorrelation, 74,75,84,1 29. 
binomial tree for the short-term cross-sectional reg- ression tests, and vector autoregressive See also autocorrelation matrices 

interest rate, 442 215. See also errors-in-variables methods, 281 cross-sectional models, 173 
birth and death options, 391 heteroskedasticity, 208 and volatility tests, 276 cross-sectional restrictions on the 
Black-Scholes and Merton option intertemporal equilibrium - Consumption Capital Asset Pricing term structure, 452 

pricing model, 339, 350. See also models, 323 Model (CCAPM), 304 cross-validation, 502 
option pricing models January effect, 100 aggregate consumption and, 316 crude Monte Carlo, 386 

estimator for a, 364 non-normality, 208 Epstein-Zin-Weil recursive utility curse of dimensionality, 504 
adjusting the Black-Scholes nonsynchronous trading, 85 model, 319. See also 

formula for predictability, 375 option pricing, 351 Epstein-Zin-Weil model data-snooping, 212,240, 246, 249, 
assumptions, 350 power of tests, 204 instrumental variables (IV) 251,523 
Black-Scholes formula, 352,371, priceearningsratio effect, 21 1 regression, 311  default risk, 406 

373,519 Sharpe-Lintner version, 182, 189 investor heterogeneity, 31 7 degrees of freedom, 523 
CAPM, 351 size effect, 211 ,496 power utility, 305. See also Delta of an option, 353,512 
deterministic volatility, 379 size of tests, 203 lognormal asset pricing models delta method, 51, 540 
estimator for a2,3 61, 374, 375 temporal dependence, 208 substituting consumption out of delta-hedging, 512, 522 
stochastic volatility, 380 unobservability of the market the model, 320 derivative securities, 339,455. See 
implied volatility, 377 portfolio, 213,216. consumption growth, 311 ,434 also fixed-income derivative 
option sensitivities, 354 CAPM. See Capital Asset Pricing consumption of stockholders and securities, option pricing 

borrowing constraints, 315 Model nonstockholders, 31 7 forward contract, 458 
Box-Cox transformation, 140 catching up with the Joneses, 327, con tinuous-record asymptotics, 364 futures contract, 459 
Box-Pierce Q-statistic, 67 328. See also habit formation contrarian investment strategies, 76 deterministic nonlinear dynamical 
Brock-Dechert-Scheinkman test, 478 models control variate method, 387 systems, 473 
Brownian motion, 344 Cauchy distribution, 18 convexity, 406 logistic map, 525 

arithmetic, 32, 344 CCAPM. See Consumption Capital correlation coefficient, 44 sensitivity to initial conditions, 473 
estimator for a,3 64 Asset Pricing Model correlation dimension, 478 tent map, 474,476,525 



Subject Index Subject Index 

testing. See testing for efficiency. See a..ymptotic efficiency event-study analysis, 149. See also exotic securities, 391 
deterministic nonlinear Efficient Markets Hypothesis nonparametric tests expansion of the states, 357 
dynamical processes (EMH), 20 abnormal return, 150, 151 EXPAR models, 470 

difference-stationary process, 65, Semistrong-Form Efficiency, 22, 30 Arbitrage Pricing Theory, 156 expectations hypothesis (EH), 413, 
372. See also unit root process Strong-Form Efficiency, 22, 30 Capital Asset Pricing Model, 156 418,419. See also pure 

diffusion function, 356 Weak-form Efficiency, 22, 30 clustering, 166 expectations hypothesis, term 
discount bonds, 396, 397 EGARCH model, 486,488 constant-mean-return model, 151, structure of interest rates 

estimating the zerocoupon term EH. See expectations hypothesis 154 empirical evidence, 418 
structure, 409 elasticity, 405 cross-sectional models, 173 log expectations hypothesis, 432, 

forward rate, 399 elasticity of intertemporal cumulative abnormal return, 160 437 
holding-period return, 398 substitution, 305 earnings-announcement example, preferred habitats, 418 
immunization, 405 hyperbolic discounting and, 334 152 yield spreads, 418 
term structure of interest rates, separating risk aversion from estimation window, 152 expected discounted value. See 

397 intertemporal substitution, 319 event window, 151 discounted value 
yield curve, 397 the riskless interest rate and, 309 event-date uncertainty, 176 exponential GARCH model, 486, 
yield spread, 397 embedding dimension, 476 factor model, 155 488 
yield to maturity, 397 EMH. See Efficient Markets generalized method of moments, exponential spline, 412 

discount function, 410. See spline Hypothesis 154,174 
estimation Epstein-Zin-Weil recursive utility inference with changing variances, face value, 396 

discounted value model, 319 167 factor analysis, 234 
of future dividends, 256 consumption-wealth ratio, 321 law and economics, 149 factor model, 155. See also 
of the stock price, 255 cross-sectional asset pricing legal liability, 149, 179 multifactor models 

discrete-time models formula, 322 market model, 151, 155, 158 fair game. See martingale 
of option pricing, 381 equity premium puzzle, 323 market-adjusted-return model, fat tail, 16, 480. See also kurtosis 
of stochastic volatility, 489 factor asset pricing model, 324 156 finitedimensional distributions 

discretization, 383, 385 substituting consumption out of nonsynchronous trading, 177 (FDDs), 344,364 
distribution. See asymptotic the model, 320 normal return, 151 Fisher information matrix. See 

distribution, returns equity premium puzzle postevent window, 157 information matrix 
dividend-price ratio, 264,268 catching up with the Joneses sampling interval, 175 fixed-income derivative securities, 
dividend-ratio model, 263 model, 328 skewness of returns, 172 455 
dividends, 12,254 Hansengagannathan volatility standardized cumulative abnormal Black-Scholes formula, 462 
double bottoms. See technical bound, 302 return, 160 Heath-JarrowMorton model, 457 

analysis lognormal asset pricing model test power, 168 Ho-Lee model, 456 
down and out options, 391 with Epstein-Zin-Weil utility, exact factor pricing, 221 homoskedastic single-factor 
drift, 31, 356 323 interpreting deviations, 242 model, 463 
dualcurrency options, 391 lognormal asset pricing model mean-variance efficient set option pricing, 461 
dual-equity options, 391 with power utility, 307 mathematics, 243 term structure of implied volatility, 
durable goods, 326, 332 equity repurchases, 256, 287 nonrisk-based alternatives, 248 463 
duration, 403. See also coupon bonds equivalent martingale measure, 355, optimal orthogonal portfolio, 243, fixed-income securities, 395 
duration of nontrading, 87 383,508 245,248 floor function, 114 
dynamic hedging strategy, 521 errors-in-variables, 216 risk-based alternative, 247 Fokker-Planck equation, 359 
dynamic trading strategies, 352, 391 Euler equation, 293, 508. See also Sharpe ratio, 245,247,248, 252 foreign currency, 5, 382, 386, 390 
Dynkin operator, 360 stochadc discount factor tangency portfolio, 245,247 forward equation, 359 

CobbDouglas utility model, 326 excess kurtosis 17, 488, 512. See also forward rate, 399,438,440. See also 
effective duration, 406. See also ratio models of habit formation, kurtosis, returns term structure of interest rates 

coupon bonds Y28 excess returns, 12, 182, 268, 291 coupon-bearing term structure, 
effective spread, 102 European option, 349 exercise price, 349 408 



Subject Index Subject Index 

forward-rate curve, 400,412 generalized inverse of a matrix, 244, maximum correlation portfolio, income risk, 318 
log forward rate, 400,408 245 298 incomplete markets, 296, 392 
pure expectations hypothesis, 414, Generalized Method of Moments mean-variance efficiency, 298 independent and identical 

417 (GMM), 174,208,222,314,359, nonnegativity constraints, 301 distribution (IID), 15, 33, 475 
yield to maturity, 400 448,449,455,489,494,532 Heath-Jan-ow-Morton model, 457. indexed bonds, 395 

forward trading, 399 asymptotic distribution, 533 See also pricing fixed-income indirect slope estimator, 505 
fractionally differenced time series, asymptotic variance, 533 derivative securities infinitesimal generator, 360 

60 Newey-West estimator, 535 Heaviside activation function, 513 information matrix, 191, 358,538 
fractionally integrated time series. stochastic differential equation, hedge portfolios, 322 information-matrix equality, 539 

See fractionally differenced time 359 hedge ratio, 352,353. See also input layer, 513 
series weighting matrix, 533 delta-hedging instrumental variables (N) 

fundamental asset, 356 geometric Brownian motion, 383. heterogeneous investors, 318,335 regression, 311 , 313,494,527, 
fundamental value, 258,288 See also Brownian motion heteroskedasticity- and 535 

risk-neutralized process, 355,370 autocorrelation-consistent instruments, 447, 528 
GMM. See Generalized Method of standard errors, 130, 174, 268, integrated GARCH model, 484 

Gamma of an option, 353 Moments 534 interest rate. See coupon bonds, 
GARCH models, 483,486,487 Goldman-Sosin-Gatto option price heteroskedasticity-consistent discount bonds, forward rate, 

absolute value GARCH model, 485 formula, 385, 394 estimators, 54 interest-rate forecasts, short-term 
additional explanatory variables, Gordon growth model, 256 hidden layer, 514 interest rate, term structure of 

488 dynamic Gordon growth model, hidden units, 514 interest rates, yield spread, 
BEKK model, 491 263 historical volatility, 378 riskless interest rate 
conditional market model, 493 government spending in the utility Ho-Lee model, 456, 464. See also interest-rate forecasts, 418 
conditional nonnormality, 488 function, 326 pricing fixed-income derivative internal rate of return, 401 
constant-correlation model, 492 Granger-causality, 91  securities interpolation problems, 516 
estimation, 487,489 Greeks. 353 holding-period return, 397 Intertemporal Capital Asset Pricing 
excess kurtosis in standardized homoskedastic single-factor Model (ICAPM), 219,221,291. 

residuals, 488 habit formation, 327 term-structure model, 429, 452, See also Capital Asset Pricing 
GARCH(1,l) model, 483,497 Abel model, 327 457 Model, multifactor models 
GARCH-M model, 494 Campbell-Cochrane model, 330 Hotelling T2 statistic, 232 intertemporal marginal rate of 
IGARCH model, 484 Constantinides model, 330 Hsieh test of nonlinearity, 475 substitution, 294 
interest rate volatility, 452 external-habit models, 327 Hull and White stochastic volatility intertemporal substitution effect, 
multivariate, 490 internal-habit models, 327 model, 380 331. See also elasticity of 
persistence, 483 difference models, 329 Hurst-Mandelbrot rescaled range intertemporal substitution 
QGARCH model, 497 ratio models, 327 statistic. See rescaled range investor heterogeneity and, 31 7 
single-factor GARCH (1,l) model, Hamilton Markov-switching model, statistic irregularly sampled data, 363 

49 1 472 hyperbasis functions, 51 7 ISE estimator, 505 
stationary distribution, 484 Hansen's test of overidentifylng hyperbolic discounting, 334 isoelastic preferences. See power 
US stock returns, 488 restrictions, 531 utility 
VECH model, 491 Hansen-Jagannathan volatility idiosyncratic risk, 72, 92, 221, 318 It6 process, 348. See also Brownian 

GARCH-in-mean model, 494 bound, 296. See also stochastic IGARCH model, 484 motion, stochastic differential 
GARCH-M model, 494 discount factor IID. See independent and identical equation 
Gaussian kernel, 501 benchmark portfolio, 298 distribution ItB's Lemma, 348, 351 
Generalized Autoregressive Equity Premium Puzzle, 302 immunization, 405 IV regression. See instrumental 

Conditionally Heteroskedastic geometric interpretation, 298 implied volatility, 377 variables regression 
models, 483. See also GARCH lognormal asset pricing model importance sampling, 388 
models with power utility and, 309 income effect, 321. See also January effect, 100 

Generalized Error Distribution, 489 market frictions and, 315 substitution effect Joseph Effect, 59 



Subject Index Subject Index 

kernel regression, 500 loglinear approximation, 260,320, Markov process, 357 Monte Carlo simulation methods, 
average derivative estimators, 505 406 Markov-switching models, 472 340,382,386 
convergence property, 501 accuracy, 262 martingale, 28, 256 antithetic variates method, 388 
curse of dimensionality, 504 coupon bonds, 406 martingale convergence theorem, comparisons with closed-fonn 
optimal bandwidth selection, 502 intertemporal budget constraint, 484 solutions, 384 
universal approximation property, 320 martingale pricing technique, 354 computational cost, 386 

515 lognormal distribution, 15 maturity, 396 control variate method, 387 
weight function, 500 lognormal model of asset pricing, maximum correlation portfolio, 298. crude Monte Carlo, 386 

Kronecker product, 532 306 See also Hansen-Jagannathan discretization, 383, 385 
kurtosis, 16, 19, 81, 480,488. See also Cobb-Douglas utility, 326 volatility bound efficiency, 386 

returns Epstein-Zin-Weil recursive utility, maximum likelihood, 7 importance sampling, 388 
319 maximum likelihood estimation, limitations, 390 

external-habit model, 328 358,536 number-theoretic method, 388 
labor income, 318 power utility, 306 asymptotic distribution, 538 pathdependent option pricing, 
Lagrangian function, 184 long-horizon regressions, 267 continuous-record asymptotics, 382 
latent-variable models, 446 R2 statistics, 271 364 stratified sampling, 388 
Law of Iterated Expectations, 24,255 orthogonality tests, 279 factor analysis, 234 variance-reduction techniques, 
lead-lag relations. See variance ratio, 272 GARCH models, 487 387 

cross-autocorrelation bias, 273 information matrix, 538 mortgage-backed securities, 406 
learning networks, 512,518 dividend-price ratio, 268 information-matrix equality, 539 multifactor models, 219, 324. See also 

Black-Scholes formula and, 519 dynamic asset-allocation models, irregularly sampled data, 363 Arbitrage Pricing Theory, exact 
limitations, 518 287 option price, 367 factor pricing, Intertemporal 
multilayer perceptrons, 512 finite-sample inference, 273 quasi-maximum likelihood Capital Asset Pricing Model, 
projection pursuit regression, 518 investment strategies, 287 estimation, 539 selection of factors 
radial basis functions, 516 long-horizon returns, 55, 78. See also stochastic differential equation, Black version of the CAPM, 224, 

legal liability, 149, 179 variance ratio 357 229 
leisure in the utility function, 326 long-range dependence, 59 White specification test, 539 crosssectional regression 
leverage hypothesis, 497 Longstaff-Schwartz model, 438 mean reversion, 89 approach, 222,233 
likelihood function, 362,537. See also lookback options, 391 mean-variance efficient-set empirical studies, 240 

maximum likelihood estimation loss aversion, 333 mathematics, 184,243,298 Epstein-Zin-Weil recursive utility 
likelihood ratio test, 193 global minimum-variance model, 324 
liquidity effects, 405 Macaulay's duration, 403 portfolio, 185, 217 estimation of expected returns, 
local averaging, 500, 502 marked to market, 459 Hansen-Jagannathan volatility 231 
local volatility, 375 market efficiency, 20, 30. See also bound, 298 estimation of risk premia, 231 
log dividend-price ratio, 264 Efficient Markets Hypothesis minimum-variance frontier, 185 factor portfolios spanning the 
log forward rate, 400, 408. See also market frictions, 314 Sharpe ratio, 188 mean-variance frontier, 228 

forward rate aggregate consumption data, 316 tangency portfolio, 188, 196, 218 Generalized Method of Moments, 
log holding-period return, 398 Hansen-Jagannathan bounds, 315 zero-beta portfolio, 182, 185, 218 222 
log pure expectations hypothesis, market microstructure, 83. See also m-histories, 112 Hotelling statistic, 232 

414 barrier models, bid-ask spread, mixed distribution, 481 macroeconomic variables as 
log yield, 397, 399, 408. See also yield bid-ask bounce, nonsynchronous mixture of normal distributions, 481 factors, 226 

to maturity trading, price discreteness, ML estimation. See maximum portfolios as factors, 223 
log-likelihood function, 190, 358, rounding models likelihood estimation termstructure models, 440 

487. See also maximum likelihood market model, 155 MLP. See multilayer perceptron multilayer perceptron (MLP), 512 
estimation conditional, 493 modified duration, 405. See also multiplicative linear congruential 

logistic function, 513 market portfolio, 155, 181, 323, 495 duration generators (MLCG) ,525 
logistic map, 525 Markov chain, 38,81, 145 moment conditions, 359 multipoint moment conditions, 361 



Subject Index Subject In&% 

mu1t iquadrics, 51 7 empirical findings, 128 perfect-foresight stock price, 275 hypothesis, term structure of 
multivariate GARCH models, 490 nontrading process, 145 observability in finite samples, 278 interest rates 
multivariate stochastic-volatility nontrading. See nonsynchronous perfectly hedged portfolio, 352. SP alternatives, 41 8 

models, 493 trading also delta-hedging strategy implications, 417 
normal distribution, 15 performance evaluation, 183 log pure expectations hypothesis, 

Nadaraya-Watson kernel estimator, number-theoretic method, 388 permanent shock, 65. See also unit 414 
500 root process preferred habitats, 418 

network topology, 514 offer. See ask price persistence in expected stock put option, 349 
networks, 512. See also learning optimal orthogonal portfolio, 243, returns, 265 

networks 245, 248. See also exact factor persistence in volatility, 483, 492 QGARCH model, 497 
neural networks, 512 . See also pricing peso problem, 310 Q-statistic, 47 

learning networks option pricing, 349 piecewise-linear models, 472 quadratic form, 528 
Newey-West estimator, 535 adjusting the Black-Scholes plain vanilla options, 349 quadratic GARCH model, 497 
news impact curve, 485 formula for predictability, 375 Poincark section, 475 quasi-maximum likelihood 
n-histories, 476 Black-Scholes and Merton option 

-p ol.y nomial models, 471 estimation, 489, 539 
no-arbitrage condition, 339 pricing model, 350 portfolio performance evaluation, 

BlackScholes formula, 371, 373 
noise traders, 317 183 

discrete-time models, 381 K 2  statistic, 271 
nominal bonds, 442 power utility, 305, 434 

estimator for a', 374 radial basis functions (RBFs), 516 
nominal stochastic discount factor, PPR. See projection pursuit 

incomplete markets, 392 rainbow options, 391 
443. See also stochastic discount regression 

martingale approach, 354 random number generators, 525 
factor precautionary savings, 310, 331 random walk, 27. See also 

nonlinear ARMA maximum likelihood estimation, 
models, 471 predictability of stock returns, 27, 

367 long-horizon returns, long-range 
nonlinear autoregressive models, 267 

nonparametric methods, 392 dependence, Random Walk 1 
471 Black-Scholes formula, 371, 375 

pathdependent options, 382 model, Random Walk 2 model, 
nonlinear dynamical systems, 473. Campbell-Cochrane model, 332 

risk-neutral pricing method, 382 Random Walk 3 model, technical 
See also deterministic nonlinear dividend-price ratio, 268 

state-price densities, 509 analysis, unit root processes, 
dynamical systems rational bubbles, 260 

option sensitivities, 353 variance difference, variance 
nonlinear-in-mean time-series time-varying risk-aversion, 332 

ordered probit model, 122, 136 ratio 
models, 469 preferred habitats, 418 

maximum likelihood estimation, continuous-time limit, 344 
nonlinear-in-variance timeseries price 

127,141 discrete-time random walk, 341 
models, 469 clustering, 109, 145 

Ornstein-Uhlenbeck process, 360, empirical evidence, 65 
nonlinear least squares estimation, discovery, 107 

371,434 Random Walk 1 model, 28,31,33 
515,518 discreteness, 109, 143 

orthogonality condition, 528 R-statistics, 34 
nonlinear moving-average models, orthogonality tests, 276 exdividend, 12 canonical correlation, 34 

469 0-U process. See impact, 107, 143 Cowles-Jones ratio, 35 
nonlinear time-series analysis, 468 Ornstein-Uhlenbeck process tick, 108 eigenvalues of the covariance 
nonlinearity testing. See testing for output layer, 513 price of risk, 432,495 matrices, 34 

nonlinear structure overfitting, 498, 523 price-earnings-ratio effect, 21 1 Kendall r correlation test, 34 
nonparametric estimation, 498,515 pricing kernel, 294 likelihood ratio statistic, 34 

universal approximation property, par, 401 principal components, 236 nonparametric tests, 34 
515 parallel processing, 515 principle of invariance, 367 runs test, 38 

nonparametric option pricing parametric option-pricing model, projection pursuit regression, 51 8 semiparametric tests, 34 
methods, 340, 392,510 340,356 prospect theory, 333 sequences and reversals, 35 

nonparametric tests, 172 pathdependent derivatives, 340 psychological models of preferences, Spearman rank correlation test, 34 
nonperiodic cycles, 63 pathdependent options, 340,382 332 Spearman's footrule test, 34 
n~nseparabil iitn~ u tility, 326 PEH. See pure expectations pure expectations hypothesis (PEH), Random Walk 2 model, 28,32,41 
nonsynchronous trading, 84, 177 hypothesis 41 3. See also expectations filter rules, 42 



Subject Index Subject Index 

technical analysis, 43 virtual, 85. See also cross-sectional generalized least stable distribution, 17 
Random Walk 3 model, 28, 33, 44. nonsynchronous trading squares (GLS), 235 standard Brownian motion, 344. Se 

See also Box-Pierce Q-statistic, riding the yield curve, 416 data-snooping, 240,246,251 also Brownian motion 
variance difference, variance risk aversion factor analysis, 234 state prices, 295,507 
ratio difference habit-formation principal components, 236 statedependen t models (SDM) ,47( 

portmanteau statistics, 47 models, 329 rotational indeterminacy, 234 stateprice density (SPD), 507 
rank test, 172 Equity Premium Puzzle, 308, 323, strict factor structure, 234, 239 stationary time-series process, 484. 
rational bubbles, 258 329 selfexciting threshold See also unit root process 
RBFs. See radial basis functions first-order, 334 autoregression (SETAR) models, stochastic approximation, 515  
regularization, 516,522 loss aversion, 333 470 stochastic differential equation, 346, 
replicating portfolio, 353,380, 391 separating risk aversion from self-financing portfolio, 339, 351 356 
replication in Monte Carlo intertemporal substitution, 319 sequences and reversals, 35 GMM estimation, 359 

simulation, 383 time-varying, 330,335 serial correlation, 44 ItB's Lemma, 348 
representative agent models, 292, risk prices, 325 SETAR models. See selfexciting maximum likelihood estimation, 

305 risk-neutral option pricing method, threshold autoregression 357 
rescaled range statistic, 62 354,370,382,509 (SETAR) models multiplication rules, 347 
returns, 254 risk-neutral pricing density, 508 Sharpe ratio, 188, 245, 247, 300 stochastic discount factor, 294, 427, 

annualized, 10 risk-neutrality, 354 short-term interest rate, 430,449. See 429. See also Euler equation 
Bernoulli distribution, 18 risk-neutralized process, 355 also discount bonds, riskless equity premium puzzle, 302 
Cauchy distribution, 18 risk-return tradeoff, 14, 181 interest rate habit-formation difference 
compound, 10 risk-sharing, 318 GARCH effects on volatility, 452 models, 331 
conditional distribution, 14 riskfree interest rate. See riskless regime-switching, 451 Hansen-Jagannathan volatility 
continuously compounded, 11, interest rate shortsales constraints, 315 bound, 296 

sign test, 172 
255 riskfree rate puzzle, 310, 329 nominal, 443 

size effect, 21 1,496 
discount bond, 407 riskless interest rate, 182, 306, 309, nonnegativity, 295,301 

- sizesorted portfolio, 70, 75, 129 
discreteness, 110 319,328,331 power Ltility, 309 

skewness, 17,81,1 72,498. See also state-price density, 508 
discreteness bias, 116 rolling standard deviation, 481 

returns uniqueness, 296 
excess, 12 rotational indeterminacy, 234 

slope estimators, 505 stochastic trend, 65. See also unit 
excess kurtosis 17, 488,512 rounding models, 114 

small stocks, 21 1, 496 root process 
forecasting returns, 268 R/S statistic. See rescaled range 

smoothing, 499, 517. See also kernel stochastic-volatility models, 379,489, 
gross, 9 statistic 

regression 493 
holding-period return, 398 RWI model. See Random Walk 1 

solvency constraint, 315 multivariate, 493 
independently and identically model 

spanning, 380, 391 stratified sampling, 388 
distributed (IID), 15 RW2 model. See Random Walk 2 

SPD. See state-price density strict factor structure, 234, 239 
joint distribution, 13 model 

specification tests strike price, 349 
kurtosis, 16, 19, 81,480,488 RW3 model. See Random Walk 3 

Hansen's test, 531 STRIPS, 396 
log, 11,255 model 

White test, 539 stroboscopic map, 475 
lognormal distribution, 15 spline estimation, 410,412,517 structural breaks, 472 
net, 9 sample paths, 383 exponential spline model, 412 Student-t distribution, 210, 489 
normal distribution, 15 sample selection biases, 212, 251 tax-adjusted spline model, 412 substitution effect, 321, 331 S e e  also 
simple, 11 sampling interval, 364 spot rate, 41 4,417 elasticity of intertemporal 
skewness, 17,81, 172, 498 scale-invariance, 305 spread. See bid-ask spread, yield substitution 
stable distribution, 17 score vector, 537 spread supershares, 507 
unconditional distribution, 15 SDM. See statedependent models spread-lock interest rate swaps, 391 support and resistance levels, 43 
unexpected stock returns, 264, security market line, 14 square-root single-factor surplus consumption ratio, 330 

284 selection of factors, 233 term-structure model, 435,454 survivorship bias, 31 1 .  .'be nlw 



Subject Index Subject Index 

sample selection biases Brock-Dechert-Scheinkman test, variance, l5 ,17 volatility smiles, 512 
synthetic convertible bonds, 391 478 variance-bounds tests, 277 volatility tests, 275 

Hsieh test, 475 variance difference. See variance finite-sample considerations, 278 
Tsay test, 476 ratio Marsh and Merton model, 277 

tail thickness, 480 
Theta, 353 variance inequality, 276 orthogonality tests, 276 

tax clientele, 405 threshold, 472 variance ratio, 48,68 unit roots, 277 
tax-adjusted spline model of the threshold autoregression (TAR), 472 long-horizon regressions, 272 variance-bounds tests, 277 

term structure, 41 2 time aggregation, 94, 129 Random Walk 1 model, 49 Volterra series, 471 
technical analysis, 43. See also time inconsistency, 334 Random Walk 3 model, 53 

Random Walk 2 model time-nonseparability in the utility variance-bounds tests, 277 Wald test, 192, 281, 539 
temporary shock, 65. See also unit function, 327, 329. See also habit variance-reduction techniques, 387 weight function, 500 

root process formation models Vasicek model, 434 weighting matrix 
tent map, 474,476,525 trace operator, 74 VECH model, 491 GMM estimation, 532, 534 
term premia, 418 Trades and Quotes (TAQ) database, vech operator, 490 N estimation, 528, 530 
term structure of implied volatility, 107 vector autoregressive (VAR) white noise, 346 

463 training a learning network, 515,518 methods White specification test, 539 
term structure of interest rates, 397. training path, 519 multiperiod forecasts, 280 Wiener process, 344. See also 

See also yield curve, term-structure transactions costs, 31 5 present-value relations, 279 arithmetic Brownian motion 
models, expectations hypothesis, transactions data, 107,136 price volatility, 280 wild card option, 459 
pure expectations hypothesis transition density function, 358 return volatility, 284 Wishart distribution, 192 

cointegration, 419 Treasury securities, 395 Vega, 353 Wold Representation Theorem, 468 
forecasting interest rates, 418 STRIPS, 396 volatility 
spline estimation, 410 Treasury bills, 396 deterministic, 379 yield curve, 397, 432, 438, 440. See 
tax effects, 411  Treasury notes and bonds, 396 historical, 378 also term structure of interest 
vector autoregressive (VAR) when-issued market, 399 implied, 377 rates 

methods, 422 zero-coupon yield curve, 397 stochastic, 378, 380, 489, 493 yield spread, 397,418 
term-structure models, 427. See also trend-stationary process, 65, 372. See volatility estimation. See ARCH yield to maturity, 397, 401 

fixed-income derivative securities also unit root process models, GARCH models, 
affine-yield models, 428, 441, 445 trending Ornstein-Uhlenbeck stochastic-volatility models zero-beta asset, 294 
Cox, Ingersoll, and Ross model, process, 371 volatility feedback, 497 zero-coupon bonds, 396 

436 Tsay test of nonlinearity, 476 
cross-sectional restrictions, 452 two-factor term-structure model, 438 
fixed-income derivative securities, two-stage least squares (2SLS) 

455 estimation, 530 
Ho-Lee model, 456,464. See also variancecovariance matrix, 531 

pricing fixed-income derivative lognormal asset pricing model 
securities with power utility, 312 

homoskedastic single-factor 
unit root process, 64, 257 

model, 429,452,457 
term structure of interest rates, 

latent-variable models, 446 
419 

Longstaff-Schwartz model, 438 volatility process, 484 
square-root single-factor model, volatility tests, 277 

435,454 universal approximation property, 
stochastic discount factor, 427 515 
two-factor model, 438 
Vasicek model, 434 VAR methods. See vector 

testing for nonlinear structure autoregressive methods